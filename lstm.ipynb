{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "MindSpore-python3.7-aarch64", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "## 1.\u5bfc\u5165\u6a21\u5757", "metadata": {}}, {"cell_type": "code", "source": "import moxing as mox\n#mox.file.copy_parallel(src_url=\"obs://nlp-kim/project/data/\", dst_url='./data/') \nmox.file.copy_parallel(src_url=\"s3://dl4nlp-my/project/data/\", dst_url='./data/') ", "metadata": {}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v1.17.3-d858ff4a\nINFO:root:Using OBS-Python-SDK-3.20.9.1\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "import sys\nimport os\nimport json\nimport pickle as pkl\nimport re\nfrom collections import Counter\nimport numpy as np\nimport random\nfrom collections import OrderedDict\nimport math\n\nimport mindspore\nimport mindspore.nn as nn\nfrom mindspore import Tensor\nfrom mindspore import context\nfrom mindspore.train.model import Model\nfrom mindspore.nn.metrics import Accuracy\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\nfrom mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\nfrom mindspore.ops import operations as ops", "metadata": {}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "## 2.\u6570\u636e\u9884\u5904\u7406", "metadata": {}}, {"cell_type": "markdown", "source": "### 2.1 \u5904\u7406\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def process_sentence(sentence):\n    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n    commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n    punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n                    '(', ')', '=', '+', '\\\\', '_', '-',\n                    '>', '<', '@', '`', ',', '?', '!']\n    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n                    \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n                    \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n                    \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n                    \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n                    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n                    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n                    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n                    \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n                    \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n                    \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n                    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n                    \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n                    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n                    \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n                    \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n                    \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n                    \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n                    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n                    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n                    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n                    \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n\n    inText = sentence.replace('\\n', ' ')\n    inText = inText.replace('\\t', ' ')\n    inText = inText.strip()\n    outText = inText\n    for p in punct:\n        if (p + ' ' in inText or ' ' + p in inText) or \\\n           (re.search(commaStrip, inText) != None):\n            outText = outText.replace(p, '')\n        else:\n            outText = outText.replace(p, ' ')\n    outText = periodStrip.sub(\"\", outText, re.UNICODE)\n    outText = outText.lower().split()\n    for wordId, word in enumerate(outText):\n        if word in contractions:\n            outText[wordId] = contractions[word]\n    outText = ' '.join(outText)\n    return outText\n\ndef process_answer(answer):\n    articles = ['a', 'an', 'the']\n    manualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n                  '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n                  'eight': '8', 'nine': '9', 'ten': '10' }\n    new_answer = process_sentence(answer)\n    outText = []\n    for word in new_answer.split():\n        if word not in articles:\n            word = manualMap.setdefault(word, word)\n            outText.append(word)\n    return ' '.join(outText)", "metadata": {}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "### 2.2 \u53d8\u91cf\u8bf4\u660e", "metadata": {}}, {"cell_type": "code", "source": "#qa\uff1aquestion\u548c\u5bf9\u5e94\u7684annotions\n#train_question_ids\uff1aquestion\u7684id\u7684\u6570\u7ec4\n\n#question_dict_count\uff1a question\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_questions\uff1a question\u8bed\u53e5split\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n#answer_dict_count\uff1a answer\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_answers\uff1a answer\u88absplit\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n\n#question_key\uff1a\u6309\u7167question\u4e2d\u51fa\u73b0\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n#answer_top_k: \u6309\u7167answer\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n", "metadata": {}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": "### 2.3 \u9884\u5904\u7406\u7ec6\u8282", "metadata": {}}, {"cell_type": "code", "source": "f = open(\"./data/questions/train.json\", \"r\")\nf1 = open(\"./data/annotations/train.json\", \"r\")\nfile = json.load(f)\nfile1 = json.load(f1)\nannotations = file1['annotations']\ntrain_question_ids = []\ntrain_image_ids = []\ntrain_questions = []\ntrain_answers = []\nquestion_dict_count = dict()\nanswer_dict_count = dict()\n\n# \u5f62\u6210qa\uff1a\u4e00\u4e2a\u5b57\u5178\uff0c\u6574\u7406\u51fa\u5bf9\u5e94question_id\u7684annotation\nqa = {ann['question_id']: [] for ann in annotations}\nfor ann in annotations:\n    qa[ann['question_id']] = ann\n\n#\u83b7\u53d6image_id question_id\nfor idx, item in enumerate(file['questions']):\n    train_question_ids.append(item['question_id'])\n    train_image_ids.append(item['image_id'])\n    \n    #process question\n    question = item['question']\n    question = process_sentence(question)\n    question = question.split()\n    for word in question:\n        question_dict_count[word] = question_dict_count.get(word, 0) + 1\n    train_questions.append(question)\n    answer = qa[item['question_id']]['answers']\n    answer_new = [process_answer(ans['answer']) for ans in answer]\n    ans_array = []\n    for ans in answer:\n        ans_array.append(ans['answer'])\n    for word in answer_new:\n        answer_dict_count[word] = answer_dict_count.get(word, 0) + 1\n    train_answers.append(ans_array)\n    if idx % 10000 == 0:\n        print ('finished processing %d in train' %(idx))\n\n# sort question dict\nquestion_count = question_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(question_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(question_count, reverse=True)\nquestion_key = list(question_dict_count.keys())\n# \u5bf9question_key\u91cd\u65b0\u6392\u5e8f\nquestion_key = [question_key[idx] for idx in sorted_index]\n# add '<unk>' to the begining\nquestion_key.insert(0, '<unk>')\n# '<unk>' begins at 1, 0 is reserved for empty words\nquestion_key = dict((key, idx + 1) for idx, key in enumerate(question_key))\n\nk = 1000\n# sort answer dict and get top k answers\ndel answer_dict_count['']\nanswer_count = answer_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(answer_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(answer_count, reverse=True)\nanswer_key = list(answer_dict_count.keys())\nanswer_key = [answer_key[idx] for idx in sorted_index]\nanswer_top_k = answer_key[:k]\nanswer_top_k = dict((key, idx) for idx, key in enumerate(answer_top_k))\n\n# convert words to idx and remove some\ntrain_question_idx = []\ntrain_answer_idx = []\ntrain_answer_counter = []\nidx_to_remove = []\nfor idx, answer in enumerate(train_answers):\n    question_idx = [question_key[word] for word in train_questions[idx]]\n    #print(question_idx)\n    #print('\\n')\n    #print(train_questions[idx])\n    train_question_idx.append(question_idx)\n    answer_idx = [answer_top_k[ans] for ans in answer\n                 if ans in answer_top_k]\n    answer_counter = Counter(answer_idx)\n    train_answer_counter.append(answer_counter)\n    train_answer_idx.append(answer_idx)\n    if not answer_idx:\n        idx_to_remove.append(idx)\nprint ('%d out of %d, %f of the question in train are removed'\\\n    %(len(idx_to_remove), len(train_question_ids),\n      len(idx_to_remove) / float(len(train_question_ids))))\n\n# transform to array and delete all the empty answer\ntrain_question_ids = np.array(train_question_ids)\ntrain_image_ids = np.array(train_image_ids)\ntrain_question_idx = np.array(train_question_idx)\ntrain_answer_idx = np.array(train_answer_idx)\ntrain_answer_counter = np.array(train_answer_counter)\n\ntrain_question_ids = np.delete(train_question_ids, idx_to_remove)\ntrain_image_ids = np.delete(train_image_ids, idx_to_remove)\ntrain_question_idx = np.delete(train_question_idx, idx_to_remove)\ntrain_answer_idx = np.delete(train_answer_idx, idx_to_remove)\ntrain_answer_counter = np.delete(train_answer_counter, idx_to_remove)\n\n# reshuffle the train data\nidx_shuffle = list(range(train_question_ids.shape[0]))\nrandom.shuffle(idx_shuffle)\ntrain_question_ids = train_question_ids[idx_shuffle]\ntrain_image_ids = train_image_ids[idx_shuffle]\ntrain_question_idx = train_question_idx[idx_shuffle]\ntrain_answer_idx = train_answer_idx[idx_shuffle]\ntrain_answer_counter = train_answer_counter[idx_shuffle]\n\n# the most frequent as label\ntrain_answer_label = [counter.most_common(1)[0][0]\n                      for counter in train_answer_counter]\ntrain_answer_label = np.array(train_answer_label)\n\n# transform from counter to dict\ntrain_answer_counter = [dict(counter) for counter in train_answer_counter]\ntrain_answer_counter = np.array(train_answer_counter)\n\nprint ('finished processing train')", "metadata": {}, "execution_count": 6, "outputs": [{"name": "stdout", "text": "finished processing 0 in train\nfinished processing 10000 in train\nfinished processing 20000 in train\nfinished processing 30000 in train\nfinished processing 40000 in train\n2105 out of 44375, 0.047437 of the question in train are removed\nfinished processing train\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### 2.4 \u6784\u5efa\u8bcd\u5411\u91cf", "metadata": {}}, {"cell_type": "code", "source": "#construct one hot vector\nall_question_vector=[]\nfor idx,question in enumerate(train_questions):\n    count = 0\n    question_vector = []\n    for word in question:\n        count = count + 1\n        if count > 10:\n            break\n        else:\n            q_emb = np.zeros((len(question_key) + 1), dtype='int32')\n            q_emb[question_key[word]] = 1\n            question_vector.append(q_emb)\n    while count < 10:\n        padding = np.zeros((len(question_key) + 1), dtype='int32')\n        question_vector.append(padding)\n        count = count + 1\n    all_question_vector.append(question_vector)", "metadata": {}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "#convert word to idx\nall_question_idx = []\nfor question in train_questions:\n    count = 0\n    one_question_idx = []\n    \n    for word in question:\n        count = count + 1\n        if count > 10:\n            break\n        else:\n            one_question_idx.append(question_key[word])     \n    while count < 10:\n        one_question_idx.append(0)\n        count = count + 1        \n    all_question_idx.append(one_question_idx)\nall_question_idx", "metadata": {}, "execution_count": 8, "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "[[3, 99, 1627, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 16, 126, 8, 0, 0, 0, 0],\n [5, 2, 295, 1164, 7, 10, 3990, 13, 0, 0],\n [4, 3, 2, 16, 31, 0, 0, 0, 0, 0],\n [4, 3, 2, 28, 20, 0, 0, 0, 0, 0],\n [144, 164, 1628, 44, 452, 7, 2, 219, 0, 0],\n [4, 3, 7, 2, 55, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 630, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 58, 0, 0, 0, 0, 0],\n [23, 3, 2, 154, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 248, 56, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 248, 0, 0, 0, 0, 0],\n [4, 3, 2, 121, 0, 0, 0, 0, 0, 0],\n [3, 2, 63, 296, 268, 0, 0, 0, 0, 0],\n [4, 13, 133, 3, 1781, 18, 2, 229, 0, 0],\n [5, 35, 58, 3991, 0, 0, 0, 0, 0, 0],\n [5, 2, 101, 108, 18, 2994, 2, 255, 0, 0],\n [4, 13, 5, 2, 145, 0, 0, 0, 0, 0],\n [11, 12, 145, 5, 413, 8, 2, 248, 0, 0],\n [11, 12, 191, 5, 7, 2, 55, 0, 0, 0],\n [4, 13, 3, 2, 36, 0, 0, 0, 0, 0],\n [4, 5, 2, 741, 56, 8, 0, 0, 0, 0],\n [3, 2, 34, 95, 2, 72, 0, 0, 0, 0],\n [929, 516, 3, 8, 2, 3992, 274, 0, 0, 0],\n [4, 5, 2, 39, 31, 0, 0, 0, 0, 0],\n [3, 6, 10, 2134, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 49, 3, 67, 0, 0, 0, 0],\n [4, 3, 2, 870, 8, 2, 243, 41, 0, 0],\n [4, 870, 3, 8, 2, 243, 41, 0, 0, 0],\n [3, 6, 699, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 3993, 5, 14, 0, 0, 0, 0, 0],\n [85, 164, 1123, 95, 0, 0, 0, 0, 0, 0],\n [3, 2, 16, 20, 1225, 871, 0, 0, 0, 0],\n [23, 3, 2, 459, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 19, 9, 8, 86, 0, 0, 0],\n [11, 12, 409, 5, 8, 2, 86, 0, 0, 0],\n [11, 12, 150, 15, 2, 63, 24, 0, 0, 0],\n [11, 12, 467, 3, 2, 28, 37, 0, 0, 0],\n [3, 14, 10, 2492, 7, 104, 9, 2, 116, 0],\n [3, 2, 339, 760, 0, 0, 0, 0, 0, 0],\n [3, 2, 102, 453, 128, 2995, 0, 0, 0, 0],\n [4, 13, 64, 3, 6, 0, 0, 0, 0, 0],\n [4, 32, 9, 1337, 3, 8, 2, 334, 0, 0],\n [4, 13, 3, 2, 59, 134, 0, 0, 0, 0],\n [4, 13, 145, 5, 8, 2, 87, 68, 0, 0],\n [3, 2, 275, 585, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 536, 1951, 0, 0, 0, 0, 0],\n [3, 2, 53, 631, 7, 6, 19, 991, 0, 0],\n [3, 2, 233, 276, 26, 80, 652, 0, 0, 0],\n [4, 13, 5, 2, 3994, 7, 2, 992, 0, 0],\n [5, 2, 17, 8, 10, 1952, 486, 0, 0, 0],\n [5, 90, 2, 632, 8, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 19, 0, 0, 0],\n [3, 2, 487, 8, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 145, 0, 0, 0, 0, 0],\n [4, 3, 2, 445, 135, 71, 10, 102, 8, 22],\n [3, 6, 10, 117, 262, 94, 0, 0, 0, 0],\n [11, 12, 2135, 15, 6, 53, 24, 0, 0, 0],\n [4, 13, 3, 2, 64, 0, 0, 0, 0, 0],\n [4, 3, 2, 136, 375, 48, 3, 20, 0, 0],\n [4, 3, 2, 1496, 16, 3995, 18, 2, 1496, 34],\n [106, 30, 397, 10, 2136, 376, 30, 3996, 6, 46],\n [5, 196, 9, 2, 285, 370, 0, 0, 0, 0],\n [4, 3, 7, 2, 59, 89, 0, 0, 0, 0],\n [23, 3, 2, 33, 0, 0, 0, 0, 0, 0],\n [11, 12, 162, 5, 8, 2, 19, 0, 0, 0],\n [5, 35, 1953, 212, 114, 0, 0, 0, 0, 0],\n [3, 14, 10, 2996, 9, 55, 8, 2, 992, 0],\n [4, 3, 2, 16, 80, 8, 0, 0, 0, 0],\n [4, 3, 189, 7, 2, 199, 0, 0, 0, 0],\n [11, 12, 682, 5, 8, 2, 82, 0, 0, 0],\n [3, 6, 361, 2137, 0, 0, 0, 0, 0, 0],\n [4, 128, 517, 5, 167, 18, 2, 64, 0, 0],\n [3, 2, 239, 168, 0, 0, 0, 0, 0, 0],\n [3, 2, 16, 8, 241, 20, 10, 190, 0, 0],\n [5, 2, 305, 56, 0, 0, 0, 0, 0, 0],\n [11, 12, 742, 0, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 114, 5, 7, 6, 27, 0, 0],\n [4, 13, 3, 2, 306, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 20, 0, 0, 0, 0, 0],\n [15, 2, 75, 24, 44, 158, 1338, 7, 2, 1075],\n [5, 35, 3997, 26, 1280, 1403, 0, 0, 0, 0],\n [11, 271, 15, 6, 175, 83, 0, 0, 0, 0],\n [51, 89, 3, 2, 28, 37, 2, 202, 7, 0],\n [3, 2, 872, 279, 0, 0, 0, 0, 0, 0],\n [23, 3, 2, 52, 56, 0, 0, 0, 0, 0],\n [4, 3, 2, 1954, 16, 7, 2, 1629, 252, 0],\n [11, 12, 307, 5, 7, 2, 19, 0, 0, 0],\n [51, 128, 5, 2138, 58, 0, 0, 0, 0, 0],\n [11, 12, 17, 1782, 24, 10, 269, 0, 0, 0],\n [11, 12, 332, 29, 30, 61, 0, 0, 0, 0],\n [4, 3, 1404, 2, 761, 0, 0, 0, 0, 0],\n [4, 13, 134, 3, 6, 16, 20, 0, 0, 0],\n [3, 10, 28, 20, 10, 2997, 1405, 0, 0, 0],\n [4, 73, 5, 35, 58, 0, 0, 0, 0, 0],\n [11, 12, 17, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 161, 191, 5, 14, 0, 0, 0, 0],\n [3, 2, 52, 1630, 0, 0, 0, 0, 0, 0],\n [11, 12, 101, 5, 151, 0, 0, 0, 0, 0],\n [4, 743, 3, 2, 16, 71, 2, 156, 0, 0],\n [84, 6, 19, 74, 40, 311, 0, 0, 0, 0],\n [4, 13, 3, 2, 104, 174, 0, 0, 0, 0],\n [23, 3, 10, 159, 586, 42, 0, 0, 0, 0],\n [4, 13, 3, 2, 1281, 0, 0, 0, 0, 0],\n [5, 326, 993, 8, 3998, 3999, 0, 0, 0, 0],\n [5, 2, 172, 7, 386, 0, 0, 0, 0, 0],\n [23, 15, 6, 19, 491, 230, 0, 0, 0, 0],\n [15, 2, 176, 83, 129, 0, 0, 0, 0, 0],\n [4, 121, 3, 810, 2, 209, 0, 0, 0, 0],\n [11, 12, 235, 3, 2, 119, 80, 8, 0, 0],\n [11, 12, 1124, 5, 7, 2, 19, 0, 0, 0],\n [11, 12, 17, 24, 501, 8, 140, 158, 0, 0],\n [3, 6, 40, 2, 335, 0, 0, 0, 0, 0],\n [4, 32, 9, 49, 5, 35, 873, 0, 0, 0],\n [4, 15, 2, 46, 97, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 247, 0, 0, 0, 0, 0],\n [280, 7, 2, 121, 1631, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 52, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 322, 231, 0, 0, 0, 0],\n [192, 208, 38, 744, 0, 0, 0, 0, 0, 0],\n [4, 112, 3, 78, 308, 0, 0, 0, 0, 0],\n [5, 30, 1339, 9, 180, 0, 0, 0, 0, 0],\n [3, 2, 213, 7, 2493, 0, 0, 0, 0, 0],\n [62, 5, 587, 9, 2, 402, 445, 0, 0, 0],\n [4, 3, 2, 129, 8, 2, 68, 0, 0, 0],\n [4, 32, 9, 141, 3, 6, 0, 0, 0, 0],\n [4, 13, 3, 2, 4000, 0, 0, 0, 0, 0],\n [4, 2494, 3, 1783, 18, 2, 596, 0, 0, 0],\n [4, 272, 3, 58, 0, 0, 0, 0, 0, 0],\n [3, 14, 176, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 1226, 394, 621, 18, 2495, 245, 2, 256],\n [4, 32, 9, 36, 3, 151, 0, 0, 0, 0],\n [3, 2, 63, 189, 784, 0, 0, 0, 0, 0],\n [4, 65, 15, 2, 653, 202, 97, 0, 0, 0],\n [4, 13, 131, 5, 2, 227, 150, 2998, 0, 0],\n [4, 3, 2, 1784, 9, 2, 336, 0, 0, 0],\n [3, 6, 10, 717, 286, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 1227, 0, 0, 0, 0, 0],\n [5, 14, 2139, 1406, 8, 2, 64, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 133, 0, 0, 0],\n [15, 48, 24, 246, 125, 8, 44, 1785, 8, 44],\n [4, 3, 2, 112, 21, 17, 5, 58, 0, 0],\n [3, 1632, 1955, 488, 518, 0, 0, 0, 0, 0],\n [4, 3, 99, 95, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 21, 39, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 156, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 37, 0, 0, 0, 0, 0],\n [4, 13, 50, 3, 6, 16, 20, 0, 0, 0],\n [3, 14, 23, 930, 414, 376, 930, 354, 18, 2999],\n [3, 6, 10, 460, 829, 0, 0, 0, 0, 0],\n [42, 30, 61, 2, 3000, 124, 0, 0, 0, 0],\n [4, 3, 2, 468, 8, 2, 454, 0, 0, 0],\n [4, 3001, 63, 3, 7, 2, 171, 0, 0, 0],\n [4, 3, 2, 41, 31, 0, 0, 0, 0, 0],\n [3, 6, 718, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 72, 785, 3002, 0, 0, 0, 0, 0],\n [3, 6, 10, 502, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 1786, 327, 0, 0],\n [11, 12, 39, 5, 7, 6, 19, 0, 0, 0],\n [4, 39, 5, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 162, 0, 0, 0],\n [11, 12, 1228, 5, 14, 0, 0, 0, 0, 0],\n [11, 12, 150, 5, 40, 2, 87, 104, 9, 2],\n [4, 3, 2, 13, 9, 2, 53, 0, 0, 0],\n [11, 12, 17, 5, 67, 0, 0, 0, 0, 0],\n [4, 3, 2, 46, 1229, 0, 0, 0, 0, 0],\n [11, 12, 597, 5, 8, 2, 46, 0, 0, 0],\n [3, 6, 10, 94, 46, 0, 0, 0, 0, 0],\n [4, 112, 3, 78, 308, 0, 0, 0, 0, 0],\n [3, 2, 302, 1407, 0, 0, 0, 0, 0, 0],\n [4, 15, 2, 480, 93, 79, 1408, 97, 8, 2],\n [11, 303, 5, 14, 0, 0, 0, 0, 0, 0],\n [3, 2, 224, 811, 8, 10, 653, 202, 0, 0],\n [11, 12, 1165, 5, 7, 2, 19, 0, 0, 0],\n [3, 2, 16, 80, 111, 0, 0, 0, 0, 0],\n [15, 6, 16, 24, 45, 1787, 8, 964, 0, 0],\n [4, 3, 6, 33, 157, 43, 0, 0, 0, 0],\n [3, 2, 16, 296, 10, 19, 9, 1497, 0, 0],\n [4, 654, 42, 542, 81, 2, 19, 0, 0, 0],\n [5, 196, 622, 633, 2, 122, 65, 0, 0, 0],\n [62, 5, 21, 17, 8, 140, 653, 874, 0, 0],\n [15, 2, 160, 24, 2140, 328, 0, 0, 0, 0],\n [11, 12, 328, 15, 2, 1040, 24, 0, 0, 0],\n [3, 2, 100, 56, 1282, 8, 2, 160, 0, 0],\n [4, 3, 2, 403, 60, 9, 0, 0, 0, 0],\n [4, 112, 3, 78, 308, 0, 0, 0, 0, 0],\n [3, 14, 10, 853, 0, 0, 0, 0, 0, 0],\n [4, 234, 3, 2, 66, 0, 0, 0, 0, 0],\n [5, 35, 1498, 0, 0, 0, 0, 0, 0, 0],\n [4, 147, 5, 2, 526, 18, 2, 70, 20, 0],\n [4, 13, 3, 2, 219, 0, 0, 0, 0, 0],\n [3, 48, 58, 10, 73, 0, 0, 0, 0, 0],\n [4, 623, 5, 151, 0, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 59, 50, 0, 0, 0, 0],\n [4, 13, 3, 2, 481, 50, 0, 0, 0, 0],\n [3, 14, 10, 1283, 8, 77, 682, 0, 0, 0],\n [11, 12, 1041, 5, 7, 2, 225, 0, 0, 0],\n [4, 440, 3, 151, 0, 0, 0, 0, 0, 0],\n [144, 2, 965, 397, 2, 142, 0, 0, 0, 0],\n [4, 65, 3003, 3, 67, 0, 0, 0, 0, 0],\n [88, 3, 37, 2, 202, 0, 0, 0, 0, 0],\n [3, 6, 1788, 0, 0, 0, 0, 0, 0, 0],\n [5, 2, 931, 8, 0, 0, 0, 0, 0, 0],\n [15, 2, 41, 830, 18, 414, 43, 10, 903, 0],\n [4, 537, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 4001, 904, 3, 415, 7, 588, 8, 6, 404],\n [4, 3, 3004, 193, 13, 8, 2, 82, 0, 0],\n [3, 14, 10, 3005, 9, 191, 7, 6, 19, 0],\n [11, 12, 905, 5, 14, 0, 0, 0, 0, 0],\n [4, 121, 3, 8, 6, 666, 0, 0, 0, 0],\n [4, 5, 2, 105, 446, 8, 2, 72, 0, 0],\n [4, 3, 2, 3006, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 469, 8, 2, 46, 0, 0],\n [4, 13, 3, 2, 41, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 100, 7, 2, 4002, 318, 0],\n [4, 13, 3, 2, 240, 195, 0, 0, 0, 0],\n [3, 14, 10, 2496, 7, 2, 69, 0, 0, 0],\n [4, 13, 3, 2, 1956, 8, 2, 247, 0, 0],\n [4, 3, 189, 429, 2, 199, 0, 0, 0, 0],\n [106, 10, 16, 38, 216, 18, 441, 54, 447, 9],\n [5, 2, 434, 231, 2, 122, 13, 0, 0, 0],\n [4, 1409, 5, 8, 2, 295, 0, 0, 0, 0],\n [5, 21, 39, 2141, 18, 4003, 0, 0, 0, 0],\n [29, 30, 61, 10, 313, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 39, 0, 0, 0, 0, 0],\n [4, 5, 21, 39, 31, 0, 0, 0, 0, 0],\n [3, 2, 2497, 1957, 4004, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 104, 9, 2, 47, 0, 0],\n [11, 12, 3007, 2498, 5, 7, 6, 19, 0, 0],\n [3, 2, 55, 371, 0, 0, 0, 0, 0, 0],\n [5, 21, 307, 26, 319, 58, 933, 0, 0, 0],\n [4, 112, 5, 35, 58, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 176, 0, 0, 0],\n [3, 14, 10, 28, 470, 8, 2, 53, 0, 0],\n [482, 930, 38, 2142, 103, 0, 0, 0, 0, 0],\n [3, 14, 10, 410, 9, 227, 0, 0, 0, 0],\n [106, 10, 28, 340, 6, 854, 9, 344, 18, 414],\n [4, 3, 8, 2, 59, 158, 0, 0, 0, 0],\n [11, 12, 105, 323, 5, 14, 0, 0, 0, 0],\n [23, 3, 2, 133, 0, 0, 0, 0, 0, 0],\n [23, 3, 2, 507, 0, 0, 0, 0, 0, 0],\n [3, 2, 100, 20, 362, 3008, 0, 0, 0, 0],\n [4, 13, 3, 2, 123, 0, 0, 0, 0, 0],\n [4, 32, 9, 313, 3, 7, 2, 543, 0, 0],\n [4, 13, 3, 2, 63, 0, 0, 0, 0, 0],\n [3, 6, 28, 812, 0, 0, 0, 0, 0, 0],\n [5, 35, 3009, 425, 0, 0, 0, 0, 0, 0],\n [5, 2, 59, 235, 461, 2, 82, 0, 0, 0],\n [3, 2, 63, 556, 7, 2, 717, 0, 0, 0],\n [3, 2, 182, 7, 2, 527, 403, 0, 0, 0],\n [4, 644, 1789, 3, 10, 1125, 9, 6, 336, 0],\n [4, 234, 3, 2, 203, 64, 0, 0, 0, 0],\n [3, 54, 10, 700, 123, 0, 0, 0, 0, 0],\n [5, 35, 1340, 40, 329, 220, 0, 0, 0, 0],\n [5, 14, 45, 147, 7, 6, 19, 0, 0, 0],\n [3, 14, 10, 118, 0, 0, 0, 0, 0, 0],\n [3, 2, 503, 1076, 8, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 47, 0, 0, 0, 0, 0],\n [4, 3, 2, 227, 131, 3010, 18, 2, 47, 0],\n [11, 12, 1790, 5, 14, 0, 0, 0, 0, 0],\n [4, 3, 483, 2, 16, 0, 0, 0, 0, 0],\n [4, 25, 9, 416, 3, 6, 0, 0, 0, 0],\n [29, 21, 786, 745, 329, 220, 0, 0, 0, 0],\n [5, 14, 17, 178, 0, 0, 0, 0, 0, 0],\n [11, 12, 341, 42, 38, 127, 0, 0, 0, 0],\n [4, 13, 5, 2, 303, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 604, 0, 0, 0, 0, 0],\n [4, 5, 2, 469, 8, 2, 104, 9, 2, 47],\n [3, 2, 47, 1077, 570, 0, 0, 0, 0, 0],\n [4, 2143, 6, 855, 0, 0, 0, 0, 0, 0],\n [4, 426, 9, 265, 5, 257, 2, 341, 0, 0],\n [4, 3, 2, 124, 9, 6, 994, 47, 0, 0],\n [4, 13, 3, 2, 47, 855, 0, 0, 0, 0],\n [3, 2, 13, 9, 2, 155, 2499, 0, 0, 0],\n [4, 3, 2, 4005, 9, 2, 107, 93, 288, 275],\n [4, 13, 3, 2, 55, 7, 2, 107, 0, 0],\n [62, 3, 2, 160, 1959, 18, 2, 538, 0, 0],\n [4, 200, 3, 8, 2, 315, 762, 88, 3, 8],\n [4, 15, 2, 1499, 46, 542, 0, 0, 0, 0],\n [4, 5, 1042, 787, 377, 120, 2, 430, 0, 0],\n [3, 14, 10, 487, 0, 0, 0, 0, 0, 0],\n [4, 15, 53, 97, 8, 104, 0, 0, 0, 0],\n [4, 537, 3, 2, 63, 189, 43, 0, 0, 0],\n [11, 165, 3, 2, 100, 0, 0, 0, 0, 0],\n [3, 2, 19, 7, 13, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 47, 0, 0, 0, 0, 0],\n [3, 14, 621, 43, 246, 221, 117, 209, 0, 0],\n [4, 3, 48, 8, 0, 0, 0, 0, 0, 0],\n [5, 2, 114, 279, 9, 402, 0, 0, 0, 0],\n [3, 6, 10, 240, 195, 0, 0, 0, 0, 0],\n [3, 6, 10, 422, 96, 0, 0, 0, 0, 0],\n [3, 2, 100, 7, 2, 4006, 3011, 77, 417, 0],\n [4, 3, 2, 132, 60, 115, 9, 0, 0, 0],\n [3, 6, 33, 1078, 2144, 0, 0, 0, 0, 0],\n [5, 35, 108, 995, 0, 0, 0, 0, 0, 0],\n [280, 8, 2, 171, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 27, 0, 0, 0],\n [11, 12, 17, 5, 14, 0, 0, 0, 0, 0],\n [3, 2, 34, 80, 26, 56, 0, 0, 0, 0],\n [62, 5, 2, 101, 20, 3012, 0, 0, 0, 0],\n [4, 13, 3, 2, 719, 9, 6, 286, 0, 0],\n [11, 12, 199, 1960, 5, 14, 0, 0, 0, 0],\n [4, 259, 3, 2500, 0, 0, 0, 0, 0, 0],\n [11, 12, 1042, 150, 5, 8, 2, 503, 0, 0],\n [4, 3, 2, 246, 4007, 3013, 9, 2, 1410, 135],\n [3, 14, 10, 171, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 557, 63, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 63, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 203, 222, 0, 0, 0, 0],\n [3, 54, 10, 137, 320, 96, 0, 0, 0, 0],\n [3, 2, 91, 387, 0, 0, 0, 0, 0, 0],\n [3, 48, 20, 10, 190, 0, 0, 0, 0, 0],\n [4, 29, 30, 61, 7, 2, 171, 0, 0, 0],\n [11, 12, 2501, 5, 14, 0, 0, 0, 0, 0],\n [5, 2, 667, 7, 10, 2502, 0, 0, 0, 0],\n [4, 3, 6, 28, 31, 0, 0, 0, 0, 0],\n [11, 12, 17, 42, 6, 288, 4008, 0, 0, 0],\n [4, 1126, 3, 6, 28, 109, 40, 0, 0, 0],\n [7, 4, 32, 9, 1791, 5, 2, 1792, 763, 0],\n [4, 5, 35, 58, 71, 0, 0, 0, 0, 0],\n [4, 25, 9, 36, 3, 2, 187, 2145, 0, 0],\n [4, 15, 2, 34, 441, 198, 77, 1633, 0, 0],\n [88, 5, 126, 2, 53, 0, 0, 0, 0, 0],\n [3, 14, 45, 3014, 8, 2, 66, 0, 0, 0],\n [3, 2, 442, 10, 3015, 442, 0, 0, 0, 0],\n [11, 12, 3016, 5, 350, 0, 0, 0, 0, 0],\n [5, 14, 682, 7, 2, 19, 0, 0, 0, 0],\n [3, 14, 45, 55, 0, 0, 0, 0, 0, 0],\n [3, 14, 204, 7, 2, 91, 0, 0, 0, 0],\n [11, 310, 3, 6, 63, 0, 0, 0, 0, 0],\n [3, 6, 27, 4009, 0, 0, 0, 0, 0, 0],\n [11, 12, 966, 9, 107, 342, 5, 14, 0, 0],\n [11, 12, 172, 5, 7, 2, 94, 0, 0, 0],\n [23, 84, 19, 74, 9, 2, 233, 0, 0, 0],\n [3, 6, 19, 136, 93, 79, 26, 13, 0, 0],\n [4, 304, 3, 2, 906, 0, 0, 0, 0, 0],\n [3, 14, 342, 7, 2, 69, 0, 0, 0, 0],\n [11, 12, 166, 5, 14, 0, 0, 0, 0, 0],\n [3, 2, 16, 20, 318, 0, 0, 0, 0, 0],\n [3, 2, 96, 289, 0, 0, 0, 0, 0, 0],\n [62, 29, 35, 441, 158, 1793, 0, 0, 0, 0],\n [5, 21, 101, 1403, 0, 0, 0, 0, 0, 0],\n [280, 2, 2146, 13, 115, 7, 4010, 9, 318, 0],\n [3, 2, 55, 249, 0, 0, 0, 0, 0, 0],\n [4, 3, 257, 2, 52, 0, 0, 0, 0, 0],\n [3, 6, 10, 746, 52, 149, 0, 0, 0, 0],\n [4, 13, 3, 2, 132, 0, 0, 0, 0, 0],\n [4, 3, 2, 41, 37, 0, 0, 0, 0, 0],\n [3, 2, 1043, 747, 0, 0, 0, 0, 0, 0],\n [11, 12, 831, 5, 14, 0, 0, 0, 0, 0],\n [3, 48, 7, 10, 4011, 1079, 0, 0, 0, 0],\n [4, 3, 6, 337, 58, 71, 0, 0, 0, 0],\n [3, 2, 389, 8, 2, 68, 10, 448, 26, 10],\n [3, 2, 720, 7, 104, 9, 2, 187, 0, 0],\n [4, 175, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 225, 0, 0, 0, 0, 0],\n [3, 14, 57, 8, 93, 268, 4012, 43, 2, 107],\n [11, 12, 17, 5, 109, 570, 81, 2, 229, 0],\n [4, 13, 3, 2, 315, 1634, 0, 0, 0, 0],\n [4, 25, 9, 316, 5, 2, 324, 8, 0, 0],\n [124, 2, 605, 8, 2, 72, 0, 0, 0, 0],\n [88, 3, 189, 6, 270, 0, 0, 0, 0, 0],\n [62, 3, 2, 260, 655, 4013, 0, 0, 0, 0],\n [5, 21, 258, 157, 18, 418, 125, 0, 0, 0],\n [4, 3, 2, 1230, 7, 2, 330, 157, 43, 0],\n [4, 3, 7, 2, 59, 89, 0, 0, 0, 0],\n [4, 25, 9, 41, 3, 6, 0, 0, 0, 0],\n [4, 147, 3, 2, 41, 0, 0, 0, 0, 0],\n [4, 3, 2, 41, 56, 8, 0, 0, 0, 0],\n [3, 22, 701, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 315, 1411, 0, 0, 0, 0],\n [376, 930, 414, 68, 40, 2, 290, 46, 4, 94],\n [7, 51, 228, 3, 2, 996, 907, 472, 0, 0],\n [4, 3, 8, 2, 82, 220, 221, 176, 0, 0],\n [4, 13, 3, 2, 832, 1412, 0, 0, 0, 0],\n [23, 15, 22, 97, 177, 68, 0, 0, 0, 0],\n [4, 3, 8, 2, 86, 0, 0, 0, 0, 0],\n [3, 2, 4014, 8, 2, 997, 7, 998, 0, 0],\n [42, 6, 1500, 1962, 18, 4015, 0, 0, 0, 0],\n [4, 13, 3, 2, 28, 80, 50, 0, 0, 0],\n [5, 2, 17, 4016, 0, 0, 0, 0, 0, 0],\n [4, 3, 167, 18, 2, 364, 9, 49, 0, 0],\n [23, 3, 2, 131, 419, 81, 0, 0, 0, 0],\n [3, 6, 34, 206, 18, 504, 0, 0, 0, 0],\n [42, 30, 61, 2, 702, 0, 0, 0, 0, 0],\n [11, 12, 242, 7, 2, 55, 0, 0, 0, 0],\n [4, 13, 3, 2, 492, 174, 0, 0, 0, 0],\n [4, 1635, 623, 5, 7, 2, 27, 0, 0, 0],\n [3, 2, 52, 279, 764, 0, 0, 0, 0, 0],\n [4, 743, 813, 5, 21, 856, 43, 0, 0, 0],\n [3, 2, 1963, 3017, 2, 133, 0, 0, 0, 0],\n [4, 3, 8, 2, 260, 1413, 0, 0, 0, 0],\n [4, 25, 9, 1080, 3, 48, 301, 0, 0, 0],\n [3, 2, 16, 183, 0, 0, 0, 0, 0, 0],\n [3, 48, 4017, 18, 44, 1636, 0, 0, 0, 0],\n [11, 362, 7, 2, 199, 3, 22, 0, 0, 0],\n [4, 282, 3, 54, 0, 0, 0, 0, 0, 0],\n [15, 2, 586, 42, 354, 18, 38, 3018, 0, 0],\n [23, 3, 2, 339, 7, 2, 91, 0, 0, 0],\n [3, 6, 10, 260, 2147, 0, 0, 0, 0, 0],\n [29, 30, 10, 2503, 7, 2, 91, 0, 0, 0],\n [3, 6, 10, 2148, 4018, 0, 0, 0, 0, 0],\n [11, 12, 703, 145, 5, 14, 0, 0, 0, 0],\n [4, 1166, 293, 111, 6, 266, 0, 0, 0, 0],\n [5, 2, 166, 95, 0, 0, 0, 0, 0, 0],\n [4, 365, 42, 38, 127, 0, 0, 0, 0, 0],\n [4, 3, 2, 100, 20, 0, 0, 0, 0, 0],\n [4, 25, 9, 327, 3, 2, 34, 1044, 0, 0],\n [3, 6, 10, 112, 9, 587, 571, 0, 0, 0],\n [11, 12, 233, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 8, 2, 82, 126, 0, 0],\n [3, 2, 556, 544, 111, 26, 153, 0, 0, 0],\n [23, 3, 2, 63, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 27, 54, 106, 38, 7, 57, 4019],\n [62, 1501, 2, 16, 875, 7, 6, 25, 9, 274],\n [4, 3, 2, 16, 37, 0, 0, 0, 0, 0],\n [3, 2, 16, 301, 10, 169, 0, 0, 0, 0],\n [15, 6, 16, 83, 1127, 0, 0, 0, 0, 0],\n [173, 6, 38, 252, 1502, 0, 0, 0, 0, 0],\n [4, 2504, 3, 8, 2, 46, 0, 0, 0, 0],\n [4, 13, 3, 2, 163, 0, 0, 0, 0, 0],\n [3, 2, 155, 217, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 416, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 833, 0, 0, 0, 0, 0],\n [5, 14, 45, 172, 8, 6, 94, 0, 0, 0],\n [5, 35, 1414, 0, 0, 0, 0, 0, 0, 0],\n [5, 90, 2, 572, 2, 122, 13, 0, 0, 0],\n [11, 12, 1045, 5, 8, 2, 154, 0, 0, 0],\n [4, 25, 9, 121, 3, 7, 2, 27, 0, 0],\n [4, 3, 7, 2, 19, 0, 0, 0, 0, 0],\n [4, 539, 3, 2, 28, 126, 2, 306, 0, 0],\n [11, 12, 102, 721, 5, 14, 0, 0, 0, 0],\n [3, 6, 178, 0, 0, 0, 0, 0, 0, 0],\n [3, 99, 20, 10, 4020, 508, 0, 0, 0, 0],\n [4, 3, 2, 28, 31, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 318, 9, 2, 119, 0, 0],\n [42, 1964, 26, 2149, 4021, 127, 0, 0, 0, 0],\n [4, 3, 2, 100, 37, 8, 18, 0, 0, 0],\n [3, 2, 315, 125, 10, 390, 13, 0, 0, 0],\n [4, 13, 3, 6, 28, 20, 0, 0, 0, 0],\n [3, 14, 342, 8, 2, 132, 0, 0, 0, 0],\n [3, 2, 34, 20, 10, 4022, 0, 0, 0, 0],\n [4, 5, 35, 58, 0, 0, 0, 0, 0, 0],\n [5, 21, 967, 0, 0, 0, 0, 0, 0, 0],\n [5, 2, 263, 178, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 2505, 606, 0, 0, 0, 0, 0],\n [4, 32, 9, 2506, 3, 7, 2, 175, 0, 0],\n [3, 14, 765, 8, 45, 9, 2, 834, 0, 0],\n [15, 6, 155, 83, 422, 0, 0, 0, 0, 0],\n [4, 3, 2, 132, 60, 9, 0, 0, 0, 0],\n [4, 3, 2, 502, 8, 2, 267, 0, 0, 0],\n [5, 14, 409, 8, 2, 86, 0, 0, 0, 0],\n [999, 26, 1637, 33, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 19, 3, 6, 0, 0, 0, 0],\n [88, 5, 7, 2, 267, 0, 0, 0, 0, 0],\n [3, 57, 36, 1000, 0, 0, 0, 0, 0, 0],\n [5, 2, 17, 7, 2, 104, 1128, 607, 1081, 1965],\n [4, 73, 5, 35, 58, 0, 0, 0, 0, 0],\n [11, 12, 287, 5, 67, 0, 0, 0, 0, 0],\n [3, 6, 10, 136, 93, 79, 27, 0, 0, 0],\n [4, 33, 9, 2, 250, 3, 6, 0, 0, 0],\n [4, 3, 135, 40, 2, 87, 9, 2, 536, 856],\n [4, 3, 2, 1129, 814, 9, 344, 103, 0, 0],\n [4, 3, 483, 2, 82, 0, 0, 0, 0, 0],\n [144, 99, 366, 355, 10, 184, 142, 0, 0, 0],\n [3, 14, 10, 186, 7, 2, 211, 0, 0, 0],\n [51, 788, 85, 1638, 493, 0, 0, 0, 0, 0],\n [4, 39, 5, 7, 104, 9, 2, 148, 0, 0],\n [3, 2, 107, 380, 111, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 265, 0, 0, 0, 0, 0],\n [4, 200, 3, 8, 2, 789, 50, 0, 0, 0],\n [4, 3, 2, 236, 811, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 86, 0, 0, 0, 0, 0],\n [4, 5, 2, 305, 31, 0, 0, 0, 0, 0],\n [3, 2, 211, 320, 0, 0, 0, 0, 0, 0],\n [3, 2, 34, 1967, 206, 3019, 0, 0, 0, 0],\n [3, 22, 10, 431, 8, 2, 291, 0, 0, 0],\n [3, 2, 372, 70, 790, 0, 0, 0, 0, 0],\n [3, 48, 432, 43, 2, 142, 0, 0, 0, 0],\n [3, 14, 10, 476, 0, 0, 0, 0, 0, 0],\n [4, 13, 871, 3, 2, 16, 20, 0, 0, 0],\n [11, 12, 147, 3, 2, 1794, 433, 0, 0, 0],\n [4, 3, 2, 124, 9, 2, 537, 8, 2, 492],\n [488, 2, 128, 258, 8, 6, 2507, 235, 857, 766],\n [23, 3, 2, 102, 0, 0, 0, 0, 0, 0],\n [4, 65, 15, 2, 102, 97, 0, 0, 0, 0],\n [3, 14, 121, 7, 6, 19, 934, 57, 668, 0],\n [5, 2, 224, 58, 184, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 121, 634, 0, 0, 0, 0],\n [11, 12, 669, 1503, 5, 405, 0, 0, 0, 0],\n [4, 13, 3, 2, 543, 0, 0, 0, 0, 0],\n [4, 3, 6, 157, 43, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 364, 220, 221, 2, 1131, 0],\n [4, 36, 4023, 2, 49, 7, 2, 2508, 624, 40],\n [11, 12, 834, 5, 70, 0, 0, 0, 0, 0],\n [4, 3, 1415, 71, 2, 148, 0, 0, 0, 0],\n [11, 12, 417, 15, 2, 36, 188, 2, 66, 24],\n [3, 2, 91, 387, 0, 0, 0, 0, 0, 0],\n [3, 2, 670, 748, 108, 111, 26, 419, 153, 0],\n [3, 2, 28, 395, 43, 519, 0, 0, 0, 0],\n [3, 6, 10, 494, 33, 0, 0, 0, 0, 0],\n [3, 1341, 20, 136, 222, 0, 0, 0, 0, 0],\n [3, 14, 45, 176, 0, 0, 0, 0, 0, 0],\n [3, 2, 19, 7, 136, 93, 79, 0, 0, 0],\n [4, 3, 1046, 111, 81, 2, 82, 366, 120, 2],\n [4, 815, 436, 3, 2, 1125, 9, 6, 656, 336],\n [4, 3, 48, 31, 0, 0, 0, 0, 0, 0],\n [4, 32, 9, 816, 3, 8, 2, 66, 0, 0],\n [4, 13, 3, 2, 64, 0, 0, 0, 0, 0],\n [3, 2, 91, 558, 0, 0, 0, 0, 0, 0],\n [23, 5, 2, 321, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 237, 2150, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 222, 8, 2, 903, 1782, 903],\n [11, 12, 263, 5, 325, 2, 229, 0, 0, 0],\n [3, 2, 19, 635, 0, 0, 0, 0, 0, 0],\n [15, 6, 155, 83, 217, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 179, 0, 0, 0, 0, 0],\n [3, 2, 36, 7, 10, 1231, 0, 0, 0, 0],\n [51, 262, 15, 2, 46, 97, 18, 414, 0, 0],\n [4, 25, 9, 190, 3, 2, 16, 20, 0, 0],\n [3, 876, 9, 117, 2509, 0, 0, 0, 0, 0],\n [4, 13, 147, 5, 2, 765, 8, 2, 606, 0],\n [4, 3, 2, 200, 8, 2, 64, 0, 0, 0],\n [3, 6, 209, 56, 8, 10, 64, 0, 0, 0],\n [3, 22, 249, 14, 0, 0, 0, 0, 0, 0],\n [5, 14, 12, 26, 3020, 255, 0, 0, 0, 0],\n [5, 767, 935, 0, 0, 0, 0, 0, 0, 0],\n [173, 2, 65, 38, 4024, 1795, 0, 0, 0, 0],\n [4, 5, 2, 722, 17, 20, 8, 140, 721, 0],\n [3, 22, 139, 0, 0, 0, 0, 0, 0, 0],\n [173, 6, 52, 1001, 7, 2, 55, 244, 0, 0],\n [3, 2, 141, 56, 8, 10, 213, 0, 0, 0],\n [4, 3, 7, 44, 70, 89, 0, 0, 0, 0],\n [3, 2, 94, 350, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 288, 3, 188, 2, 267, 0, 0],\n [4, 3, 2, 28, 189, 7, 2, 91, 0, 0],\n [4, 3, 1047, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 160, 223, 0, 0, 0, 0, 0, 0],\n [4, 29, 654, 559, 2, 75, 2510, 7, 2, 749],\n [4, 13, 5, 77, 179, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 59, 455, 0, 0, 0, 0],\n [4, 282, 3, 2, 16, 37, 0, 0, 0, 0],\n [11, 12, 3021, 5, 8, 2, 373, 0, 0, 0],\n [3, 48, 331, 657, 0, 0, 0, 0, 0, 0],\n [3, 48, 10, 100, 0, 0, 0, 0, 0, 0],\n [3, 48, 20, 10, 768, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 59, 89, 0, 0, 0, 0],\n [3, 6, 10, 211, 0, 0, 0, 0, 0, 0],\n [4, 137, 3, 151, 8, 2, 50, 9, 2, 16],\n [3, 2, 34, 109, 40, 2, 229, 0, 0, 0],\n [4, 6, 100, 3, 873, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 789, 455, 0, 0, 0, 0],\n [3, 2, 34, 8, 2, 435, 68, 9, 2, 98],\n [11, 12, 4025, 15, 2, 100, 24, 7, 77, 704],\n [4, 3, 40, 2, 87, 9, 2, 1796, 0, 0],\n [11, 12, 224, 5, 113, 7, 2, 69, 0, 0],\n [3, 22, 1002, 18, 441, 289, 8, 6, 92, 0],\n [3, 6, 28, 68, 790, 0, 0, 0, 0, 0],\n [3, 6, 28, 10, 3022, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 1956, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 19, 0, 0, 0],\n [23, 5, 2, 1167, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 0, 0, 0, 0, 0],\n [4, 36, 3, 6, 0, 0, 0, 0, 0, 0],\n [3, 22, 936, 0, 0, 0, 0, 0, 0, 0],\n [4, 15, 2, 388, 8, 2, 63, 97, 0, 0],\n [4, 32, 9, 1075, 3, 7, 6, 19, 0, 0],\n [4, 3, 205, 40, 2, 297, 9, 2, 27, 0],\n [4, 200, 3, 8, 2, 86, 0, 0, 0, 0],\n [11, 12, 224, 0, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 49, 3, 60, 7, 57, 361, 110],\n [3, 14, 937, 36, 113, 54, 1048, 10, 154, 0],\n [5, 14, 128, 370, 17, 7, 2, 19, 0, 0],\n [4, 25, 9, 274, 3, 2, 161, 56, 7, 0],\n [4, 32, 9, 495, 3, 8, 2, 241, 0, 0],\n [11, 12, 1504, 5, 14, 0, 0, 0, 0, 0],\n [4, 25, 9, 112, 3, 6, 0, 0, 0, 0],\n [15, 2, 28, 181, 18, 38, 381, 0, 0, 0],\n [15, 6, 452, 181, 320, 0, 0, 0, 0, 0],\n [4, 5, 35, 489, 18, 29, 0, 0, 0, 0],\n [23, 5, 2, 572, 56, 0, 0, 0, 0, 0],\n [15, 6, 138, 1003, 362, 908, 9, 1968, 0, 0],\n [3, 6, 72, 0, 0, 0, 0, 0, 0, 0],\n [4, 5, 35, 189, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 28, 7, 2, 149, 0, 0, 0],\n [11, 12, 298, 5, 14, 0, 0, 0, 0, 0],\n [4, 3, 2, 28, 31, 0, 0, 0, 0, 0],\n [4, 293, 3, 2, 492, 96, 0, 0, 0, 0],\n [4, 3, 8, 87, 9, 2, 1232, 0, 0, 0],\n [4, 13, 3, 2, 2511, 170, 0, 0, 0, 0],\n [4, 76, 3, 8, 2, 104, 87, 9, 2, 53],\n [3, 2, 53, 309, 0, 0, 0, 0, 0, 0],\n [3, 2, 86, 4026, 0, 0, 0, 0, 0, 0],\n [3, 2, 143, 7, 10, 335, 0, 0, 0, 0],\n [4, 644, 3, 2, 16, 20, 0, 0, 0, 0],\n [84, 1416, 157, 18, 293, 2, 993, 0, 0, 0],\n [4, 25, 9, 33, 5, 2, 17, 95, 7, 0],\n [4, 13, 3, 2, 227, 131, 0, 0, 0, 0],\n [5, 2, 323, 3023, 18, 290, 0, 0, 0, 0],\n [23, 3, 2, 79, 636, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 227, 131, 54, 42, 38, 127],\n [3, 2, 131, 1969, 227, 18, 2512, 0, 0, 0],\n [3, 6, 337, 95, 10, 1131, 0, 0, 0, 0],\n [4, 3, 214, 206, 117, 9, 2, 39, 0, 0],\n [3, 22, 10, 139, 92, 0, 0, 0, 0, 0],\n [3, 14, 10, 19, 299, 8, 2, 86, 0, 0],\n [3, 2, 58, 0, 0, 0, 0, 0, 0, 0],\n [3, 14, 176, 7, 2, 98, 0, 0, 0, 0],\n [4, 73, 3, 6, 0, 0, 0, 0, 0, 0],\n [23, 3, 2, 94, 131, 0, 0, 0, 0, 0],\n [4, 251, 3, 1970, 8, 2, 403, 0, 0, 0],\n [7, 4, 259, 5, 35, 58, 0, 0, 0, 0],\n [4, 3, 658, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 1505, 938, 0, 0, 0, 0],\n [3, 6, 41, 108, 18, 2999, 8, 44, 1004, 0],\n [4, 304, 5, 2, 287, 2513, 7, 0, 0, 0],\n [5, 35, 4027, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 130, 484, 0, 0, 0, 0, 0, 0],\n [11, 12, 659, 5, 8, 2, 66, 0, 0, 0],\n [3, 2, 72, 870, 2514, 26, 1639, 0, 0, 0],\n [3, 6, 7, 608, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 211, 0, 0, 0],\n [4, 5, 2, 17, 489, 43, 0, 0, 0, 0],\n [4, 13, 5, 2, 4028, 8, 2, 835, 0, 0],\n [4, 15, 2, 28, 24, 7, 44, 232, 0, 0],\n [3, 54, 10, 1168, 190, 0, 0, 0, 0, 0],\n [4, 13, 3, 44, 190, 0, 0, 0, 0, 0],\n [62, 3, 2, 59, 226, 609, 81, 2, 27, 0],\n [4, 33, 3, 6, 0, 0, 0, 0, 0, 0],\n [11, 12, 243, 298, 5, 14, 7, 2, 19, 0],\n [85, 10, 836, 152, 74, 115, 9, 2, 243, 41],\n [4, 3, 2, 36, 273, 8, 0, 0, 0, 0],\n [3, 6, 49, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 334, 0, 0, 0, 0, 0],\n [3, 14, 545, 8, 2, 72, 0, 0, 0, 0],\n [4, 25, 9, 134, 3, 6, 28, 20, 0, 0],\n [3, 2, 149, 60, 0, 0, 0, 0, 0, 0],\n [11, 12, 858, 5, 7, 2, 19, 0, 0, 0],\n [3, 6, 10, 610, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 53, 7, 2, 19, 0, 0],\n [3, 6, 53, 317, 9, 10, 344, 588, 0, 0],\n [11, 12, 791, 905, 0, 0, 0, 0, 0, 0],\n [11, 12, 473, 5, 2515, 0, 0, 0, 0, 0],\n [4, 5, 2, 258, 7, 2, 199, 0, 0, 0],\n [4, 3, 2, 16, 37, 0, 0, 0, 0, 0],\n [4, 13, 3, 44, 277, 0, 0, 0, 0, 0],\n [4, 5, 21, 17, 20, 0, 0, 0, 0, 0],\n [4, 3, 2, 462, 13, 0, 0, 0, 0, 0],\n [3, 2, 131, 8, 0, 0, 0, 0, 0, 0],\n [3, 2, 999, 909, 0, 0, 0, 0, 0, 0],\n [3, 2, 239, 750, 0, 0, 0, 0, 0, 0],\n [11, 12, 546, 9, 520, 589, 3, 113, 0, 0],\n [5, 14, 1506, 7, 6, 19, 0, 0, 0, 0],\n [4, 13, 3, 2, 3024, 270, 0, 0, 0, 0],\n [5, 21, 333, 26, 1971, 0, 0, 0, 0, 0],\n [3, 6, 10, 422, 47, 0, 0, 0, 0, 0],\n [3, 22, 10, 387, 92, 0, 0, 0, 0, 0],\n [5, 2, 101, 20, 509, 0, 0, 0, 0, 0],\n [4, 251, 3, 6, 0, 0, 0, 0, 0, 0],\n [51, 33, 3, 6, 0, 0, 0, 0, 0, 0],\n [11, 12, 314, 877, 5, 67, 0, 0, 0, 0],\n [4, 33, 84, 6, 74, 7, 0, 0, 0, 0],\n [11, 12, 238, 5, 14, 0, 0, 0, 0, 0],\n [11, 12, 238, 5, 80, 0, 0, 0, 0, 0],\n [4, 73, 3, 6, 0, 0, 0, 0, 0, 0],\n [5, 2, 180, 81, 2, 122, 683, 0, 0, 0],\n [3, 14, 10, 131, 8, 2, 86, 0, 0, 0],\n [4, 275, 9, 2, 611, 5, 35, 7, 0, 0],\n [11, 12, 307, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 224, 20, 4029, 0, 0, 0, 0, 0],\n [3, 2, 16, 423, 18, 4030, 425, 268, 2, 96],\n [4, 25, 9, 114, 5, 7, 2, 69, 0, 0],\n [4, 3, 2, 289, 375, 7, 453, 2, 427, 9],\n [3, 2, 52, 343, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 63, 31, 0, 0, 0, 0, 0],\n [3, 2, 28, 1973, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 390, 1974, 0, 0, 0, 0, 0],\n [4, 135, 3, 2, 16, 37, 7, 44, 89, 0],\n [23, 3, 2, 119, 109, 40, 0, 0, 0, 0],\n [4, 13, 3, 2, 133, 0, 0, 0, 0, 0],\n [5, 117, 9, 2, 1169, 258, 10, 3025, 0, 0],\n [5, 2, 160, 748, 93, 2, 474, 968, 7, 587],\n [3, 6, 10, 363, 73, 0, 0, 0, 0, 0],\n [3, 22, 139, 178, 0, 0, 0, 0, 0, 0],\n [6, 3, 57, 769, 43, 11, 12, 878, 0, 0],\n [5, 14, 114, 7, 6, 130, 0, 0, 0, 0],\n [4, 13, 3, 2, 96, 0, 0, 0, 0, 0],\n [15, 6, 28, 24, 128, 235, 8, 2, 269, 0],\n [4, 13, 3, 2, 420, 7, 104, 9, 2, 52],\n [5, 21, 3026, 8, 10, 723, 0, 0, 0, 0],\n [4, 3, 2, 388, 8, 2, 46, 1975, 0, 0],\n [4, 13, 3, 2, 46, 0, 0, 0, 0, 0],\n [5, 14, 751, 7, 2, 19, 0, 0, 0, 0],\n [15, 48, 441, 10, 792, 190, 0, 0, 0, 0],\n [15, 2, 16, 181, 18, 38, 20, 10, 519, 560],\n [15, 6, 83, 110, 22, 3, 81, 57, 165, 1082],\n [5, 14, 172, 489, 18, 1797, 0, 0, 0, 0],\n [4, 3, 2, 34, 356, 0, 0, 0, 0, 0],\n [29, 30, 368, 6, 3, 10, 561, 262, 18, 1507],\n [4, 3, 2, 34, 7, 105, 31, 0, 0, 0],\n [11, 12, 101, 5, 80, 0, 0, 0, 0, 0],\n [4, 147, 5, 8, 2, 59, 684, 0, 0, 0],\n [4, 13, 3, 2, 1284, 0, 0, 0, 0, 0],\n [3, 2, 239, 347, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 134, 0, 0, 0, 0, 0],\n [4, 73, 5, 35, 58, 0, 0, 0, 0, 0],\n [4, 258, 5, 7, 2, 3027, 0, 0, 0, 0],\n [11, 12, 101, 5, 20, 159, 509, 0, 0, 0],\n [3, 6, 57, 4031, 259, 0, 0, 0, 0, 0],\n [11, 12, 406, 3028, 42, 30, 61, 0, 0, 0],\n [4, 3, 2, 289, 437, 0, 0, 0, 0, 0],\n [3, 2, 233, 276, 218, 26, 198, 2, 742, 0],\n [4, 3, 2, 116, 60, 9, 0, 0, 0, 0],\n [23, 3, 2, 36, 78, 1170, 0, 0, 0, 0],\n [4, 3, 483, 2, 132, 0, 0, 0, 0, 0],\n [5, 14, 597, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 742, 2, 122, 13, 421, 57, 143, 0],\n [3, 2, 119, 183, 206, 95, 0, 0, 0, 0],\n [15, 2, 1285, 1132, 2, 162, 235, 0, 0, 0],\n [23, 3, 2, 41, 343, 0, 0, 0, 0, 0],\n [11, 12, 114, 5, 113, 8, 2, 68, 146, 9],\n [4, 36, 3, 67, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 476, 9, 10, 213, 8, 2, 170],\n [3, 2, 200, 8, 2, 86, 264, 216, 413, 26],\n [4, 13, 3, 2, 547, 573, 0, 0, 0, 0],\n [11, 12, 233, 0, 0, 0, 0, 0, 0, 0],\n [3, 6, 7, 608, 0, 0, 0, 0, 0, 0],\n [11, 144, 6, 28, 397, 111, 103, 0, 0, 0],\n [4, 3, 76, 3029, 31, 0, 0, 0, 0, 0],\n [4, 15, 2, 724, 64, 542, 0, 0, 0, 0],\n [11, 12, 303, 5, 8, 2, 170, 0, 0, 0],\n [11, 12, 622, 5, 151, 0, 0, 0, 0, 0],\n [4, 129, 282, 3, 7, 6, 266, 0, 0, 0],\n [106, 2, 623, 1798, 212, 6, 4032, 24, 45, 2516],\n [4, 3, 205, 8, 2, 174, 0, 0, 0, 0],\n [15, 6, 187, 24, 1976, 9, 4034, 0, 0, 0],\n [3, 22, 281, 178, 0, 0, 0, 0, 0, 0],\n [3, 6, 28, 246, 216, 18, 97, 4035, 4036, 26],\n [4, 13, 306, 15, 48, 24, 0, 0, 0, 0],\n [144, 2, 28, 504, 0, 0, 0, 0, 0, 0],\n [5, 196, 9, 21, 17, 95, 0, 0, 0, 0],\n [5, 14, 45, 831, 8, 2, 210, 0, 0, 0],\n [4, 25, 9, 141, 3, 54, 0, 0, 0, 0],\n [4, 13, 3, 2, 176, 0, 0, 0, 0, 0],\n [5, 14, 128, 101, 7, 6, 19, 0, 0, 0],\n [4, 3, 2, 364, 120, 2, 474, 0, 0, 0],\n [4, 5, 35, 296, 49, 115, 9, 7, 2, 3030],\n [4, 3, 2, 369, 9, 2, 41, 0, 0, 0],\n [4, 3, 2, 41, 356, 0, 0, 0, 0, 0],\n [23, 5, 35, 0, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 3031, 1508, 7, 2, 27, 0, 0],\n [4, 251, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 245, 2, 195, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 87, 9, 2, 1075, 0, 0],\n [3, 2, 175, 562, 1417, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 64, 0, 0, 0, 0, 0],\n [4, 39, 5, 127, 0, 0, 0, 0, 0, 0],\n [84, 425, 299, 7, 2, 87, 68, 443, 0, 0],\n [5, 35, 939, 93, 2151, 0, 0, 0, 0, 0],\n [62, 3, 2, 187, 7, 79, 2152, 8, 44, 1418],\n [4, 32, 9, 129, 282, 3, 78, 528, 0, 0],\n [11, 165, 5, 2, 166, 0, 0, 0, 0, 0],\n [3, 117, 9, 2, 166, 109, 40, 2, 229, 0],\n [3, 6, 33, 217, 0, 0, 0, 0, 0, 0],\n [11, 12, 637, 5, 14, 0, 0, 0, 0, 0],\n [3, 2, 186, 8, 26, 268, 0, 0, 0, 0],\n [3, 6, 521, 137, 320, 0, 0, 0, 0, 0],\n [4, 15, 2, 685, 24, 205, 8, 22, 0, 0],\n [4, 13, 5, 2, 298, 2518, 0, 0, 0, 0],\n [3, 2, 41, 8, 2, 68, 381, 0, 0, 0],\n [11, 12, 184, 2153, 5, 14, 0, 0, 0, 0],\n [11, 12, 305, 5, 7, 6, 130, 0, 0, 0],\n [3, 6, 10, 183, 4038, 0, 0, 0, 0, 0],\n [11, 12, 598, 5, 7, 6, 19, 0, 0, 0],\n [4, 3, 2, 154, 31, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 315, 134, 0, 0, 0, 0],\n [3, 6, 28, 56, 8, 2, 612, 0, 0, 0],\n [4, 76, 9, 39, 5, 80, 8, 2, 70, 146],\n [11, 12, 2519, 5, 60, 67, 0, 0, 0, 0],\n [4, 645, 9, 505, 3, 2, 161, 7, 0, 0],\n [4, 3, 2, 161, 31, 0, 0, 0, 0, 0],\n [11, 12, 2154, 2155, 2, 590, 0, 0, 0, 0],\n [24, 220, 17, 152, 7, 2, 219, 0, 0, 0],\n [11, 12, 341, 42, 38, 127, 7, 2, 219, 0],\n [4, 32, 9, 96, 3, 54, 0, 0, 0, 0],\n [3, 14, 45, 2514, 0, 0, 0, 0, 0, 0],\n [3, 6, 1640, 2156, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 34, 31, 56, 8, 2, 82, 0],\n [4, 3, 8, 2, 338, 0, 0, 0, 0, 0],\n [4, 5, 21, 101, 95, 0, 0, 0, 0, 0],\n [51, 228, 3, 2, 121, 1046, 0, 0, 0, 0],\n [5, 21, 3032, 0, 0, 0, 0, 0, 0, 0],\n [4, 32, 9, 63, 0, 0, 0, 0, 0, 0],\n [3, 2, 28, 20, 10, 3033, 0, 0, 0, 0],\n [4, 65, 9, 92, 3, 22, 0, 0, 0, 0],\n [5, 14, 45, 17, 0, 0, 0, 0, 0, 0],\n [4, 3034, 3, 2, 155, 267, 0, 0, 0, 0],\n [11, 12, 321, 5, 7, 2, 91, 0, 0, 0],\n [3, 14, 57, 1799, 0, 0, 0, 0, 0, 0],\n [4, 5, 2, 101, 31, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 4039, 0, 0, 0, 0, 0],\n [3, 6, 966, 4040, 0, 0, 0, 0, 0, 0],\n [3, 6, 1005, 26, 1978, 0, 0, 0, 0, 0],\n [3, 2, 16, 837, 0, 0, 0, 0, 0, 0],\n [11, 3, 99, 1641, 0, 0, 0, 0, 0, 0],\n [4, 192, 21, 1286, 4041, 574, 3006, 110, 1083, 35],\n [4, 32, 9, 174, 3, 7, 2, 55, 0, 0],\n [3, 2, 389, 448, 26, 431, 0, 0, 0, 0],\n [3, 6, 133, 1133, 599, 0, 0, 0, 0, 0],\n [11, 12, 879, 3, 14, 0, 0, 0, 0, 0],\n [3, 2, 34, 7, 703, 1006, 357, 0, 0, 0],\n [3, 48, 31, 10, 3035, 0, 0, 0, 0, 0],\n [4, 121, 3, 7, 2, 209, 0, 0, 0, 0],\n [85, 164, 1123, 95, 6, 266, 686, 0, 0, 0],\n [3, 2, 407, 296, 230, 7, 4042, 0, 0, 0],\n [4, 15, 2, 46, 542, 0, 0, 0, 0, 0],\n [4, 25, 9, 72, 3, 6, 0, 0, 0, 0],\n [23, 3, 2, 72, 81, 0, 0, 0, 0, 0],\n [11, 12, 104, 328, 29, 30, 61, 7, 329, 215],\n [3, 14, 2, 4043, 43, 10, 4044, 0, 0, 0],\n [5, 21, 17, 10, 600, 0, 0, 0, 0, 0],\n [5, 2, 17, 58, 10, 73, 0, 0, 0, 0],\n [3, 14, 10, 406, 1007, 0, 0, 0, 0, 0],\n [4, 3, 2, 52, 273, 8, 0, 0, 0, 0],\n [4, 39, 5, 67, 0, 0, 0, 0, 0, 0],\n [3, 6, 47, 2520, 0, 0, 0, 0, 0, 0],\n [3, 2, 671, 8, 0, 0, 0, 0, 0, 0],\n [11, 3, 6, 496, 880, 18, 4045, 1642, 784, 115],\n [4, 13, 50, 15, 2, 16, 24, 8, 0, 0],\n [4, 13, 134, 3, 2, 28, 8, 2, 70, 20],\n [5, 35, 3036, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 171, 168, 0, 0, 0, 0, 0, 0],\n [3, 14, 208, 7, 2, 27, 0, 0, 0, 0],\n [4, 304, 15, 6, 66, 181, 18, 38, 60, 81],\n [23, 3, 2, 459, 0, 0, 0, 0, 0, 0],\n [3, 6, 16, 1050, 0, 0, 0, 0, 0, 0],\n [4, 25, 9, 408, 5, 120, 2, 316, 0, 0],\n [5, 2, 17, 7, 2, 69, 4046, 0, 0, 0],\n [11, 12, 39, 5, 7, 6, 27, 0, 0, 0],\n [4, 36, 3, 6, 0, 0, 0, 0, 0, 0],\n [3, 14, 45, 171, 7, 2, 33, 0, 0, 0],\n [3, 14, 208, 7, 2, 33, 0, 0, 0, 0],\n [4, 13, 3, 2, 170, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 0, 0, 0, 0, 0],\n [5, 2, 101, 3037, 18, 2, 831, 0, 0, 0],\n [4, 13, 3, 2, 575, 9, 2, 250, 0, 0],\n [5, 14, 247, 7, 6, 19, 0, 0, 0, 0],\n [5, 2, 162, 20, 3038, 0, 0, 0, 0, 0],\n [4, 73, 5, 35, 58, 0, 0, 0, 0, 0],\n [4, 3, 2, 445, 116, 120, 2, 17, 157, 43],\n [3, 6, 817, 351, 0, 0, 0, 0, 0, 0],\n [3, 6, 187, 1287, 153, 0, 0, 0, 0, 0],\n [11, 165, 3, 2, 34, 0, 0, 0, 0, 0],\n [3, 6, 10, 793, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 2521, 3039, 0, 0, 0, 0],\n [11, 12, 247, 5, 14, 0, 0, 0, 0, 0],\n [3, 6, 10, 700, 16, 8, 10, 215, 0, 0],\n [4, 15, 6, 59, 881, 0, 0, 0, 0, 0],\n [4, 3, 910, 8, 2, 163, 0, 0, 0, 0],\n [11, 12, 207, 5, 113, 7, 6, 98, 0, 0],\n [15, 2, 16, 24, 8, 10, 134, 0, 0, 0],\n [3, 14, 10, 240, 195, 7, 6, 27, 0, 0],\n [11, 12, 770, 5, 463, 8, 0, 0, 0, 0],\n [3, 14, 208, 540, 0, 0, 0, 0, 0, 0],\n [280, 2, 194, 110, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 416, 0, 0, 0, 0, 0],\n [5, 21, 1288, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 241, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 86, 0, 0, 0, 0, 0],\n [4, 200, 3, 8, 2, 646, 0, 0, 0, 0],\n [11, 12, 17, 5, 56, 153, 0, 0, 0, 0],\n [3, 14, 10, 102, 8, 2, 449, 0, 0, 0],\n [3, 6, 3040, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 203, 277, 0, 0, 0, 0],\n [4, 33, 3, 613, 0, 0, 0, 0, 0, 0],\n [3, 2, 171, 394, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 604, 0, 0, 0, 0, 0],\n [4, 32, 9, 53, 3, 22, 0, 0, 0, 0],\n [4, 369, 9, 41, 3, 6, 0, 0, 0, 0],\n [11, 12, 39, 5, 273, 153, 0, 0, 0, 0],\n [29, 30, 61, 45, 114, 7, 2, 27, 0, 0],\n [11, 12, 3041, 5, 14, 0, 0, 0, 0, 0],\n [4, 15, 2, 630, 1051, 0, 0, 0, 0, 0],\n [3, 2, 16, 7, 2, 55, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 69, 0, 0, 0],\n [4, 25, 9, 142, 3, 54, 0, 0, 0, 0],\n [144, 2, 34, 366, 2522, 95, 0, 0, 0, 0],\n [88, 3, 37, 2, 1337, 0, 0, 0, 0, 0],\n [4, 3, 2, 34, 31, 71, 2, 497, 93, 507],\n [11, 12, 4047, 5, 7, 2, 199, 0, 0, 0],\n [23, 5, 35, 0, 0, 0, 0, 0, 0, 0],\n [3, 22, 206, 18, 705, 0, 0, 0, 0, 0],\n [3, 2, 52, 8, 10, 260, 1233, 0, 0, 0],\n [3, 2, 34, 20, 156, 0, 0, 0, 0, 0],\n [4, 25, 9, 179, 3, 2, 119, 20, 0, 0],\n [4, 13, 3, 2, 315, 50, 0, 0, 0, 0],\n [3, 2, 16, 1008, 26, 833, 2, 256, 0, 0],\n [5, 14, 45, 350, 156, 0, 0, 0, 0, 0],\n [3, 2, 1963, 448, 26, 431, 0, 0, 0, 0],\n [3, 2, 52, 80, 8, 425, 0, 0, 0, 0],\n [11, 12, 39, 0, 0, 0, 0, 0, 0, 0],\n [23, 3, 2, 102, 0, 0, 0, 0, 0, 0],\n [4, 32, 9, 114, 5, 405, 7, 6, 19, 0],\n [4048, 1509, 26, 4049, 4050, 0, 0, 0, 0, 0],\n [3, 6, 16, 1050, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 16, 95, 0, 0, 0, 0, 0],\n [3, 14, 339, 419, 429, 2, 171, 0, 0, 0],\n [4, 13, 5, 2, 358, 0, 0, 0, 0, 0],\n [3, 6, 130, 74, 230, 40, 311, 26, 940, 2],\n [3, 2, 19, 7, 13, 0, 0, 0, 0, 0],\n [4, 25, 9, 1080, 3, 6, 0, 0, 0, 0],\n [4, 5, 2, 136, 93, 193, 377, 8, 2, 687],\n [4, 25, 9, 304, 1979, 382, 2, 129, 2157, 0],\n [3, 6, 182, 8, 10, 933, 272, 0, 0, 0],\n [3, 2, 194, 510, 0, 0, 0, 0, 0, 0],\n [3, 54, 10, 521, 54, 2, 16, 3, 37, 0],\n [3, 2, 94, 350, 0, 0, 0, 0, 0, 0],\n [4, 436, 3, 10, 1125, 9, 6, 336, 0, 0],\n [3, 6, 10, 184, 1419, 7, 4051, 0, 0, 0],\n [3, 2, 1009, 4052, 0, 0, 0, 0, 0, 0],\n [11, 12, 39, 7, 6, 27, 0, 0, 0, 0],\n [3, 2, 339, 760, 8, 2, 4053, 0, 0, 0],\n [4, 1342, 5, 8, 2, 86, 0, 0, 0, 0],\n [4, 3, 2, 28, 80, 8, 0, 0, 0, 0],\n [11, 357, 3, 2, 240, 195, 0, 0, 0, 0],\n [4, 13, 3, 2, 240, 195, 0, 0, 0, 0],\n [5, 90, 2, 1234, 2, 122, 548, 0, 0, 0],\n [4, 5, 2, 17, 80, 212, 0, 0, 0, 0],\n [3, 2, 107, 380, 111, 0, 0, 0, 0, 0],\n [3, 14, 10, 266, 8, 10, 266, 0, 0, 0],\n [4, 25, 9, 49, 3, 67, 0, 0, 0, 0],\n [51, 16, 85, 1289, 2523, 0, 0, 0, 0, 0],\n [62, 3, 2, 53, 309, 0, 0, 0, 0, 0],\n [23, 3, 2, 1643, 18, 1172, 2, 107, 0, 0],\n [4, 13, 3, 2, 64, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 318, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 364, 8, 2, 66],\n [4, 13, 3, 6, 315, 125, 0, 0, 0, 0],\n [4, 25, 9, 121, 3, 8, 2, 138, 0, 0],\n [4, 3, 8, 2, 59, 177, 0, 0, 0, 0],\n [3, 6, 16, 20, 179, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 156, 0, 0, 0, 0, 0],\n [3, 48, 31, 10, 771, 838, 0, 0, 0, 0],\n [3, 6, 10, 363, 336, 0, 0, 0, 0, 0],\n [5, 21, 422, 941, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 66, 0, 0, 0, 0, 0],\n [3, 6, 10, 2524, 43, 4054, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 0, 0, 0, 0, 0],\n [23, 3, 2525, 1134, 0, 0, 0, 0, 0, 0],\n [3, 2, 19, 371, 0, 0, 0, 0, 0, 0],\n [3, 48, 20, 10, 818, 0, 0, 0, 0, 0],\n [3, 2, 118, 273, 8, 882, 0, 0, 0, 0],\n [3, 2, 186, 8, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 116, 0, 0, 0, 0, 0],\n [11, 12, 831, 5, 67, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 20, 1010, 1135, 0, 0, 0],\n [4, 13, 3, 2, 86, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 37, 7, 44, 68, 89, 0],\n [85, 208, 296, 10, 836, 9, 2, 72, 686, 0],\n [3, 14, 10, 660, 7, 2, 27, 0, 0, 0],\n [3, 14, 45, 688, 113, 7, 2, 794, 0, 0],\n [11, 12, 39, 5, 7, 6, 19, 0, 0, 0],\n [11, 12, 39, 5, 67, 0, 0, 0, 0, 0],\n [3, 6, 2, 2522, 9, 10, 591, 0, 0, 0],\n [4, 13, 3, 2, 86, 0, 0, 0, 0, 0],\n [4, 15, 2, 46, 1011, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 56, 40, 911, 7, 6, 33],\n [11, 12, 17, 5, 7, 2, 19, 0, 0, 0],\n [4, 3, 2, 34, 296, 10, 19, 9, 0, 0],\n [5, 2, 263, 95, 176, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 213, 1235, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 2526, 46, 0, 0],\n [4, 13, 3, 2, 116, 0, 0, 0, 0, 0],\n [4, 76, 42, 30, 61, 1173, 0, 0, 0, 0],\n [3, 6, 57, 1980, 211, 0, 0, 0, 0, 0],\n [3, 14, 131, 760, 8, 2, 225, 0, 0, 0],\n [4, 5, 2, 529, 997, 7, 2, 383, 43, 0],\n [4, 13, 3, 2, 244, 0, 0, 0, 0, 0],\n [3, 6, 10, 4055, 2158, 266, 0, 0, 0, 0],\n [3, 2, 63, 7, 1236, 0, 0, 0, 0, 0],\n [4, 3, 2, 381, 16, 31, 0, 0, 0, 0],\n [62, 3, 10, 752, 7, 2, 174, 0, 0, 0],\n [3, 2, 65, 9, 92, 2527, 117, 1420, 9, 1421],\n [3, 2, 41, 183, 0, 0, 0, 0, 0, 0],\n [3, 22, 139, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 1644, 8, 2, 86, 0, 0, 0, 0],\n [4, 65, 3, 614, 8, 2, 102, 0, 0, 0],\n [4, 13, 3, 2, 142, 0, 0, 0, 0, 0],\n [4, 3, 2, 201, 60, 9, 0, 0, 0, 0],\n [4, 5, 2, 101, 37, 0, 0, 0, 0, 0],\n [4, 3, 205, 8, 87, 9, 2, 290, 46, 0],\n [3, 54, 10, 2528, 149, 0, 0, 0, 0, 0],\n [3, 22, 362, 1422, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 124, 9, 2, 63, 0, 0, 0],\n [4, 3, 2, 76, 8, 2, 404, 0, 0, 0],\n [4, 15, 2, 63, 24, 205, 8, 391, 433, 0],\n [3, 6, 10, 41, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 8, 2, 210, 0, 0, 0],\n [4, 3, 2, 129, 49, 0, 0, 0, 0, 0],\n [3, 6, 10, 2529, 102, 0, 0, 0, 0, 0],\n ...]"}, "metadata": {}}]}, {"cell_type": "code", "source": "#convert word to idx", "metadata": {}, "execution_count": 94, "outputs": []}, {"cell_type": "code", "source": "x = [(idx,len(item)) for idx,item in enumerate(train_questions)]", "metadata": {}, "execution_count": 95, "outputs": []}, {"cell_type": "markdown", "source": "## 3.\u8bad\u7ec3\u8fc7\u7a0b", "metadata": {}}, {"cell_type": "markdown", "source": "### 3.1 \u8d85\u53c2\u6570\u8bbe\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "#from easydict import EasyDict as edict\noptions = OrderedDict()\n# data related\noptions['data_path'] = './data/'\n#options['feature_file'] = 'trainval_feat.h5'\n#options['expt_folder'] = 'expt_1'\noptions['model_name'] = 'imageqa'\noptions['train_split'] = 'trainval1'\noptions['val_split'] = 'val2'\noptions['shuffle'] = True\noptions['reverse'] = True\noptions['sample_answer'] = True\n\noptions['num_region'] = 196\noptions['region_dim'] = 512\n\noptions['n_words'] = 13746\noptions['n_output'] = 1000\n\n# structure options\noptions['combined_num_mlp'] = 1\noptions['combined_mlp_drop_0'] = True\noptions['combined_mlp_act_0'] = 'linear'\noptions['sent_drop'] = False\noptions['use_tanh'] = False\n\noptions['use_attention_drop'] = False\n\n# dimensions\noptions['n_emb'] = 500\noptions['n_dim'] = 1024\noptions['n_image_feat'] = options['region_dim']\noptions['n_common_feat'] = 500\noptions['n_attention'] = 512\n\n# initialization\noptions['init_type'] = 'uniform'\noptions['range'] = 0.01\noptions['std'] = 0.01\noptions['init_lstm_svd'] = False\n\noptions['forget_bias'] = np.float32(1.0)\n\n# learning parameters\noptions['optimization'] = 'sgd' # choices\noptions['batch_size'] = 100\noptions['lr'] = np.float32(0.05)\noptions['w_emb_lr'] = np.float32(80)\noptions['momentum'] = np.float32(0.9)\noptions['gamma'] = 1\noptions['step'] = 10\noptions['step_start'] = 100\noptions['max_epochs'] = 50\noptions['weight_decay'] = 0.0005\noptions['decay_rate'] = np.float32(0.999)\noptions['drop_ratio'] = np.float32(0.5)\noptions['smooth'] = np.float32(1e-8)\noptions['grad_clip'] = np.float32(0.1)\n\n# log params\noptions['disp_interval'] = 10\noptions['eval_interval'] = 1000\noptions['save_interval'] = 500\n\n#new\noptions['dict_size'] = 6620", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend', device_id=0)", "metadata": {}, "execution_count": 11, "outputs": []}, {"cell_type": "markdown", "source": "### 3.2\u6a21\u578b\u642d\u5efa", "metadata": {}}, {"cell_type": "code", "source": "def init_weight(n, d, options):\n    ''' initialize weight matrix\n    options['init_type'] determines\n    gaussian or uniform initlizaiton\n    '''\n    if options['init_type'] == 'gaussian':\n        return (np.random.randn(n, d).astype(floatX)) * options['std']\n    elif options['init_type'] == 'uniform':\n        # [-range, range]\n        return ((np.random.rand(n, d) * 2 - 1) * \\\n                options['range']).astype(floatX)\ndef ortho_weight(ndim):\n    \"\"\"\n    Random orthogonal weights, we take\n    the right matrix in the SVD.\n\n    Remember in SVD, u has the same # rows as W\n    and v has the same # of cols as W. So we\n    are ensuring that the rows are\n    orthogonal.\n    \"\"\"\n    W = np.random.randn(ndim, ndim)\n    u, _, _ = np.linalg.svd(W)\n    return u.astype('float32')\n\ndef init_fflayer(params, nin, nout, options, prefix='ff'):\n    ''' initialize ff layer\n    '''\n    params[prefix + '_w'] = init_weight(nin, nout, options)\n    params[prefix + '_b'] = np.zeros(nout, dtype='float32')\n    return params\n\ndef init_lstm_layer(params, nin, ndim, options, prefix='lstm'):\n    ''' initializt lstm layer\n    '''\n    params[prefix + '_w_x'] = init_weight(nin, 4 * ndim, options)\n    # use svd trick to initializ\n    if options['init_lstm_svd']:\n        params[prefix + '_w_h'] = np.concatenate([ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim)],\n                                                 axis=1)\n    else:\n        params[prefix + '_w_h'] = init_weight(ndim, 4 * ndim, options)\n    params[prefix + '_b_h'] = np.zeros(4 * ndim, dtype='float32')\n    # set forget bias to be positive\n    params[prefix + '_b_h'][ndim : 2*ndim] = np.float32(options.get('forget_bias', 0))\n    return params\n\n# initialize the parmaters\ndef init_params(options):\n    ''' Initialize all the parameters\n    '''\n    params = OrderedDict()\n    n_words = options['n_words']\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    n_image_feat = options['n_image_feat']\n    n_common_feat = options['n_common_feat']\n    n_output = options['n_output']\n    n_attention = options['n_attention']\n\n    params['w_emb'] = ((np.random.rand(n_words, n_emb) * 2 - 1) * 0.5).astype(floatX)\n\n    params = init_fflayer(params, n_image_feat, n_dim, options,\n                          prefix='image_mlp')\n\n    # attention model based parameters\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_1')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_2')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_2')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_2')\n\n\n    # params for sentence image mlp\n    for i in range(options['combined_num_mlp']):\n        if i == 0 and options['combined_num_mlp'] == 1:\n            params = init_fflayer(params, n_dim, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == 0 and options['combined_num_mlp'] != 1:\n            params = init_fflayer(params, n_dim, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == options['combined_num_mlp'] - 1 :\n            params = init_fflayer(params, n_common_feat, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        else:\n            params = init_fflayer(params, n_common_feat, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n\n    # lstm layer\n    params = init_lstm_layer(params, n_emb, n_dim, options, prefix='sent_lstm')\n\n    return params\n\ndef init_shared_params(params):\n    ''' return a shared version of all parameters\n    '''\n    global shared_params\n    shared_params = OrderedDict()\n    for k, p in params.items():\n        shared_params[k] = params[k]\n\n    return shared_params\n\ndef get_lr(options, curr_epoch):\n    if options['optimization'] == 'sgd':\n        power = max((curr_epoch - options['step_start']) / options['step'], 0)\n        power = math.ceil(power)\n        return options['lr'] * (options['gamma'] ** power)  #\n    else:\n        return options['lr']", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "def lstm_layer(shared_params, x, mask, h_0, c_0, options, prefix='lstm'):\n    ''' lstm layer:\n    :param shared_params: shared parameters\n    :param x: input, T x batch_size x n_emb\n    :param mask: mask for x, T x batch_size\n    '''\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    # weight matrix for x, n_emb x 4*n_dim (ifoc)\n    lstm_w_x = shared_params[prefix + '_w_x']\n    # weight matrix for h, n_dim x 4*n_dim\n    lstm_w_h = shared_params[prefix + '_w_h']\n    lstm_b_h = shared_params[prefix + '_b_h']\n    h_0 = h_0[:x.shape[1]]\n    c_0 = c_0[:x.shape[1]]\n    question_net = LSTM(n_emb, n_dim)\n    output, (h, c) = question_net(x, (h_0, c_0))\n    return h, c", "metadata": {}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": "def build_model(shared_params, options):\n    #input_idx = Tensor.imatrix('input_idx')\n    input_idx = Tensor()\n    global empty_word\n    empty_word = np.zeros((1, options['n_emb']), dtype='float32')\n    w_emb_extend = Tensor.concatenate([empty_word, shared_params['w_emb']],\n                                 axis=0)\n    input_emb = w_emb_extend[input_idx]\n    \n    # get the transformed image feature\n    global h_0, c_0\n    h_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    c_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    h_encode, c_encode = lstm_layer(shared_params, input_emb, input_mask,\n                                    h_0, c_0, options, prefix='sent_lstm')\n    return h_encodem, c_encode", "metadata": {}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "floatX = np.float32\nbatch_size = options['batch_size']\nmax_epochs = options['max_epochs']\n\n###############\n# build model #\n###############\nparams = init_params(options)\nshared_params = init_shared_params(params)", "metadata": {}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "input_idx = np.ones((6618,100),dtype = 'int32')\nshared_params['w_emb'] = ((np.random.rand(13746, 500) * 2 - 1) * 0.5).astype(floatX)\nempty_word = np.zeros((1, 500), dtype='float32')\nw_emb_extend = shared_params['w_emb']\ninput_emb = w_emb_extend[input_idx]", "metadata": {}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "class LSTM(nn.Cell):\n    def __init__(self, options, is_training=True):\n        super(LSTM, self).__init__()\n        if is_training:\n            self.batch_size = options['batch_size']\n        else:\n            self.batch_size = 1\n            \n        self.n_dim = options['n_dim']\n        self.n_emb = options['n_emb']\n        self.dropout = options['drop_ratio']\n        \n        # TODO\n        self.h = Tensor(np.zeros((1,self.batch_size, self.n_dim), dtype='float32'))\n        self.c = Tensor(np.zeros((1,self.batch_size, self.n_dim), dtype='float32'))\n        \n        self.rnn = nn.LSTM(self.n_emb,self.n_dim,1,True,True,self.dropout)\n        #self.cast = P.Cast()\n\n    def construct(self, x):\n        #x = self.cast(x, mstype.float16)\n        output,(h1,c1) = self.rnn(x, (self.h,self.c))\n        return output,(h1,c1)\n\nclass Question(nn.Cell):\n    def __init__(self, options, is_training=True):\n        super(Question, self).__init__()\n        #dict_size(vocab_size)\n        self.dict_size = options['dict_size']\n        #n_dim (hidden_size)\n        self.n_dim = options['n_dim']\n        self.n_emb = options['n_emb']\n        \n        if is_training:\n            self.batch_size = options['batch_size']\n        else:\n            self.batch_size = 1\n\n        #self.trans = P.Transpose()\n        #self.perm = (1, 0, 2)\n        \n        #HIGHLIGHT \u7b2c\u4e8c\u4e2a\u53c2\u6570n_dim -> n_emb\n        self.embedding = nn.Embedding(self.dict_size, self.n_emb)\n        #?\n        self.lstm = LSTM(options, is_training=is_training).to_float(mstype.float16)\n        #self.h = Tensor(np.zeros((self.batch_size, self.n_dim)).astype(np.float16))\n        #self.c = Tensor(np.zeros((self.batch_size, self.n_dim)).astype(np.float16))\n\n    def construct(self, question_input):\n        embeddings = self.embedding(question_input)\n        #embeddings = self.trans(embeddings, self.perm)\n        output, (hn,cn) = self.lstm(embeddings)\n        return output, hn, cn\n", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "def fflayer(shared_params, x, options, prefix='ff', act_func='tanh'):\n    ''' fflayer: multiply weight then add bias\n    '''\n    return nn.Tanh(mindspore.ops.dot(x, shared_params[prefix + '_w']) +\n                          shared_params[prefix + '_b'])\n\nclass VQA(nn.Cell):\n    def __init__(self, config, is_train=True):\n        super(Seq2Seq, self).__init__()\n        self.max_len = config.max_seq_length\n        self.is_train = is_train\n\n        #self.encoder = Encoder(config, is_train)\n        #self.decoder = Decoder(config, is_train)\n        self.expanddims = P.ExpandDims()\n        self.squeeze = P.Squeeze(axis=0)\n        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n        self.concat = P.Concat(axis=1)\n        self.concat2 = P.Concat(axis=0)\n        self.select = P.Select()\n        self.softmax = nn.Softmax()\n        \n        \n        ##### my #####\n        self.question = Question(options, is_train)\n    \n    \n    def construct(self, src, dst):\n        ### TODO:get image_feat\n        \n        output,h_encode,c_encode = self.question(src)\n        h_encode = h_encode[0][-1]\n        \n        image_feat_down = fflayer(shared_params, image_feat, options,\n                             prefix='image_mlp',\n                              act_func=options.get('image_mlp_act',\n                                                   'tanh'))\n\n        image_feat_attention_1 = fflayer(shared_params, image_feat_down, options,\n                                         prefix='image_att_mlp_1',\n                                         act_func=options.get('image_att_mlp_act',\n                                                              'tanh'))\n        \n        h_encode_attention_1 = fflayer(shared_params, h_encode, options,\n                                       prefix='sent_att_mlp_1',\n                                       act_func=options.get('sent_att_mlp_act',\n                                                            'tanh'))  #\n        combined_feat_attention_1 = image_feat_attention_1 + \\\n                                    h_encode_attention_1[:, None, :]\n        \n        ###\u6682\u65f6\u4e0d\u7ba1\n        #if options['use_attention_drop']:\n            #combined_feat_attention_1 = dropout_layer(combined_feat_attention_1,\n                                                      #dropout, trng, drop_ratio)\n            \n        combined_feat_attention_1 = fflayer(shared_params,\n                                            combined_feat_attention_1, options,\n                                            prefix='combined_att_mlp_1',\n                                            act_func=options.get(\n                                                'combined_att_mlp_act',\n                                                'tanh'))\n        \n        prob_attention_1 = self.softmax(combined_feat_attention_1[:, :, 0])\n        image_feat_ave_1 = (prob_attention_1[:, :, None] * image_feat_down).sum(axis=1)\n\n        combined_hidden_1 = image_feat_ave_1 + h_encode\n        \n        # second layer attention model\n        image_feat_attention_2 = fflayer(shared_params, image_feat_down, options,\n                                         prefix='image_att_mlp_2',\n                                         act_func=options.get('image_att_mlp_act',\n                                                              'tanh'))\n        h_encode_attention_2 = fflayer(shared_params, combined_hidden_1, options,\n                                       prefix='sent_att_mlp_2',\n                                       act_func=options.get('sent_att_mlp_act',\n                                                            'tanh'))\n        combined_feat_attention_2 = image_feat_attention_2 + \\\n                                    h_encode_attention_2[:, None, :]\n        \n        ### \u6682\u65f6\u4e0d\u505a\n        #if options['use_attention_drop']:\n            #combined_feat_attention_2 = dropout_layer(combined_feat_attention_2,\n                                                      #dropout, trng, drop_ratio)\n\n        combined_feat_attention_2 = fflayer(shared_params,\n                                            combined_feat_attention_2, options,\n                                            prefix='combined_att_mlp_2',\n                                            act_func=options.get(\n                                                'combined_att_mlp_act', 'tanh'))\n        \n        prob_attention_2 = self.softmax(combined_feat_attention_2[:, :, 0])\n\n        image_feat_ave_2 = (prob_attention_2[:, :, None] * image_feat_down).sum(axis=1)\n\n        return outputs", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "options.get('image_mlp_act',\n                                                   'tanh')", "metadata": {}, "execution_count": 50, "outputs": [{"execution_count": 50, "output_type": "execute_result", "data": {"text/plain": "'tanh'"}, "metadata": {}}]}, {"cell_type": "code", "source": "from mindspore import dtype as mstype\ntest_batch = 8\nall_question_idx = Tensor(np.array(all_question_idx),mstype.int32)", "metadata": {}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "x = all_question_idx[0:8,]", "metadata": {}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "#modify\nembedding = nn.Embedding(options['dict_size'], options['n_emb'],True)\nembeddings = embedding(x)", "metadata": {}, "execution_count": 21, "outputs": []}, {"cell_type": "code", "source": "print(embeddings.shape)", "metadata": {}, "execution_count": 22, "outputs": [{"name": "stdout", "text": "(8, 10, 500)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "#\u8fd9\u91cc\u76848\u662fbatch_size\nh = Tensor(np.zeros((1,8, options['n_dim']), dtype='float32'))\nc = Tensor(np.zeros((1,8, options['n_dim']), dtype='float32'))\n        \nnet = nn.LSTM(options['n_emb'],options['n_dim'],1,True,True)\noutput,(h1,c1) = net(embeddings, (h,c))", "metadata": {}, "execution_count": 24, "outputs": []}, {"cell_type": "code", "source": "h1[0][-1]", "metadata": {}, "execution_count": 31, "outputs": [{"execution_count": 31, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[1024], dtype=Float32, value= [ 8.98742676e-03,  9.22393799e-03,  1.33590698e-02,  2.92968750e-03,  7.48443604e-03,  1.69849396e-03,  5.61904907e-03, -9.92584229e-03,  3.23486328e-03, -2.92205811e-03, -1.00402832e-02,  2.30216980e-03, \n -1.04522705e-02, -4.16183472e-03, -1.07803345e-02, -2.21633911e-03,  2.01225281e-03, -8.78906250e-03, -1.07002258e-03,  4.51087952e-04,  9.27734375e-03, -9.95635986e-03, -1.16539001e-03,  4.73022461e-03, \n -1.31225586e-02, -1.08795166e-02,  7.14874268e-03,  1.12991333e-02, -2.97164917e-03, -2.96401978e-03, -1.66320801e-02, -9.59014893e-03,  1.43966675e-02, -3.15475464e-03,  1.08413696e-02, -1.70898438e-02, \n -1.67083740e-02,  3.79753113e-03,  7.95745850e-03,  1.24130249e-02, -1.20925903e-02,  1.27792358e-02, -1.37786865e-02, -2.83050537e-03,  9.22203064e-04, -1.03759766e-02, -1.28784180e-02,  6.46972656e-03, \n  1.08184814e-02, -1.36256218e-04,  4.58145142e-03, -1.33285522e-02, -8.91113281e-03,  7.35473633e-03,  1.01776123e-02,  1.12533569e-02, -4.10461426e-03,  3.15093994e-03, -1.57318115e-02,  1.00860596e-02, \n -6.93130493e-03, -4.21905518e-03,  3.13949585e-03, -5.11932373e-03, -6.04629517e-03,  3.43132019e-03,  4.70352173e-03, -4.09698486e-03,  4.03594971e-03, -5.50842285e-03, -8.59069824e-03,  9.38415527e-03, \n  1.00173950e-02, -1.62658691e-02, -1.04370117e-02, -9.85717773e-03, -2.13241577e-03,  4.57000732e-03,  1.72119141e-02,  7.09533691e-03, -1.58843994e-02,  3.56292725e-03,  7.24792480e-03,  7.60269165e-03, \n  1.28479004e-02,  2.65312195e-03,  8.56018066e-03, -1.23262405e-04, -4.90188599e-03,  7.20977783e-04, -1.57012939e-02,  1.75595284e-04,  8.59737396e-04, -3.61824036e-03,  5.67436218e-04, -1.42288208e-02, \n  2.87055969e-04, -5.94615936e-04,  1.71661377e-02,  7.07244873e-03, -6.29043579e-03, -1.22909546e-02, -1.16577148e-02, -3.00025940e-03, -1.15356445e-02, -6.33621216e-03,  1.19094849e-02,  2.01988220e-03, \n  7.57694244e-04, -1.90429688e-02, -7.47680664e-03, -5.86318970e-03,  3.79943848e-03, -1.97296143e-02,  7.57598877e-03, -1.22528076e-02,  4.29534912e-03, -2.84194946e-03, -8.86917114e-04, -8.39233398e-03, \n  1.04293823e-02, -1.21917725e-02,  1.33132935e-02, -1.20162964e-02,  1.21612549e-02,  1.63726807e-02,  1.41220093e-02,  4.48226929e-03, -1.67846680e-02,  3.55720520e-03,  8.87298584e-03, -1.43508911e-02, \n -5.58471680e-03,  1.88636780e-03, -1.77459717e-02, -1.43203735e-02, -4.78363037e-03,  1.17492676e-02,  1.31454468e-02, -1.21765137e-02,  1.08566284e-02, -3.64303589e-03, -5.47790527e-03,  8.40759277e-03, \n -6.50405884e-03,  8.32366943e-03, -5.01632690e-03,  4.84466553e-03, -1.06887817e-02, -1.57470703e-02, -3.99017334e-03, -8.29315186e-03,  1.17778778e-03,  1.25503540e-02,  1.28784180e-02,  1.39007568e-02, \n  1.04522705e-02, -6.48880005e-03,  5.13458252e-03, -1.48620605e-02, -1.35498047e-02,  6.65283203e-03, -1.35345459e-02, -8.76617432e-03,  7.49206543e-03,  1.52282715e-02, -5.00488281e-03,  3.16047668e-03, \n  2.84957886e-03, -1.23443604e-02, -9.96589661e-04,  9.26208496e-03, -8.70513916e-03, -3.41606140e-03, -3.75747681e-03, -1.42288208e-03,  1.00173950e-02, -1.66168213e-02,  6.40869141e-03, -1.42669678e-02, \n  7.07244873e-03, -8.08715820e-03, -8.24737549e-03,  1.48391724e-02, -1.39083862e-02,  7.09533691e-03, -5.94139099e-04,  1.97601318e-02, -6.77871704e-03, -1.04598999e-02,  3.74794006e-03, -1.64947510e-02, \n -1.48239136e-02,  1.02844238e-02,  9.49096680e-03,  5.77926636e-03,  1.31835938e-02,  1.41296387e-02,  1.02615356e-02,  2.91442871e-03,  1.00708008e-02, -5.70678711e-03, -7.06863403e-03, -9.84191895e-03, \n -9.68170166e-03, -1.64642334e-02,  2.31552124e-03,  1.10092163e-02,  1.35421753e-02,  3.26728821e-03, -2.01225281e-03,  7.53402710e-04, -2.97737122e-03, -5.36727905e-03,  1.51138306e-02,  5.36346436e-03, \n  1.13830566e-02,  1.87835693e-02,  9.02557373e-03, -9.89532471e-03, -6.14929199e-03,  7.36236572e-03,  7.30514526e-03, -4.31442261e-03, -1.44805908e-02, -1.15814209e-02,  8.02612305e-03,  6.48498535e-03, \n -6.11305237e-04, -1.15814209e-02, -1.36566162e-02, -1.04751587e-02, -9.53674316e-03, -3.77082825e-03, -1.14345551e-03,  1.42765045e-03, -2.67982483e-03,  2.40135193e-03,  3.97872925e-03,  1.02233887e-03, \n -2.21633911e-03, -1.11236572e-02, -1.79195404e-03,  1.43966675e-02,  5.99861145e-04,  4.52423096e-03,  1.00784302e-02, -9.30023193e-03,  1.11618042e-02, -1.13906860e-02, -4.18853760e-03, -1.80358887e-02, \n -3.52478027e-03, -1.48239136e-02,  5.70678711e-03, -1.24359131e-02, -1.59149170e-02,  1.43508911e-02,  1.54876709e-02,  4.94384766e-03,  1.16577148e-02,  8.40759277e-03,  8.66699219e-03, -8.02040100e-04, \n -8.43811035e-03, -1.64489746e-02,  1.22299194e-02, -6.26754761e-03,  1.13449097e-02,  4.90570068e-03, -8.53729248e-03,  7.14111328e-03, -1.73645020e-02, -5.26428223e-03,  5.03063202e-04,  1.43051147e-02, \n -1.82037354e-02, -1.21536255e-02,  1.08032227e-02,  8.16345215e-03,  1.60064697e-02, -7.04193115e-03,  1.70745850e-02, -1.42211914e-02,  6.93511963e-03,  3.82041931e-03, -1.10321045e-02,  1.36795044e-02, \n -2.61068344e-04,  3.97109985e-03,  2.99453735e-03, -1.01699829e-02,  4.04739380e-03, -6.91604614e-03, -1.09863281e-02, -8.69750977e-03,  8.68225098e-03,  1.59301758e-02,  2.86674500e-03,  3.65257263e-04, \n  2.74181366e-04, -6.92367554e-04,  1.47247314e-03,  1.58691406e-02, -9.94873047e-03,  6.56509399e-03,  3.47137451e-03, -9.11712646e-03,  9.86480713e-03,  1.29604340e-03,  2.81095505e-04, -1.94244385e-02, \n -8.89587402e-03,  6.75582886e-03, -7.68661499e-03, -5.77926636e-03, -1.39083862e-02, -5.34057617e-03,  1.17874146e-02,  3.79180908e-03, -1.08623505e-03,  1.85394287e-02, -1.27944946e-02, -8.22448730e-03, \n -6.55746460e-03, -4.76074219e-03, -1.17797852e-02,  1.61132812e-02, -1.45568848e-02, -4.47845459e-03, -1.23519897e-02, -8.07952881e-03, -1.55925751e-03, -1.00479126e-02, -3.00598145e-03, -1.53884888e-02, \n  1.35421753e-02, -1.11618042e-02, -2.43186951e-03, -1.25579834e-02, -5.56945801e-03, -1.54800415e-02,  8.67247581e-05, -5.98907471e-03, -1.51138306e-02, -2.01797485e-03,  7.38501549e-05,  1.41143799e-02, \n  5.57422638e-04,  9.97161865e-03,  1.32322311e-04,  1.03073120e-02, -1.32980347e-02, -9.57489014e-03, -7.10296631e-03,  3.08036804e-03, -1.20162964e-02,  6.32095337e-03, -1.44348145e-02, -4.79888916e-03, \n -1.05133057e-02,  8.84246826e-03, -1.86614990e-02, -5.30624390e-03, -9.33074951e-03,  6.77108765e-03,  6.94274902e-03, -1.37557983e-02,  2.73466110e-04,  3.88717651e-03,  7.03811646e-03, -1.28402710e-02, \n  5.23757935e-03, -4.39071655e-03,  1.09710693e-02, -1.04370117e-02, -8.78143311e-03,  1.11770630e-02,  1.58081055e-02,  1.39999390e-02,  1.83868408e-03,  7.36999512e-03,  1.95770264e-02,  1.54876709e-03, \n  9.33074951e-03, -2.27737427e-03, -1.15585327e-03,  9.47570801e-03, -2.15454102e-02,  3.42941284e-03,  1.33285522e-02, -1.22146606e-02, -5.59234619e-03, -1.26571655e-02,  2.09999084e-03,  1.22985840e-02, \n -6.07299805e-03, -1.57775879e-02,  8.11767578e-03, -3.12423706e-03, -1.38549805e-02,  4.61196899e-03,  5.32913208e-03, -1.83410645e-02,  8.87298584e-03,  7.68280029e-03,  6.54220581e-04,  1.06430054e-02, \n  1.61743164e-03, -1.20391846e-02,  6.94656372e-03, -1.77154541e-02,  1.15890503e-02, -6.77108765e-04, -7.94219971e-03, -8.56781006e-03,  6.01577759e-03,  4.10079956e-03,  7.30037689e-04, -1.10931396e-02, \n -7.94982910e-03,  7.62176514e-03, -9.46044922e-03,  7.45391846e-03,  7.11822510e-03, -8.33892822e-03, -6.01196289e-03, -1.41220093e-02,  4.32205200e-03, -7.86590576e-03,  1.89876556e-03,  1.17950439e-02, \n -8.77380371e-03, -1.34944916e-03,  1.47018433e-02,  1.29318237e-02, -5.09643555e-03,  1.41143799e-03,  7.90405273e-03,  1.80206299e-02,  7.15255737e-03, -1.81732178e-02,  1.79443359e-02,  4.86755371e-03, \n -2.39181519e-03,  8.84246826e-03, -1.54876709e-02, -4.74929810e-03, -1.52587891e-02,  1.05590820e-02,  1.29623413e-02,  8.78906250e-03,  4.95529175e-03, -7.58361816e-03, -1.25732422e-02, -8.05664062e-03, \n -8.00323486e-03,  1.96266174e-03, -4.60052490e-03, -1.31454468e-02, -1.04370117e-02,  3.55720520e-03,  8.95690918e-03,  2.87055969e-03, -1.97982788e-03,  2.85339355e-03,  1.26495361e-02, -9.38415527e-03, \n -1.13143921e-02, -5.57708740e-03, -6.66427612e-03, -5.27954102e-03,  1.58538818e-02,  1.17397308e-03, -1.32904053e-02,  3.02124023e-03,  4.82559204e-03,  3.51667404e-04, -1.97906494e-02, -1.59606934e-02, \n -1.28936768e-03, -5.41687012e-03, -1.53961182e-02,  5.00488281e-03, -1.75189972e-03,  6.44302368e-03,  8.41522217e-03,  8.03375244e-03, -5.82504272e-03,  2.07824707e-02, -2.60543823e-03, -8.23497772e-04, \n -7.49588013e-03,  5.51605225e-03, -8.24737549e-03, -5.50460815e-03, -5.56564331e-03,  1.11083984e-02, -6.16073608e-03, -6.03485107e-03,  7.43103027e-03,  1.90639496e-03,  5.90133667e-03, -1.82952881e-02, \n  1.94835663e-03, -1.48487091e-03,  8.11004639e-03,  4.42123413e-03, -9.84191895e-04, -2.78091431e-03, -9.68933105e-03, -1.10702515e-02,  9.99450684e-03,  8.43811035e-03, -1.07421875e-02, -1.57070160e-03, \n -4.22286987e-03,  7.43865967e-03, -1.72882080e-02, -1.47399902e-02, -1.39312744e-02, -1.33972168e-02, -1.66034698e-03, -1.06887817e-02, -7.27176666e-04,  1.75170898e-02, -1.19476318e-02,  1.05209351e-02, \n  6.81304932e-03,  7.97271729e-03, -6.38961792e-03, -5.22613525e-03,  1.09100342e-02,  1.40285492e-03, -3.27873230e-03, -3.62586975e-03, -6.19888306e-03,  1.23977661e-02,  1.74713135e-02,  1.11579895e-03, \n  1.69677734e-02, -6.05392456e-03, -1.02996826e-03,  1.56402588e-02,  1.88140869e-02, -2.75039673e-03, -1.94702148e-02,  1.18255615e-02,  4.19235229e-03,  7.25555420e-03, -9.07135010e-03, -4.89807129e-03, \n -1.29852295e-02,  1.25350952e-02,  2.09236145e-03,  1.29699707e-02, -9.14764404e-03, -1.12819672e-03, -1.65100098e-02, -1.40762329e-02, -1.75628662e-02,  1.51824951e-02,  7.61508942e-04,  1.02844238e-02, \n  6.49642944e-03, -3.31497192e-03,  5.87463379e-03, -6.30569458e-03,  2.06451416e-02, -7.34329224e-03,  1.09634399e-02,  1.84020996e-02,  9.95635986e-03,  1.53884888e-02,  1.05819702e-02,  1.11312866e-02, \n  1.86157227e-02, -9.58251953e-03, -2.48336792e-03, -7.60650635e-03, -3.19957733e-04,  4.54711914e-03, -1.34963989e-02, -3.39508057e-03, -1.02844238e-02,  1.07955933e-02, -6.37769699e-06, -1.47323608e-02, \n  1.40762329e-03,  6.17599487e-03,  4.84466553e-03, -5.68008423e-03, -1.16195679e-02,  1.35574341e-02,  4.17327881e-03, -1.57775879e-02,  3.98635864e-03,  1.81484222e-03,  9.33074951e-03,  6.43539429e-03, \n -4.45556641e-03, -4.18853760e-03,  1.03225708e-02, -1.94244385e-02, -4.42504883e-03,  1.02005005e-02, -5.42831421e-03, -8.01849365e-03,  3.01551819e-03, -7.91168213e-03, -5.82885742e-03,  5.23757935e-03, \n -1.31530762e-02, -8.42285156e-03,  4.28771973e-03,  5.80596924e-03, -7.51495361e-03,  8.12530518e-03, -6.91604614e-03,  6.61468506e-03, -1.59549713e-03,  4.12750244e-03,  1.32446289e-02,  1.57623291e-02, \n  1.48296356e-04, -1.35040283e-02,  2.01721191e-02, -9.80377197e-03,  4.50611115e-04, -2.12669373e-03, -6.19506836e-03,  1.63879395e-02,  1.90277100e-02, -1.35116577e-02, -1.16729736e-02, -3.60870361e-03, \n  5.13458252e-03,  1.64031982e-02,  6.01577759e-03,  1.09786987e-02,  1.65820122e-04,  2.64549255e-03,  1.02996826e-02,  9.60540771e-03,  2.82096863e-03,  2.25639343e-03, -7.40051270e-03,  4.70733643e-03, \n  1.58843994e-02, -1.56402588e-02,  1.36032104e-02, -1.50299072e-02,  6.76155090e-04,  1.21154785e-02,  1.89208984e-03, -1.04522705e-02,  5.52368164e-03, -1.23596191e-02,  8.26716423e-05,  3.32260132e-03, \n  1.09329224e-02, -7.82012939e-03, -5.23757935e-03,  8.68225098e-03,  4.73022461e-03,  1.13906860e-02,  4.63104248e-03, -1.67083740e-02,  1.03378296e-02,  1.39312744e-02, -1.67541504e-02, -1.09481812e-02, \n -4.91142273e-04, -1.04751587e-02,  5.07354736e-03, -1.77001953e-03, -2.44903564e-03, -1.60980225e-02, -1.03836060e-02,  1.91955566e-02, -1.46865845e-02,  1.47857666e-02,  1.35879517e-02,  4.16946411e-03, \n -2.42424011e-03,  5.54275513e-03,  2.75230408e-03, -5.75637817e-03,  5.87463379e-03, -6.56890869e-03,  2.45475769e-03, -1.47857666e-02,  1.28479004e-02, -7.22885132e-04,  5.55801392e-03, -1.55715942e-02, \n -2.60353088e-03,  7.30514526e-03, -1.20697021e-02, -2.82287598e-03, -1.82189941e-02,  5.10787964e-03, -1.59454346e-02, -4.68826294e-03,  1.36089325e-03, -7.77435303e-03, -1.03988647e-02,  5.82504272e-03, \n -7.57980347e-03, -3.48472595e-03,  5.65719604e-03,  1.07879639e-02,  1.66473389e-02,  1.41372681e-02, -1.68914795e-02, -7.32803345e-03, -1.99432373e-02, -1.08184814e-02,  3.88526917e-03, -4.95910645e-03, \n -9.34600830e-03,  1.55868530e-02,  3.39889526e-03, -1.31988525e-02, -2.84957886e-03, -5.60760498e-03, -9.98687744e-03,  7.85350800e-04,  1.16119385e-02,  1.17301941e-04, -1.08566284e-02,  1.46389008e-03, \n -5.96237183e-03, -9.29260254e-03,  1.46179199e-02,  4.50897217e-03, -1.78833008e-02, -8.97216797e-03,  2.39753723e-03,  5.60379028e-03, -1.59606934e-02,  2.33054161e-04,  5.61904907e-03, -7.83538818e-03, \n  8.85772705e-03, -5.35202026e-03, -1.20925903e-02, -8.85772705e-03,  5.85174561e-03, -8.14056396e-03,  4.47463989e-03,  7.03048706e-03, -1.42097473e-03,  9.39178467e-03, -6.87026978e-03,  1.09786987e-02, \n -1.57165527e-02,  1.36089325e-03,  9.68170166e-03, -1.47628784e-02,  2.14934349e-04, -3.66973877e-03,  4.91333008e-03,  6.40106201e-03,  2.23517418e-04, -1.05209351e-02, -1.03302002e-02,  1.20391846e-02, \n -1.79290771e-03, -6.85119629e-03,  1.44767761e-03,  1.15356445e-02,  3.14712524e-03,  1.84173584e-02, -6.81686401e-03,  1.44424438e-02, -8.91113281e-03,  1.37939453e-02,  2.83050537e-03,  1.26457214e-03, \n -4.55856323e-04,  5.15365601e-03, -5.95855713e-03, -1.78623199e-03, -4.16183472e-03,  7.77053833e-03,  5.65719604e-03,  7.55310059e-03, -1.14135742e-02, -9.44519043e-03,  8.75091553e-03, -1.25579834e-02, \n -1.03530884e-02,  1.99127197e-02,  4.76074219e-03, -1.98364258e-03,  3.24440002e-03,  1.42974854e-02, -1.36718750e-02,  2.27165222e-03, -1.90429688e-02, -1.37405396e-02,  1.00250244e-02,  5.04302979e-03, \n  1.79100037e-03, -1.25408173e-03,  8.76617432e-03,  1.29241943e-02, -1.40228271e-02, -6.07967377e-04, -4.81009483e-05, -1.03759766e-03,  7.98034668e-03, -5.85174561e-03,  1.16729736e-02, -2.73513794e-03, \n -1.31454468e-02,  5.53894043e-03,  5.89847565e-04,  1.93595886e-03,  9.33837891e-03,  7.19833374e-03, -9.07135010e-03, -1.58691406e-03,  3.42369080e-03, -2.09999084e-03, -1.01394653e-02,  2.94303894e-03, \n -3.44467163e-03,  1.71279907e-03, -1.98974609e-02,  1.55029297e-02, -8.74328613e-03, -8.13603401e-05,  6.33239746e-03,  2.34069824e-02, -1.80969238e-02, -1.39999390e-02,  1.62696838e-03, -7.22503662e-03, \n  2.21061707e-03,  1.10931396e-02, -5.19752502e-04, -1.93939209e-02, -1.39312744e-02, -1.44805908e-02,  5.92803955e-03, -1.01318359e-02,  1.30538940e-02, -1.26953125e-02,  1.14517212e-02,  4.77218628e-03, \n  9.68170166e-03, -1.36871338e-02,  5.05065918e-03, -1.55029297e-02,  1.47933960e-02,  1.12686157e-02,  7.52258301e-03,  1.05972290e-02, -3.77082825e-03, -2.93493271e-04, -4.71496582e-03, -9.81140137e-03, \n  6.42013550e-03,  9.78851318e-03, -1.43508911e-02, -3.41415405e-03,  1.78070068e-02, -1.07421875e-02,  1.19018555e-02, -2.76565552e-05,  9.24682617e-03, -5.69152832e-03,  1.41620636e-03, -8.95690918e-03, \n  4.01687622e-03,  1.02691650e-02,  7.96508789e-03, -9.88769531e-03, -1.35192871e-02,  2.73513794e-03, -1.10244751e-02, -1.35650635e-02, -1.54876709e-02,  5.65338135e-03,  1.42974854e-02,  1.04751587e-02, \n  6.08825684e-03,  1.39312744e-02, -1.34124756e-02,  6.32858276e-03, -8.77380371e-03,  5.79833984e-03,  5.65719604e-03,  7.59124756e-03, -1.14669800e-02, -7.08389282e-03, -1.26342773e-02, -5.79071045e-03, \n -5.93948364e-03, -1.44195557e-02,  3.79753113e-03, -3.74221802e-03, -6.19125366e-03, -3.92913818e-03, -6.40487671e-03, -7.65228271e-03, -1.41067505e-02,  6.98852539e-03, -5.88607788e-03,  1.23748779e-02, \n -1.60369873e-02,  1.11007690e-02,  1.20391846e-02, -9.26208496e-03, -2.10380554e-03, -6.99234009e-03, -4.35638428e-03, -8.63647461e-03, -1.36032104e-02, -3.22914124e-03,  1.67999268e-02,  7.91931152e-03, \n  4.52041626e-03, -9.72747803e-03,  3.58963013e-03, -4.74548340e-03, -4.98962402e-03, -9.33837891e-03, -8.10241699e-03, -6.29806519e-03, -2.02560425e-03, -7.00759888e-03,  1.55487061e-02, -7.84301758e-03, \n -4.08172607e-03,  2.88200378e-03,  1.20315552e-02,  1.65252686e-02, -6.52313232e-03, -1.25274658e-02,  1.40762329e-02, -1.58538818e-02, -7.13825226e-04, -1.26419067e-02,  3.26919556e-03,  1.27487183e-02, \n -3.30734253e-03, -8.68988037e-03, -2.96592712e-04,  8.25500488e-03,  1.53045654e-02, -1.69372559e-02, -5.49697876e-03,  1.54876709e-02,  2.10762024e-03, -4.90951538e-03,  1.33705139e-03, -1.23062134e-02, \n -1.54724121e-02,  5.16128540e-03, -6.60324097e-03,  9.20867920e-03, -4.90951538e-03,  1.46408081e-02,  1.46560669e-02, -1.03225708e-02,  1.02310181e-02,  6.38198853e-03, -9.48333740e-03, -1.86462402e-02, \n -3.09944153e-05,  6.99234009e-03, -9.84191895e-04,  1.37710571e-02,  3.42559814e-03,  1.97029114e-03, -1.01318359e-02,  1.03988647e-02, -3.09944153e-03,  3.35884094e-03,  5.80596924e-03, -1.36337280e-02, \n -2.45475769e-03, -3.37600708e-03,  1.94091797e-02, -1.06353760e-02, -6.12258911e-03,  5.37109375e-03, -9.52911377e-03, -2.07710266e-03,  4.53567505e-03,  5.82885742e-03,  1.13105774e-03,  3.89099121e-03, \n -9.40704346e-03, -5.06210327e-03, -3.93295288e-03, -9.10186768e-03, -1.05361938e-02, -1.54037476e-02,  7.47680664e-03, -1.87225342e-02, -1.26495361e-02, -1.17034912e-02, -1.01547241e-02,  1.32751465e-02, \n  3.98254395e-03,  1.28250122e-02, -1.61437988e-02, -6.37435913e-03, -1.16577148e-02, -1.11770630e-02, -3.33786011e-03, -2.75802612e-03,  1.53121948e-02,  9.20104980e-03, -6.19411469e-04,  4.77313995e-04, \n  5.24139404e-03, -1.21536255e-02,  7.75527954e-03,  6.51550293e-03])"}, "metadata": {}}]}, {"cell_type": "code", "source": "\nnet = nn.Embedding(20000, 768,  True)\ninput_data = Tensor(np.ones([8, 128]), mindspore.int32)\n\n# Maps the input word IDs to word embedding.\noutput = net(input_data)\nresult = output.shape\nprint(result)\n\n", "metadata": {}, "execution_count": 10, "outputs": [{"name": "stdout", "text": "(8, 128, 768)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "h = Tensor([[1,2,1],\n            [1,1,1]], mstype.float32)\nc = Tensor(np.ones((1,3,2)), mstype.float32)", "metadata": {}, "execution_count": 39, "outputs": []}, {"cell_type": "code", "source": "output = mindspore.ops.dot(h,c)", "metadata": {}, "execution_count": 40, "outputs": []}, {"cell_type": "code", "source": "output", "metadata": {}, "execution_count": 41, "outputs": [{"execution_count": 41, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[2, 1, 2], dtype=Float32, value=\n[[[ 4.00000000e+00,  4.00000000e+00]],\n [[ 3.00000000e+00,  3.00000000e+00]]])"}, "metadata": {}}]}, {"cell_type": "code", "source": "h.shape", "metadata": {}, "execution_count": 42, "outputs": [{"execution_count": 42, "output_type": "execute_result", "data": {"text/plain": "(2, 3)"}, "metadata": {}}]}, {"cell_type": "code", "source": "c.shape", "metadata": {}, "execution_count": 43, "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {"text/plain": "(1, 3, 2)"}, "metadata": {}}]}, {"cell_type": "code", "source": "input_x1 = Tensor(np.ones(shape=[2, 3]), mindspore.float32)\ninput_x2 = Tensor(np.ones(shape=[1, 3, 2]), mindspore.float32)\noutput = mindspore.ops.dot(input_x1, input_x2)\nprint(output)\n", "metadata": {}, "execution_count": 45, "outputs": [{"name": "stdout", "text": "[[[3. 3.]]\n\n [[3. 3.]]]\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "print(output.shape)\nprint(input_x1.shape)\nprint(input_x2.shape)", "metadata": {}, "execution_count": 49, "outputs": [{"name": "stdout", "text": "(2, 1, 2)\n(2, 3)\n(1, 3, 2)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}