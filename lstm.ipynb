{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "MindSpore-python3.7-aarch64", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "code", "source": "import moxing as mox\nmox.file.copy_parallel(src_url=\"obs://nlp-kim/project/data/\", dst_url='./data/') ", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v1.17.3-d858ff4a\nINFO:root:Using OBS-Python-SDK-3.20.9.1\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "import sys\nimport os\nimport json\nimport pickle as pkl\nimport re\nfrom collections import Counter\nimport numpy as np\nimport random\nfrom collections import OrderedDict\nimport math\ndef process_sentence(sentence):\n    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n    commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n    punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n                    '(', ')', '=', '+', '\\\\', '_', '-',\n                    '>', '<', '@', '`', ',', '?', '!']\n    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n                    \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n                    \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n                    \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n                    \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n                    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n                    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n                    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n                    \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n                    \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n                    \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n                    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n                    \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n                    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n                    \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n                    \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n                    \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n                    \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n                    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n                    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n                    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n                    \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n\n    inText = sentence.replace('\\n', ' ')\n    inText = inText.replace('\\t', ' ')\n    inText = inText.strip()\n    outText = inText\n    for p in punct:\n        if (p + ' ' in inText or ' ' + p in inText) or \\\n           (re.search(commaStrip, inText) != None):\n            outText = outText.replace(p, '')\n        else:\n            outText = outText.replace(p, ' ')\n    outText = periodStrip.sub(\"\", outText, re.UNICODE)\n    outText = outText.lower().split()\n    for wordId, word in enumerate(outText):\n        if word in contractions:\n            outText[wordId] = contractions[word]\n    outText = ' '.join(outText)\n    return outText\n\ndef process_answer(answer):\n    articles = ['a', 'an', 'the']\n    manualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n                  '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n                  'eight': '8', 'nine': '9', 'ten': '10' }\n    new_answer = process_sentence(answer)\n    outText = []\n    for word in new_answer.split():\n        if word not in articles:\n            word = manualMap.setdefault(word, word)\n            outText.append(word)\n    return ' '.join(outText)", "metadata": {"trusted": true}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "source": "#qa\uff1aquestion\u548c\u5bf9\u5e94\u7684annotions\n#train_question_ids\uff1aquestion\u7684id\u7684\u6570\u7ec4\n\n#question_dict_count\uff1a question\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_questions\uff1a question\u8bed\u53e5split\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n#answer_dict_count\uff1a answer\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_answers\uff1a answer\u88absplit\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n\n#question_key\uff1a\u6309\u7167question\u4e2d\u51fa\u73b0\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n#answer_top_k: \u6309\u7167answer\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "f = open(\"./data/questions/train.json\", \"r\")\nf1 = open(\"./data/annotations/train.json\", \"r\")\nfile = json.load(f)\nfile1 = json.load(f1)\nannotations = file1['annotations']\ntrain_question_ids = []\ntrain_image_ids = []\ntrain_questions = []\ntrain_answers = []\nquestion_dict_count = dict()\nanswer_dict_count = dict()\n\n# \u5f62\u6210qa\uff1a\u4e00\u4e2a\u5b57\u5178\uff0c\u6574\u7406\u51fa\u5bf9\u5e94question_id\u7684annotation\nqa = {ann['question_id']: [] for ann in annotations}\nfor ann in annotations:\n    qa[ann['question_id']] = ann\n\n#\u83b7\u53d6image_id question_id\nfor idx, item in enumerate(file['questions']):\n    train_question_ids.append(item['question_id'])\n    train_image_ids.append(item['image_id'])\n    \n    #process question\n    question = item['question']\n    question = process_sentence(question)\n    question = question.split()\n    for word in question:\n        question_dict_count[word] = question_dict_count.get(word, 0) + 1\n    train_questions.append(question)\n    answer = qa[item['question_id']]['answers']\n    answer_new = [process_answer(ans['answer']) for ans in answer]\n    ans_array = []\n    for ans in answer:\n        ans_array.append(ans['answer'])\n    for word in answer_new:\n        answer_dict_count[word] = answer_dict_count.get(word, 0) + 1\n    train_answers.append(ans_array)\n    if idx % 10000 == 0:\n        print ('finished processing %d in train' %(idx))\n\n# sort question dict\nquestion_count = question_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(question_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(question_count, reverse=True)\nquestion_key = list(question_dict_count.keys())\n# \u5bf9question_key\u91cd\u65b0\u6392\u5e8f\nquestion_key = [question_key[idx] for idx in sorted_index]\n# add '<unk>' to the begining\nquestion_key.insert(0, '<unk>')\n# '<unk>' begins at 1, 0 is reserved for empty words\nquestion_key = dict((key, idx + 1) for idx, key in enumerate(question_key))\n\nk = 1000\n# sort answer dict and get top k answers\ndel answer_dict_count['']\nanswer_count = answer_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(answer_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(answer_count, reverse=True)\nanswer_key = list(answer_dict_count.keys())\nanswer_key = [answer_key[idx] for idx in sorted_index]\nanswer_top_k = answer_key[:k]\nanswer_top_k = dict((key, idx) for idx, key in enumerate(answer_top_k))\n\n# convert words to idx and remove some\ntrain_question_idx = []\ntrain_answer_idx = []\ntrain_answer_counter = []\nidx_to_remove = []\nfor idx, answer in enumerate(train_answers):\n    question_idx = [question_key[word] for word in train_questions[idx]]\n    #print(question_idx)\n    #print('\\n')\n    #print(train_questions[idx])\n    train_question_idx.append(question_idx)\n    answer_idx = [answer_top_k[ans] for ans in answer\n                 if ans in answer_top_k]\n    answer_counter = Counter(answer_idx)\n    train_answer_counter.append(answer_counter)\n    train_answer_idx.append(answer_idx)\n    if not answer_idx:\n        idx_to_remove.append(idx)\nprint ('%d out of %d, %f of the question in train are removed'\\\n    %(len(idx_to_remove), len(train_question_ids),\n      len(idx_to_remove) / float(len(train_question_ids))))\n\n# transform to array and delete all the empty answer\ntrain_question_ids = np.array(train_question_ids)\ntrain_image_ids = np.array(train_image_ids)\ntrain_question_idx = np.array(train_question_idx)\ntrain_answer_idx = np.array(train_answer_idx)\ntrain_answer_counter = np.array(train_answer_counter)\n\ntrain_question_ids = np.delete(train_question_ids, idx_to_remove)\ntrain_image_ids = np.delete(train_image_ids, idx_to_remove)\ntrain_question_idx = np.delete(train_question_idx, idx_to_remove)\ntrain_answer_idx = np.delete(train_answer_idx, idx_to_remove)\ntrain_answer_counter = np.delete(train_answer_counter, idx_to_remove)\n\n# reshuffle the train data\nidx_shuffle = list(range(train_question_ids.shape[0]))\nrandom.shuffle(idx_shuffle)\ntrain_question_ids = train_question_ids[idx_shuffle]\ntrain_image_ids = train_image_ids[idx_shuffle]\ntrain_question_idx = train_question_idx[idx_shuffle]\ntrain_answer_idx = train_answer_idx[idx_shuffle]\ntrain_answer_counter = train_answer_counter[idx_shuffle]\n\n# the most frequent as label\ntrain_answer_label = [counter.most_common(1)[0][0]\n                      for counter in train_answer_counter]\ntrain_answer_label = np.array(train_answer_label)\n\n# transform from counter to dict\ntrain_answer_counter = [dict(counter) for counter in train_answer_counter]\ntrain_answer_counter = np.array(train_answer_counter)\n\nprint ('finished processing train')", "metadata": {"trusted": true}, "execution_count": 4, "outputs": [{"name": "stdout", "text": "finished processing 0 in train\nfinished processing 10000 in train\nfinished processing 20000 in train\nfinished processing 30000 in train\nfinished processing 40000 in train\n2105 out of 44375, 0.047437 of the question in train are removed\nfinished processing train\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "all_question_vector=[]\nfor idx,question in enumerate(train_questions):\n    count = 0\n    question_vector = []\n    for word in question:\n        count = count + 1\n        if count > 10:\n            break\n        else:\n            q_emb = np.zeros((len(question_key) + 1), dtype='int32')\n            q_emb[question_key[word]] = 1\n            question_vector.append(q_emb)\n    while count < 10:\n        padding = np.zeros((len(question_key) + 1), dtype='int32')\n        question_vector.append(padding)\n        count = count + 1\n    all_question_vector.append(question_vector)", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "x = [(idx,len(item)) for idx,item in enumerate(train_questions)]", "metadata": {"trusted": true}, "execution_count": 19, "outputs": []}, {"cell_type": "code", "source": "import mindspore\nimport mindspore.nn as nn\nfrom mindspore import Tensor\nfrom mindspore import context\nfrom mindspore.train.model import Model\nfrom mindspore.nn.metrics import Accuracy\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\nfrom mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\nfrom mindspore.ops import operations as ops\nfrom mindspore.nn import LSTM", "metadata": {}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "#from easydict import EasyDict as edict\noptions = OrderedDict()\n# data related\noptions['data_path'] = './data/'\n#options['feature_file'] = 'trainval_feat.h5'\n#options['expt_folder'] = 'expt_1'\noptions['model_name'] = 'imageqa'\noptions['train_split'] = 'trainval1'\noptions['val_split'] = 'val2'\noptions['shuffle'] = True\noptions['reverse'] = True\noptions['sample_answer'] = True\n\noptions['num_region'] = 196\noptions['region_dim'] = 512\n\noptions['n_words'] = 13746\noptions['n_output'] = 1000\n\n# structure options\noptions['combined_num_mlp'] = 1\noptions['combined_mlp_drop_0'] = True\noptions['combined_mlp_act_0'] = 'linear'\noptions['sent_drop'] = False\noptions['use_tanh'] = False\n\noptions['use_attention_drop'] = False\n\n# dimensions\noptions['n_emb'] = 500\noptions['n_dim'] = 1024\noptions['n_image_feat'] = options['region_dim']\noptions['n_common_feat'] = 500\noptions['n_attention'] = 512\n\n# initialization\noptions['init_type'] = 'uniform'\noptions['range'] = 0.01\noptions['std'] = 0.01\noptions['init_lstm_svd'] = False\n\noptions['forget_bias'] = np.float32(1.0)\n\n# learning parameters\noptions['optimization'] = 'sgd' # choices\noptions['batch_size'] = 100\noptions['lr'] = np.float32(0.05)\noptions['w_emb_lr'] = np.float32(80)\noptions['momentum'] = np.float32(0.9)\noptions['gamma'] = 1\noptions['step'] = 10\noptions['step_start'] = 100\noptions['max_epochs'] = 50\noptions['weight_decay'] = 0.0005\noptions['decay_rate'] = np.float32(0.999)\noptions['drop_ratio'] = np.float32(0.5)\noptions['smooth'] = np.float32(1e-8)\noptions['grad_clip'] = np.float32(0.1)\n\n# log params\noptions['disp_interval'] = 10\noptions['eval_interval'] = 1000\noptions['save_interval'] = 500", "metadata": {}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend', device_id=0)", "metadata": {}, "execution_count": 7, "outputs": []}, {"cell_type": "code", "source": "def init_weight(n, d, options):\n    ''' initialize weight matrix\n    options['init_type'] determines\n    gaussian or uniform initlizaiton\n    '''\n    if options['init_type'] == 'gaussian':\n        return (np.random.randn(n, d).astype(floatX)) * options['std']\n    elif options['init_type'] == 'uniform':\n        # [-range, range]\n        return ((np.random.rand(n, d) * 2 - 1) * \\\n                options['range']).astype(floatX)\ndef ortho_weight(ndim):\n    \"\"\"\n    Random orthogonal weights, we take\n    the right matrix in the SVD.\n\n    Remember in SVD, u has the same # rows as W\n    and v has the same # of cols as W. So we\n    are ensuring that the rows are\n    orthogonal.\n    \"\"\"\n    W = np.random.randn(ndim, ndim)\n    u, _, _ = np.linalg.svd(W)\n    return u.astype('float32')\n\ndef init_fflayer(params, nin, nout, options, prefix='ff'):\n    ''' initialize ff layer\n    '''\n    params[prefix + '_w'] = init_weight(nin, nout, options)\n    params[prefix + '_b'] = np.zeros(nout, dtype='float32')\n    return params\n\ndef init_lstm_layer(params, nin, ndim, options, prefix='lstm'):\n    ''' initializt lstm layer\n    '''\n    params[prefix + '_w_x'] = init_weight(nin, 4 * ndim, options)\n    # use svd trick to initializ\n    if options['init_lstm_svd']:\n        params[prefix + '_w_h'] = np.concatenate([ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim)],\n                                                 axis=1)\n    else:\n        params[prefix + '_w_h'] = init_weight(ndim, 4 * ndim, options)\n    params[prefix + '_b_h'] = np.zeros(4 * ndim, dtype='float32')\n    # set forget bias to be positive\n    params[prefix + '_b_h'][ndim : 2*ndim] = np.float32(options.get('forget_bias', 0))\n    return params\n\n# initialize the parmaters\ndef init_params(options):\n    ''' Initialize all the parameters\n    '''\n    params = OrderedDict()\n    n_words = options['n_words']\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    n_image_feat = options['n_image_feat']\n    n_common_feat = options['n_common_feat']\n    n_output = options['n_output']\n    n_attention = options['n_attention']\n\n    params['w_emb'] = ((np.random.rand(n_words, n_emb) * 2 - 1) * 0.5).astype(floatX)\n\n    params = init_fflayer(params, n_image_feat, n_dim, options,\n                          prefix='image_mlp')\n\n    # attention model based parameters\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_1')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_2')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_2')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_2')\n\n\n    # params for sentence image mlp\n    for i in range(options['combined_num_mlp']):\n        if i == 0 and options['combined_num_mlp'] == 1:\n            params = init_fflayer(params, n_dim, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == 0 and options['combined_num_mlp'] != 1:\n            params = init_fflayer(params, n_dim, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == options['combined_num_mlp'] - 1 :\n            params = init_fflayer(params, n_common_feat, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        else:\n            params = init_fflayer(params, n_common_feat, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n\n    # lstm layer\n    params = init_lstm_layer(params, n_emb, n_dim, options, prefix='sent_lstm')\n\n    return params\n\ndef init_shared_params(params):\n    ''' return a shared version of all parameters\n    '''\n    global shared_params\n    shared_params = OrderedDict()\n    for k, p in params.items():\n        shared_params[k] = params[k]\n\n    return shared_params\n\ndef get_lr(options, curr_epoch):\n    if options['optimization'] == 'sgd':\n        power = max((curr_epoch - options['step_start']) / options['step'], 0)\n        power = math.ceil(power)\n        return options['lr'] * (options['gamma'] ** power)  #\n    else:\n        return options['lr']", "metadata": {}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": "def lstm_layer(shared_params, x, mask, h_0, c_0, options, prefix='lstm'):\n    ''' lstm layer:\n    :param shared_params: shared parameters\n    :param x: input, T x batch_size x n_emb\n    :param mask: mask for x, T x batch_size\n    '''\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    # weight matrix for x, n_emb x 4*n_dim (ifoc)\n    lstm_w_x = shared_params[prefix + '_w_x']\n    # weight matrix for h, n_dim x 4*n_dim\n    lstm_w_h = shared_params[prefix + '_w_h']\n    lstm_b_h = shared_params[prefix + '_b_h']\n    h_0 = h_0[:x.shape[1]]\n    c_0 = c_0[:x.shape[1]]\n    question_net = LSTM(n_emb, n_dim)\n    output, (h, c) = question_net(x, (h_0, c_0))\n    return h, c", "metadata": {}, "execution_count": 9, "outputs": []}, {"cell_type": "code", "source": "def build_model(shared_params, options):\n    #input_idx = Tensor.imatrix('input_idx')\n    input_idx = Tensor()\n    global empty_word\n    empty_word = np.zeros((1, options['n_emb']), dtype='float32')\n    w_emb_extend = Tensor.concatenate([empty_word, shared_params['w_emb']],\n                                 axis=0)\n    input_emb = w_emb_extend[input_idx]\n    \n    # get the transformed image feature\n    global h_0, c_0\n    h_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    c_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    h_encode, c_encode = lstm_layer(shared_params, input_emb, input_mask,\n                                    h_0, c_0, options, prefix='sent_lstm')\n    return h_encodem, c_encode", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "floatX = np.float32\nbatch_size = options['batch_size']\nmax_epochs = options['max_epochs']\n\n###############\n# build model #\n###############\nparams = init_params(options)\nshared_params = init_shared_params(params)", "metadata": {}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": "input_idx = np.ones((6618,100),dtype = 'int32')\nshared_params['w_emb'] = ((np.random.rand(13746, 500) * 2 - 1) * 0.5).astype(floatX)\nempty_word = np.zeros((1, 500), dtype='float32')\nw_emb_extend = shared_params['w_emb']\ninput_emb = w_emb_extend[input_idx]", "metadata": {}, "execution_count": 26, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}