{"metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "MindSpore-python3.7-aarch64", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "## 1.\u5bfc\u5165\u6a21\u5757", "metadata": {}}, {"cell_type": "code", "source": "import moxing as mox\ndata_url = \"s3://nlp.final/san/data/\"\ncode_url = \"s3://nlp.final/san/code/image_model/src\"\n#mox.file.copy_parallel(src_url=\"obs://nlp-kim/project/data/\", dst_url='./data/') \n# mox.file.copy_parallel(src_url=\"s3://dl4nlp-my/project/data/\", dst_url='./data/') \nmox.file.copy_parallel(src_url=data_url, dst_url='./data/') \nmox.file.copy_parallel(src_url = code_url,dst_url='./src/')", "metadata": {"scrolled": true, "trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v1.17.3-d858ff4a\nINFO:root:Using OBS-Python-SDK-3.20.9.1\nINFO:root:Listing OBS: 1000\nINFO:root:Listing OBS: 2000\nINFO:root:Listing OBS: 3000\nINFO:root:Listing OBS: 4000\nINFO:root:Listing OBS: 5000\nINFO:root:Listing OBS: 6000\nINFO:root:Listing OBS: 7000\nINFO:root:Listing OBS: 8000\nINFO:root:Listing OBS: 9000\nINFO:root:Listing OBS: 10000\nINFO:root:Listing OBS: 11000\nINFO:root:Listing OBS: 12000\nINFO:root:Listing OBS: 13000\nINFO:root:Listing OBS: 14000\nINFO:root:Listing OBS: 15000\nINFO:root:Listing OBS: 16000\nINFO:root:Listing OBS: 17000\nINFO:root:Listing OBS: 18000\nINFO:root:Listing OBS: 19000\nINFO:root:Listing OBS: 20000\nINFO:root:Listing OBS: 21000\nINFO:root:Listing OBS: 22000\nINFO:root:Listing OBS: 23000\nINFO:root:Listing OBS: 24000\nINFO:root:Listing OBS: 25000\nINFO:root:Listing OBS: 26000\nINFO:root:Listing OBS: 27000\nINFO:root:Listing OBS: 28000\nINFO:root:Listing OBS: 29000\nINFO:root:Listing OBS: 30000\nINFO:root:Listing OBS: 31000\nINFO:root:Listing OBS: 32000\nINFO:root:Listing OBS: 33000\nINFO:root:Listing OBS: 34000\nINFO:root:Listing OBS: 35000\nINFO:root:Listing OBS: 36000\nINFO:root:Listing OBS: 37000\nINFO:root:Listing OBS: 38000\nINFO:root:Listing OBS: 39000\nINFO:root:Listing OBS: 40000\nINFO:root:Listing OBS: 41000\nINFO:root:Listing OBS: 42000\nINFO:root:Listing OBS: 43000\nINFO:root:Listing OBS: 44000\nINFO:root:Listing OBS: 45000\nINFO:root:Listing OBS: 46000\nINFO:root:Listing OBS: 47000\nINFO:root:Listing OBS: 48000\nINFO:root:Listing OBS: 49000\nINFO:root:Listing OBS: 50000\nINFO:root:Listing OBS: 51000\nINFO:root:Listing OBS: 52000\nINFO:root:Listing OBS: 53000\nINFO:root:Listing OBS: 54000\nINFO:root:Listing OBS: 55000\nINFO:root:Listing OBS: 56000\nINFO:root:Listing OBS: 57000\nINFO:root:Listing OBS: 58000\nINFO:root:Listing OBS: 59000\nINFO:root:Listing OBS: 60000\nINFO:root:Listing OBS: 61000\nINFO:root:Listing OBS: 62000\nINFO:root:Listing OBS: 63000\nINFO:root:Listing OBS: 64000\nINFO:root:Listing OBS: 65000\nINFO:root:Listing OBS: 66000\nINFO:root:Listing OBS: 67000\nINFO:root:Listing OBS: 68000\nINFO:root:Listing OBS: 69000\nINFO:root:Listing OBS: 70000\nINFO:root:Listing OBS: 71000\nINFO:root:pid: None.\t1000/71899\nINFO:root:pid: None.\t2000/71899\nINFO:root:pid: None.\t3000/71899\nINFO:root:pid: None.\t4000/71899\nINFO:root:pid: None.\t5000/71899\nINFO:root:pid: None.\t6000/71899\nINFO:root:pid: None.\t7000/71899\nINFO:root:pid: None.\t8000/71899\nINFO:root:pid: None.\t9000/71899\nINFO:root:pid: None.\t10000/71899\nINFO:root:pid: None.\t11000/71899\nINFO:root:pid: None.\t12000/71899\nINFO:root:pid: None.\t13000/71899\nINFO:root:pid: None.\t14000/71899\nINFO:root:pid: None.\t15000/71899\nINFO:root:pid: None.\t16000/71899\nINFO:root:pid: None.\t17000/71899\nINFO:root:pid: None.\t18000/71899\nINFO:root:pid: None.\t19000/71899\nINFO:root:pid: None.\t20000/71899\nINFO:root:pid: None.\t21000/71899\nINFO:root:pid: None.\t22000/71899\nINFO:root:pid: None.\t23000/71899\nINFO:root:pid: None.\t24000/71899\nINFO:root:pid: None.\t25000/71899\nINFO:root:pid: None.\t26000/71899\nINFO:root:pid: None.\t27000/71899\nINFO:root:pid: None.\t28000/71899\nINFO:root:pid: None.\t29000/71899\nINFO:root:pid: None.\t30000/71899\nINFO:root:pid: None.\t31000/71899\nINFO:root:pid: None.\t32000/71899\nINFO:root:pid: None.\t33000/71899\nINFO:root:pid: None.\t34000/71899\nINFO:root:pid: None.\t35000/71899\nINFO:root:pid: None.\t36000/71899\nINFO:root:pid: None.\t37000/71899\nINFO:root:pid: None.\t38000/71899\nINFO:root:pid: None.\t39000/71899\nINFO:root:pid: None.\t40000/71899\nINFO:root:pid: None.\t41000/71899\nINFO:root:pid: None.\t42000/71899\nINFO:root:pid: None.\t43000/71899\nINFO:root:pid: None.\t44000/71899\nINFO:root:pid: None.\t45000/71899\nINFO:root:pid: None.\t46000/71899\nINFO:root:pid: None.\t47000/71899\nINFO:root:pid: None.\t48000/71899\nINFO:root:pid: None.\t49000/71899\nINFO:root:pid: None.\t50000/71899\nINFO:root:pid: None.\t51000/71899\nINFO:root:pid: None.\t52000/71899\nINFO:root:pid: None.\t53000/71899\nINFO:root:pid: None.\t54000/71899\nINFO:root:pid: None.\t55000/71899\nINFO:root:pid: None.\t56000/71899\nINFO:root:pid: None.\t57000/71899\nINFO:root:pid: None.\t58000/71899\nINFO:root:pid: None.\t59000/71899\nINFO:root:pid: None.\t60000/71899\nINFO:root:pid: None.\t61000/71899\nINFO:root:pid: None.\t62000/71899\nINFO:root:pid: None.\t63000/71899\nINFO:root:pid: None.\t64000/71899\nINFO:root:pid: None.\t65000/71899\nINFO:root:pid: None.\t66000/71899\nINFO:root:pid: None.\t67000/71899\nINFO:root:pid: None.\t68000/71899\nINFO:root:pid: None.\t69000/71899\nINFO:root:pid: None.\t70000/71899\nINFO:root:pid: None.\t71000/71899\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "import sys\nimport os\nimport json\nimport pickle as pkl\nimport re\nfrom collections import Counter\nimport numpy as np\nimport random\nfrom collections import OrderedDict\nimport math\n\nimport mindspore\nimport mindspore.nn as nn\nfrom mindspore import Tensor\nfrom mindspore import context\nfrom mindspore.train.model import Model\nfrom mindspore.nn.metrics import Accuracy\nfrom mindspore.train.serialization import load_checkpoint, load_param_into_net\nfrom mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\nfrom mindspore.ops import operations as ops\nfrom mindspore import dtype as mstype", "metadata": {}, "execution_count": 21, "outputs": []}, {"cell_type": "markdown", "source": "## 2.\u6570\u636e\u9884\u5904\u7406", "metadata": {}}, {"cell_type": "markdown", "source": "### 2.1 \u5904\u7406\u51fd\u6570", "metadata": {}}, {"cell_type": "code", "source": "def process_sentence(sentence):\n    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n    commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n    punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n                    '(', ')', '=', '+', '\\\\', '_', '-',\n                    '>', '<', '@', '`', ',', '?', '!']\n    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n                    \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n                    \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n                    \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n                    \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n                    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n                    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n                    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n                    \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n                    \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n                    \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n                    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n                    \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n                    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n                    \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n                    \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n                    \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n                    \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n                    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n                    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n                    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n                    \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n\n    inText = sentence.replace('\\n', ' ')\n    inText = inText.replace('\\t', ' ')\n    inText = inText.strip()\n    outText = inText\n    for p in punct:\n        if (p + ' ' in inText or ' ' + p in inText) or \\\n           (re.search(commaStrip, inText) != None):\n            outText = outText.replace(p, '')\n        else:\n            outText = outText.replace(p, ' ')\n    outText = periodStrip.sub(\"\", outText, re.UNICODE)\n    outText = outText.lower().split()\n    for wordId, word in enumerate(outText):\n        if word in contractions:\n            outText[wordId] = contractions[word]\n    outText = ' '.join(outText)\n    return outText\n\ndef process_answer(answer):\n    articles = ['a', 'an', 'the']\n    manualMap = { 'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three':\n                  '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7',\n                  'eight': '8', 'nine': '9', 'ten': '10' }\n    new_answer = process_sentence(answer)\n    outText = []\n    for word in new_answer.split():\n        if word not in articles:\n            word = manualMap.setdefault(word, word)\n            outText.append(word)\n    return ' '.join(outText)", "metadata": {}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "### 2.2 \u53d8\u91cf\u8bf4\u660e", "metadata": {}}, {"cell_type": "code", "source": "#qa\uff1aquestion\u548c\u5bf9\u5e94\u7684annotions\n#train_question_ids\uff1aquestion\u7684id\u7684\u6570\u7ec4\n\n#question_dict_count\uff1a question\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_questions\uff1a question\u8bed\u53e5split\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n#answer_dict_count\uff1a answer\u4e2d\u7684\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7edf\u8ba1\n#train_answers\uff1a answer\u88absplit\u4e3aword\u7684\u6570\u7ec4\u7684\u6570\u7ec4\n\n#question_key\uff1a\u6309\u7167question\u4e2d\u51fa\u73b0\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n#answer_top_k: \u6309\u7167answer\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\u8fdb\u884c\u6392\u5e8f\n", "metadata": {}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "### 2.3 \u9884\u5904\u7406\u7ec6\u8282", "metadata": {}}, {"cell_type": "code", "source": "f = open(\"./data/questions/train.json\", \"r\")\nf1 = open(\"./data/annotations/train.json\", \"r\")\nfile = json.load(f)\nfile1 = json.load(f1)\nannotations = file1['annotations']\ntrain_question_ids = []\ntrain_image_ids = []\ntrain_questions = []\ntrain_answers = []\nquestion_dict_count = dict()\nanswer_dict_count = dict()\n\n# \u5f62\u6210qa\uff1a\u4e00\u4e2a\u5b57\u5178\uff0c\u6574\u7406\u51fa\u5bf9\u5e94question_id\u7684annotation\nqa = {ann['question_id']: [] for ann in annotations}\nfor ann in annotations:\n    qa[ann['question_id']] = ann\n\n#\u83b7\u53d6image_id question_id\n# \u9700\u8981\u8fdb\u884c\u5c01\u88c5\u4ee5\u9002\u7528\u4e8e\u8bad\u7ec3\u3001\u6d4b\u8bd5\u3001\u8bc4\u4f30\nimg_not_exist_count = 0\nfor idx, item in enumerate(file['questions']):\n    img_path = os.path.join(\"./data/images/train\",\"COCO_{0}2014_{1}.jpg\".format(\"train\",str(item['image_id']).zfill(12)))\n    if not os.path.exists(img_path):\n        img_not_exist_count += 1\n        continue\n\n    train_question_ids.append(item['question_id'])\n    train_image_ids.append(item['image_id'])\n    \n    #process question\n    question = item['question']\n    question = process_sentence(question)\n    question = question.split()\n    for word in question:\n        question_dict_count[word] = question_dict_count.get(word, 0) + 1\n    train_questions.append(question)\n    answer = qa[item['question_id']]['answers']\n    answer_new = [process_answer(ans['answer']) for ans in answer]\n    ans_array = []\n    for ans in answer:\n        ans_array.append(ans['answer'])\n    for word in answer_new:\n        answer_dict_count[word] = answer_dict_count.get(word, 0) + 1\n    train_answers.append(ans_array)\n    if idx % 10000 == 0:\n        print ('finished processing %d in train' %(idx))\n\n# sort question dict\nquestion_count = question_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(question_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(question_count, reverse=True)\nquestion_key = list(question_dict_count.keys())\n# \u5bf9question_key\u91cd\u65b0\u6392\u5e8f\nquestion_key = [question_key[idx] for idx in sorted_index]\n# add '<unk>' to the begining\nquestion_key.insert(0, '<unk>')\n# '<unk>' begins at 1, 0 is reserved for empty words\nquestion_key = dict((key, idx + 1) for idx, key in enumerate(question_key))\n\nk = 1000\n# sort answer dict and get top k answers\ndel answer_dict_count['']\nanswer_count = answer_dict_count.values()\nsorted_index = [count[0] for count in\n                sorted(enumerate(answer_count),\n                       key = lambda x : x[1],\n                       reverse=True)]\nsorted_count = sorted(answer_count, reverse=True)\nanswer_key = list(answer_dict_count.keys())\nanswer_key = [answer_key[idx] for idx in sorted_index]\nanswer_top_k = answer_key[:k]\nanswer_top_k = dict((key, idx) for idx, key in enumerate(answer_top_k))\n\n# convert words to idx and remove some\ntrain_question_idx = []\ntrain_answer_idx = []\ntrain_answer_counter = []\nidx_to_remove = []\nfor idx, answer in enumerate(train_answers):\n    question_idx = [question_key[word] for word in train_questions[idx]]\n    #print(question_idx)\n    #print('\\n')\n    #print(train_questions[idx])\n    train_question_idx.append(question_idx)\n    answer_idx = [answer_top_k[ans] for ans in answer\n                 if ans in answer_top_k]\n    answer_counter = Counter(answer_idx)\n    train_answer_counter.append(answer_counter)\n    train_answer_idx.append(answer_idx)\n    if not answer_idx:\n        idx_to_remove.append(idx)\nprint ('%d out of %d, %f of the question in train are removed'\\\n    %(len(idx_to_remove), len(train_question_ids),\n      len(idx_to_remove) / float(len(train_question_ids))))\n\n# transform to array and delete all the empty answer\ntrain_question_ids = np.array(train_question_ids)\ntrain_image_ids = np.array(train_image_ids)\ntrain_question_idx = np.array(train_question_idx)\ntrain_answer_idx = np.array(train_answer_idx)\ntrain_answer_counter = np.array(train_answer_counter)\n\ntrain_question_ids = np.delete(train_question_ids, idx_to_remove)\ntrain_image_ids = np.delete(train_image_ids, idx_to_remove)\ntrain_question_idx = np.delete(train_question_idx, idx_to_remove)\ntrain_answer_idx = np.delete(train_answer_idx, idx_to_remove)\ntrain_answer_counter = np.delete(train_answer_counter, idx_to_remove)\n\n# reshuffle the train data\nidx_shuffle = list(range(train_question_ids.shape[0]))\nrandom.shuffle(idx_shuffle)\ntrain_question_ids = train_question_ids[idx_shuffle]\ntrain_image_ids = train_image_ids[idx_shuffle]\ntrain_question_idx = train_question_idx[idx_shuffle]\ntrain_answer_idx = train_answer_idx[idx_shuffle]\ntrain_answer_counter = train_answer_counter[idx_shuffle]\n\n# the most frequent as label\ntrain_answer_label = [counter.most_common(1)[0][0]\n                      for counter in train_answer_counter]\ntrain_answer_label = np.array(train_answer_label)\n\n# transform from counter to dict\ntrain_answer_counter = [dict(counter) for counter in train_answer_counter]\ntrain_answer_counter = np.array(train_answer_counter)\n\nprint ('finished processing train')\nprint('{0} images not exist'.format(img_not_exist_count))", "metadata": {}, "execution_count": 5, "outputs": [{"name": "stdout", "text": "finished processing 10000 in train\nfinished processing 20000 in train\n949 out of 23071, 0.041134 of the question in train are removed\nfinished processing train\n21304 images not exist\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### 2.4 \u6784\u5efa\u8bcd\u5411\u91cf", "metadata": {}}, {"cell_type": "code", "source": "#construct one hot vector\nall_question_vector=[]\nfor idx,question in enumerate(train_questions):\n    count = 0\n    question_vector = []\n    for word in question:\n        count = count + 1\n        if count > 10:\n            break\n        else:\n            q_emb = np.zeros((len(question_key) + 1), dtype='int32')\n            q_emb[question_key[word]] = 1\n            question_vector.append(q_emb)\n    while count < 10:\n        padding = np.zeros((len(question_key) + 1), dtype='int32')\n        question_vector.append(padding)\n        count = count + 1\n    all_question_vector.append(question_vector)", "metadata": {}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "#convert word to idx\nall_question_idx = []\nfor question in train_questions:\n    count = 0\n    one_question_idx = []\n    \n    for word in question:\n        count = count + 1\n        if count > 10:\n            break\n        else:\n            one_question_idx.append(question_key[word])     \n    while count < 10:\n        one_question_idx.append(0)\n        count = count + 1        \n    all_question_idx.append(one_question_idx)\nall_question_idx", "metadata": {"scrolled": true}, "execution_count": 7, "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "[[5, 2, 242, 1035, 7, 10, 2857, 13, 0, 0],\n [4, 3, 2, 16, 31, 0, 0, 0, 0, 0],\n [4, 3, 2, 28, 20, 0, 0, 0, 0, 0],\n [148, 162, 2134, 46, 447, 7, 2, 196, 0, 0],\n [22, 3, 2, 166, 0, 0, 0, 0, 0, 0],\n [3, 2, 61, 288, 259, 0, 0, 0, 0, 0],\n [4, 13, 136, 3, 1490, 19, 2, 212, 0, 0],\n [5, 34, 65, 2858, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 37, 0, 0, 0, 0, 0],\n [4, 5, 2, 739, 53, 8, 0, 0, 0, 0],\n [4, 26, 9, 47, 3, 69, 0, 0, 0, 0],\n [4, 3, 2, 740, 8, 2, 255, 42, 0, 0],\n [4, 740, 3, 8, 2, 255, 42, 0, 0, 0],\n [3, 6, 708, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 2859, 5, 14, 0, 0, 0, 0, 0],\n [87, 162, 1109, 94, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 1734, 7, 101, 9, 2, 114, 0],\n [3, 2, 272, 649, 0, 0, 0, 0, 0, 0],\n [3, 2, 115, 470, 121, 2135, 0, 0, 0, 0],\n [4, 13, 3, 2, 58, 134, 0, 0, 0, 0],\n [3, 2, 44, 779, 7, 6, 18, 897, 0, 0],\n [4, 13, 5, 2, 2860, 7, 2, 1036, 0, 0],\n [3, 6, 10, 117, 250, 92, 0, 0, 0, 0],\n [11, 12, 1735, 15, 6, 44, 25, 0, 0, 0],\n [4, 13, 3, 2, 63, 0, 0, 0, 0, 0],\n [4, 3, 2, 1212, 16, 2861, 19, 2, 1212, 35],\n [104, 30, 413, 10, 1736, 369, 30, 2862, 6, 48],\n [4, 3, 2, 16, 80, 8, 0, 0, 0, 0],\n [4, 3, 163, 7, 2, 191, 0, 0, 0, 0],\n [11, 12, 681, 5, 8, 2, 78, 0, 0, 0],\n [3, 6, 448, 2863, 0, 0, 0, 0, 0, 0],\n [4, 121, 449, 5, 182, 19, 2, 63, 0, 0],\n [11, 265, 15, 6, 169, 79, 0, 0, 0, 0],\n [54, 88, 3, 2, 28, 38, 2, 197, 7, 0],\n [22, 3, 2, 57, 53, 0, 0, 0, 0, 0],\n [4, 3, 2, 2136, 16, 7, 2, 1213, 260, 0],\n [11, 12, 299, 5, 7, 2, 18, 0, 0, 0],\n [54, 121, 5, 1491, 65, 0, 0, 0, 0, 0],\n [11, 12, 17, 1492, 25, 10, 273, 0, 0, 0],\n [11, 12, 373, 29, 30, 55, 0, 0, 0, 0],\n [4, 3, 1342, 2, 709, 0, 0, 0, 0, 0],\n [4, 13, 134, 3, 6, 16, 20, 0, 0, 0],\n [3, 10, 28, 20, 10, 2137, 1493, 0, 0, 0],\n [11, 12, 187, 243, 5, 14, 0, 0, 0, 0],\n [4, 13, 3, 2, 101, 164, 0, 0, 0, 0],\n [22, 15, 6, 18, 513, 251, 0, 0, 0, 0],\n [15, 2, 188, 79, 125, 0, 0, 0, 0, 0],\n [3, 6, 40, 2, 334, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 289, 0, 0, 0, 0, 0],\n [279, 7, 2, 116, 1494, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 57, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 321, 244, 0, 0, 0, 0],\n [189, 192, 36, 620, 0, 0, 0, 0, 0, 0],\n [4, 111, 3, 83, 300, 0, 0, 0, 0, 0],\n [5, 30, 1110, 9, 198, 0, 0, 0, 0, 0],\n [3, 2, 209, 7, 1737, 0, 0, 0, 0, 0],\n [68, 5, 594, 9, 2, 395, 471, 0, 0, 0],\n [4, 3, 2, 125, 8, 2, 64, 0, 0, 0],\n [4, 33, 9, 129, 3, 6, 0, 0, 0, 0],\n [4, 13, 3, 2, 2864, 0, 0, 0, 0, 0],\n [4, 327, 3, 65, 0, 0, 0, 0, 0, 0],\n [3, 14, 188, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 1343, 384, 572, 19, 1738, 236, 2, 274],\n [4, 13, 126, 5, 2, 226, 145, 2865, 0, 0],\n [4, 3, 2, 1495, 9, 2, 385, 0, 0, 0],\n [3, 6, 10, 741, 292, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 1111, 0, 0, 0, 0, 0],\n [5, 14, 1739, 1214, 8, 2, 63, 0, 0, 0],\n [15, 49, 25, 227, 130, 8, 46, 1496, 8, 46],\n [4, 3, 2, 111, 23, 17, 5, 65, 0, 0],\n [3, 1344, 2866, 498, 573, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 151, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 38, 0, 0, 0, 0, 0],\n [4, 13, 52, 3, 6, 16, 20, 0, 0, 0],\n [4, 3, 2, 406, 8, 2, 396, 0, 0, 0],\n [4, 2867, 61, 3, 7, 2, 175, 0, 0, 0],\n [4, 3, 2, 42, 31, 0, 0, 0, 0, 0],\n [3, 6, 621, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 71, 710, 2138, 0, 0, 0, 0, 0],\n [3, 6, 10, 450, 0, 0, 0, 0, 0, 0],\n [11, 12, 145, 5, 40, 2, 84, 101, 9, 2],\n [4, 3, 2, 13, 9, 2, 44, 0, 0, 0],\n [4, 3, 2, 48, 1037, 0, 0, 0, 0, 0],\n [11, 12, 711, 5, 8, 2, 48, 0, 0, 0],\n [3, 6, 10, 92, 48, 0, 0, 0, 0, 0],\n [3, 2, 304, 1215, 0, 0, 0, 0, 0, 0],\n [4, 15, 2, 483, 96, 85, 1345, 109, 8, 2],\n [11, 256, 5, 14, 0, 0, 0, 0, 0, 0],\n [3, 2, 16, 288, 10, 18, 9, 1497, 0, 0],\n [68, 5, 23, 17, 8, 158, 574, 970, 0, 0],\n [15, 2, 154, 25, 1740, 314, 0, 0, 0, 0],\n [3, 2, 98, 53, 1741, 8, 2, 154, 0, 0],\n [4, 3, 2, 472, 62, 9, 0, 0, 0, 0],\n [4, 111, 3, 83, 300, 0, 0, 0, 0, 0],\n [4, 266, 3, 2, 60, 0, 0, 0, 0, 0],\n [5, 34, 1498, 0, 0, 0, 0, 0, 0, 0],\n [4, 155, 5, 2, 548, 19, 2, 74, 20, 0],\n [4, 13, 3, 2, 196, 0, 0, 0, 0, 0],\n [3, 49, 65, 10, 70, 0, 0, 0, 0, 0],\n [4, 514, 5, 142, 0, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 58, 52, 0, 0, 0, 0],\n [3, 14, 10, 1112, 8, 82, 681, 0, 0, 0],\n [4, 423, 3, 142, 0, 0, 0, 0, 0, 0],\n [4, 3, 2139, 230, 13, 8, 2, 78, 0, 0],\n [3, 14, 10, 2868, 9, 243, 7, 6, 18, 0],\n [11, 12, 971, 5, 14, 0, 0, 0, 0, 0],\n [4, 116, 3, 8, 6, 549, 0, 0, 0, 0],\n [4, 5, 2, 120, 473, 8, 2, 71, 0, 0],\n [4, 3, 2, 2869, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 42, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 98, 7, 2, 2870, 305, 0],\n [4, 13, 3, 2, 210, 185, 0, 0, 0, 0],\n [3, 14, 10, 2140, 7, 2, 66, 0, 0, 0],\n [4, 13, 3, 2, 1742, 8, 2, 289, 0, 0],\n [5, 2, 397, 244, 2, 133, 13, 0, 0, 0],\n [3, 2, 2871, 1743, 2872, 8, 0, 0, 0, 0],\n [11, 12, 2873, 2874, 5, 7, 6, 18, 0, 0],\n [3, 2, 56, 328, 0, 0, 0, 0, 0, 0],\n [5, 23, 299, 24, 329, 65, 1216, 0, 0, 0],\n [4, 111, 5, 34, 65, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 188, 0, 0, 0],\n [4, 13, 3, 2, 122, 0, 0, 0, 0, 0],\n [4, 33, 9, 322, 3, 7, 2, 550, 0, 0],\n [3, 2, 193, 7, 2, 530, 472, 0, 0, 0],\n [4, 650, 1744, 3, 10, 1217, 9, 6, 385, 0],\n [3, 14, 10, 127, 0, 0, 0, 0, 0, 0],\n [11, 12, 1346, 5, 14, 0, 0, 0, 0, 0],\n [11, 12, 315, 41, 36, 131, 0, 0, 0, 0],\n [4, 13, 5, 2, 256, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 515, 0, 0, 0, 0, 0],\n [4, 5, 2, 414, 8, 2, 101, 9, 2, 45],\n [3, 2, 45, 857, 499, 0, 0, 0, 0, 0],\n [4, 1745, 6, 742, 0, 0, 0, 0, 0, 0],\n [4, 370, 9, 280, 5, 237, 2, 315, 0, 0],\n [4, 3, 2, 118, 9, 6, 1113, 45, 0, 0],\n [4, 13, 3, 2, 45, 742, 0, 0, 0, 0],\n [3, 2, 13, 9, 2, 146, 2141, 0, 0, 0],\n [68, 3, 2, 154, 1746, 19, 2, 575, 0, 0],\n [4, 202, 3, 8, 2, 306, 817, 99, 3, 8],\n [4, 15, 2, 1747, 48, 595, 0, 0, 0, 0],\n [4, 5, 972, 898, 398, 123, 2, 386, 0, 0],\n [3, 14, 10, 484, 0, 0, 0, 0, 0, 0],\n [4, 531, 3, 2, 61, 163, 50, 0, 0, 0],\n [11, 149, 3, 2, 98, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 45, 0, 0, 0, 0, 0],\n [4, 3, 49, 8, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 210, 185, 0, 0, 0, 0, 0],\n [3, 6, 10, 407, 103, 0, 0, 0, 0, 0],\n [4, 3, 2, 132, 62, 108, 9, 0, 0, 0],\n [3, 6, 32, 1114, 1499, 0, 0, 0, 0, 0],\n [5, 34, 107, 1115, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 27, 0, 0, 0],\n [11, 12, 17, 5, 14, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 1038, 9, 6, 292, 0, 0],\n [11, 12, 191, 1347, 5, 14, 0, 0, 0, 0],\n [4, 252, 3, 1748, 0, 0, 0, 0, 0, 0],\n [11, 12, 972, 145, 5, 8, 2, 451, 0, 0],\n [4, 3, 2, 227, 2875, 2142, 9, 2, 1218, 138],\n [3, 14, 10, 175, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 532, 61, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 61, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 184, 234, 0, 0, 0, 0],\n [3, 2, 76, 353, 0, 0, 0, 0, 0, 0],\n [7, 4, 33, 9, 1500, 5, 2, 1749, 682, 0],\n [99, 5, 119, 2, 44, 0, 0, 0, 0, 0],\n [3, 14, 43, 2876, 8, 2, 60, 0, 0, 0],\n [3, 2, 424, 10, 2877, 424, 0, 0, 0, 0],\n [11, 12, 2143, 5, 434, 0, 0, 0, 0, 0],\n [3, 14, 173, 7, 2, 76, 0, 0, 0, 0],\n [11, 307, 3, 6, 61, 0, 0, 0, 0, 0],\n [3, 6, 27, 2878, 0, 0, 0, 0, 0, 0],\n [22, 89, 18, 75, 9, 2, 238, 0, 0, 0],\n [4, 293, 3, 2, 899, 0, 0, 0, 0, 0],\n [3, 14, 374, 7, 2, 66, 0, 0, 0, 0],\n [11, 12, 160, 5, 14, 0, 0, 0, 0, 0],\n [3, 2, 16, 20, 305, 0, 0, 0, 0, 0],\n [68, 29, 34, 500, 170, 1750, 0, 0, 0, 0],\n [3, 2, 56, 294, 0, 0, 0, 0, 0, 0],\n [3, 2, 415, 8, 2, 64, 10, 416, 24, 10],\n [3, 2, 712, 7, 101, 9, 2, 220, 0, 0],\n [4, 169, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 221, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 2144, 0, 0, 0, 0],\n [4, 26, 9, 301, 5, 2, 346, 8, 0, 0],\n [99, 3, 163, 6, 222, 0, 0, 0, 0, 0],\n [68, 3, 2, 295, 651, 2879, 0, 0, 0, 0],\n [4, 26, 9, 42, 3, 6, 0, 0, 0, 0],\n [4, 155, 3, 2, 42, 0, 0, 0, 0, 0],\n [4, 3, 2, 42, 53, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 1219, 0, 0, 0, 0],\n [3, 2, 2880, 8, 2, 900, 7, 1116, 0, 0],\n [5, 2, 17, 2881, 0, 0, 0, 0, 0, 0],\n [22, 3, 2, 126, 375, 86, 0, 0, 0, 0],\n [4, 1501, 514, 5, 7, 2, 27, 0, 0, 0],\n [3, 2, 57, 276, 683, 0, 0, 0, 0, 0],\n [3, 2, 1751, 2145, 2, 136, 0, 0, 0, 0],\n [4, 26, 9, 818, 3, 49, 290, 0, 0, 0],\n [3, 2, 16, 203, 0, 0, 0, 0, 0, 0],\n [4, 302, 3, 51, 0, 0, 0, 0, 0, 0],\n [15, 2, 576, 41, 361, 19, 36, 2146, 0, 0],\n [22, 3, 2, 272, 7, 2, 76, 0, 0, 0],\n [3, 6, 10, 295, 1502, 0, 0, 0, 0, 0],\n [29, 30, 10, 1752, 7, 2, 76, 0, 0, 0],\n [11, 12, 684, 135, 5, 14, 0, 0, 0, 0],\n [4, 1039, 281, 105, 6, 223, 0, 0, 0, 0],\n [4, 3, 2, 98, 20, 0, 0, 0, 0, 0],\n [4, 26, 9, 354, 3, 2, 35, 1220, 0, 0],\n [11, 12, 238, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 8, 2, 78, 119, 0, 0],\n [3, 2, 596, 622, 105, 24, 140, 0, 0, 0],\n [22, 3, 2, 61, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 27, 51, 104, 36, 7, 59, 2882],\n [68, 1348, 2, 16, 858, 7, 6, 26, 9, 261],\n [4, 3, 2, 16, 38, 0, 0, 0, 0, 0],\n [3, 2, 16, 290, 10, 152, 0, 0, 0, 0],\n [15, 6, 16, 79, 1117, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 181, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 362, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 743, 0, 0, 0, 0, 0],\n [4, 551, 3, 2, 28, 119, 2, 347, 0, 0],\n [3, 6, 199, 0, 0, 0, 0, 0, 0, 0],\n [3, 95, 20, 10, 2883, 623, 0, 0, 0, 0],\n [4, 3, 2, 28, 31, 0, 0, 0, 0, 0],\n [3, 2, 35, 20, 10, 2884, 0, 0, 0, 0],\n [4, 5, 34, 65, 0, 0, 0, 0, 0, 0],\n [5, 23, 859, 0, 0, 0, 0, 0, 0, 0],\n [5, 2, 262, 199, 0, 0, 0, 0, 0, 0],\n [15, 6, 146, 79, 407, 0, 0, 0, 0, 0],\n [4, 3, 2, 132, 62, 9, 0, 0, 0, 0],\n [4, 3, 2, 450, 8, 2, 282, 0, 0, 0],\n [5, 14, 435, 8, 2, 77, 0, 0, 0, 0],\n [780, 24, 1349, 32, 0, 0, 0, 0, 0, 0],\n [4, 26, 9, 18, 3, 6, 0, 0, 0, 0],\n [99, 5, 7, 2, 282, 0, 0, 0, 0, 0],\n [5, 2, 17, 7, 2, 101, 1040, 624, 973, 1503],\n [4, 70, 5, 34, 65, 0, 0, 0, 0, 0],\n [11, 12, 275, 5, 69, 0, 0, 0, 0, 0],\n [4, 3, 485, 2, 78, 0, 0, 0, 0, 0],\n [148, 95, 323, 335, 10, 194, 153, 0, 0, 0],\n [3, 2, 106, 348, 105, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 280, 0, 0, 0, 0, 0],\n [4, 202, 3, 8, 2, 781, 52, 0, 0, 0],\n [4, 3, 2, 245, 685, 8, 0, 0, 0, 0],\n [3, 2, 35, 1754, 190, 2885, 0, 0, 0, 0],\n [3, 21, 10, 399, 8, 2, 246, 0, 0, 0],\n [3, 14, 10, 533, 0, 0, 0, 0, 0, 0],\n [11, 12, 155, 3, 2, 1755, 425, 0, 0, 0],\n [4, 3, 2, 118, 9, 2, 531, 8, 2, 534],\n [498, 2, 121, 247, 8, 6, 2886, 253, 1041, 686],\n [4, 13, 3, 2, 116, 552, 0, 0, 0, 0],\n [4, 13, 3, 2, 550, 0, 0, 0, 0, 0],\n [4, 3, 6, 167, 50, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 387, 254, 239, 2, 975, 0],\n [4, 37, 2887, 2, 47, 7, 2, 1756, 597, 40],\n [11, 12, 744, 5, 74, 0, 0, 0, 0, 0],\n [3, 2, 782, 860, 107, 105, 24, 375, 140, 0],\n [3, 6, 10, 463, 32, 0, 0, 0, 0, 0],\n [3, 14, 43, 188, 0, 0, 0, 0, 0, 0],\n [3, 2, 18, 7, 161, 96, 85, 0, 0, 0],\n [4, 3, 1221, 105, 86, 2, 78, 323, 123, 2],\n [4, 33, 9, 1042, 3, 8, 2, 60, 0, 0],\n [4, 13, 3, 2, 63, 0, 0, 0, 0, 0],\n [3, 6, 10, 228, 1757, 0, 0, 0, 0, 0],\n [3, 2, 18, 625, 0, 0, 0, 0, 0, 0],\n [54, 250, 15, 2, 48, 109, 19, 376, 0, 0],\n [4, 26, 9, 204, 3, 2, 16, 20, 0, 0],\n [4, 3, 2, 202, 8, 2, 63, 0, 0, 0],\n [3, 6, 200, 53, 8, 10, 63, 0, 0, 0],\n [179, 2, 67, 36, 2888, 1350, 0, 0, 0, 0],\n [4, 5, 2, 626, 17, 20, 8, 158, 713, 0],\n [3, 21, 137, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 129, 53, 8, 10, 209, 0, 0, 0],\n [3, 2, 92, 434, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 28, 163, 7, 2, 76, 0, 0],\n [3, 2, 154, 195, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 82, 205, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 58, 388, 0, 0, 0, 0],\n [4, 302, 3, 2, 16, 38, 0, 0, 0, 0],\n [11, 12, 2889, 5, 8, 2, 330, 0, 0, 0],\n [3, 49, 316, 745, 0, 0, 0, 0, 0, 0],\n [3, 49, 10, 98, 0, 0, 0, 0, 0, 0],\n [3, 49, 20, 10, 861, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 58, 88, 0, 0, 0, 0],\n [3, 6, 10, 208, 0, 0, 0, 0, 0, 0],\n [4, 147, 3, 142, 8, 2, 52, 9, 2, 16],\n [3, 2, 35, 91, 40, 2, 212, 0, 0, 0],\n [4, 6, 98, 3, 819, 0, 0, 0, 0, 0],\n [4, 3, 7, 2, 781, 388, 0, 0, 0, 0],\n [3, 2, 35, 8, 2, 462, 64, 9, 2, 93],\n [11, 12, 2890, 15, 2, 98, 25, 7, 82, 687],\n [4, 3, 40, 2, 84, 9, 2, 1758, 0, 0],\n [11, 12, 206, 5, 100, 7, 2, 66, 0, 0],\n [3, 21, 1043, 19, 500, 324, 8, 6, 81, 0],\n [3, 6, 28, 64, 820, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 18, 0, 0, 0],\n [3, 21, 862, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 211, 40, 2, 325, 9, 2, 27, 0],\n [4, 202, 3, 8, 2, 77, 0, 0, 0, 0],\n [11, 12, 206, 0, 0, 0, 0, 0, 0, 0],\n [4, 26, 9, 47, 3, 62, 7, 59, 448, 102],\n [3, 14, 1044, 37, 100, 51, 901, 10, 166, 0],\n [5, 14, 121, 426, 17, 7, 2, 18, 0, 0],\n [4, 26, 9, 261, 3, 2, 187, 53, 7, 0],\n [15, 6, 447, 172, 342, 0, 0, 0, 0, 0],\n [15, 6, 139, 1045, 355, 976, 9, 1759, 0, 0],\n [4, 90, 3, 8, 2, 101, 84, 9, 2, 44],\n [3, 2, 44, 285, 0, 0, 0, 0, 0, 0],\n [3, 2, 150, 7, 10, 334, 0, 0, 0, 0],\n [89, 1351, 167, 19, 281, 2, 863, 0, 0, 0],\n [4, 26, 9, 32, 5, 2, 17, 94, 7, 0],\n [4, 13, 3, 2, 226, 126, 0, 0, 0, 0],\n [5, 2, 308, 2147, 19, 319, 0, 0, 0, 0],\n [22, 3, 2, 85, 598, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 226, 126, 51, 41, 36, 131],\n [3, 2, 126, 1760, 226, 19, 2148, 0, 0, 0],\n [4, 3, 248, 190, 117, 9, 2, 39, 0, 0],\n [3, 21, 10, 137, 81, 0, 0, 0, 0, 0],\n [3, 14, 10, 18, 317, 8, 2, 77, 0, 0],\n [3, 2, 65, 0, 0, 0, 0, 0, 0, 0],\n [3, 14, 188, 7, 2, 93, 0, 0, 0, 0],\n [4, 70, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 267, 3, 2149, 8, 2, 472, 0, 0, 0],\n [7, 4, 252, 5, 34, 65, 0, 0, 0, 0],\n [4, 3, 746, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 714, 5, 8, 2, 60, 0, 0, 0],\n [3, 2, 71, 740, 2891, 24, 1761, 0, 0, 0],\n [3, 6, 7, 486, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 208, 0, 0, 0],\n [4, 5, 2, 17, 535, 50, 0, 0, 0, 0],\n [11, 12, 255, 296, 5, 14, 7, 2, 18, 0],\n [87, 10, 715, 144, 75, 108, 9, 2, 255, 42],\n [4, 13, 5, 2, 349, 0, 0, 0, 0, 0],\n [3, 6, 10, 627, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 44, 7, 2, 18, 0, 0],\n [3, 6, 44, 400, 9, 10, 309, 652, 0, 0],\n [4, 3, 2, 436, 13, 0, 0, 0, 0, 0],\n [3, 2, 126, 8, 0, 0, 0, 0, 0, 0],\n [3, 2, 780, 902, 0, 0, 0, 0, 0, 0],\n [3, 2, 231, 716, 0, 0, 0, 0, 0, 0],\n [11, 12, 599, 9, 553, 427, 3, 100, 0, 0],\n [5, 14, 1762, 7, 6, 18, 0, 0, 0, 0],\n [4, 13, 3, 2, 2150, 222, 0, 0, 0, 0],\n [5, 23, 283, 24, 1504, 0, 0, 0, 0, 0],\n [3, 6, 10, 407, 45, 0, 0, 0, 0, 0],\n [3, 21, 10, 353, 81, 0, 0, 0, 0, 0],\n [4, 32, 89, 6, 75, 7, 0, 0, 0, 0],\n [11, 12, 232, 5, 14, 0, 0, 0, 0, 0],\n [11, 12, 232, 5, 80, 0, 0, 0, 0, 0],\n [5, 2, 198, 86, 2, 133, 628, 0, 0, 0],\n [4, 277, 9, 2, 501, 5, 34, 7, 0, 0],\n [4, 26, 9, 112, 5, 7, 2, 66, 0, 0],\n [3, 2, 57, 356, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 136, 0, 0, 0, 0, 0],\n [5, 117, 9, 2, 1118, 247, 10, 2892, 0, 0],\n [5, 14, 112, 7, 6, 128, 0, 0, 0, 0],\n [5, 23, 2893, 8, 10, 653, 0, 0, 0, 0],\n [4, 3, 2, 428, 8, 2, 48, 1352, 0, 0],\n [4, 13, 3, 2, 48, 0, 0, 0, 0, 0],\n [15, 49, 500, 10, 821, 204, 0, 0, 0, 0],\n [15, 2, 16, 172, 19, 36, 20, 10, 577, 654],\n [15, 6, 79, 102, 21, 3, 86, 59, 149, 1119],\n [5, 14, 168, 535, 19, 1763, 0, 0, 0, 0],\n [4, 3, 2, 35, 377, 0, 0, 0, 0, 0],\n [29, 30, 343, 6, 3, 10, 536, 250, 19, 2151],\n [4, 3, 2, 35, 7, 120, 31, 0, 0, 0],\n [11, 12, 113, 5, 80, 0, 0, 0, 0, 0],\n [4, 155, 5, 8, 2, 58, 903, 0, 0, 0],\n [4, 13, 3, 2, 134, 0, 0, 0, 0, 0],\n [3, 6, 59, 2894, 252, 0, 0, 0, 0, 0],\n [4, 3, 2, 324, 429, 0, 0, 0, 0, 0],\n [3, 2, 238, 268, 263, 24, 213, 2, 747, 0],\n [4, 3, 2, 114, 62, 9, 0, 0, 0, 0],\n [22, 3, 2, 37, 83, 1222, 0, 0, 0, 0],\n [4, 3, 485, 2, 132, 0, 0, 0, 0, 0],\n [5, 14, 711, 0, 0, 0, 0, 0, 0, 0],\n [3, 2, 747, 2, 133, 13, 455, 59, 150, 0],\n [3, 2, 124, 203, 190, 94, 0, 0, 0, 0],\n [22, 3, 2, 42, 356, 0, 0, 0, 0, 0],\n [4, 37, 3, 69, 0, 0, 0, 0, 0, 0],\n [3, 2, 202, 8, 2, 77, 235, 207, 456, 24],\n [3, 6, 7, 486, 0, 0, 0, 0, 0, 0],\n [4, 3, 90, 2152, 31, 0, 0, 0, 0, 0],\n [11, 12, 578, 5, 142, 0, 0, 0, 0, 0],\n [4, 125, 302, 3, 7, 6, 223, 0, 0, 0],\n [15, 6, 220, 25, 2153, 9, 2895, 0, 0, 0],\n [148, 2, 28, 516, 0, 0, 0, 0, 0, 0],\n [5, 183, 9, 23, 17, 94, 0, 0, 0, 0],\n [4, 26, 9, 129, 3, 51, 0, 0, 0, 0],\n [4, 5, 34, 288, 47, 108, 9, 7, 2, 2896],\n [4, 3, 2, 336, 9, 2, 42, 0, 0, 0],\n [4, 3, 2, 42, 377, 0, 0, 0, 0, 0],\n [22, 5, 34, 0, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 2897, 1764, 7, 2, 27, 0, 0],\n [3, 2, 169, 554, 1765, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 63, 0, 0, 0, 0, 0],\n [4, 33, 9, 125, 302, 3, 83, 464, 0, 0],\n [11, 149, 5, 2, 160, 0, 0, 0, 0, 0],\n [3, 117, 9, 2, 160, 91, 40, 2, 212, 0],\n [4, 13, 5, 2, 296, 2898, 0, 0, 0, 0],\n [3, 2, 42, 8, 2, 64, 331, 0, 0, 0],\n [11, 12, 194, 2154, 5, 14, 0, 0, 0, 0],\n [4, 3, 2, 166, 31, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 306, 134, 0, 0, 0, 0],\n [3, 6, 28, 53, 8, 2, 487, 0, 0, 0],\n [11, 12, 2155, 2156, 2, 579, 0, 0, 0, 0],\n [3, 2, 28, 20, 10, 2157, 0, 0, 0, 0],\n [4, 67, 9, 81, 3, 21, 0, 0, 0, 0],\n [4, 2899, 3, 2, 146, 282, 0, 0, 0, 0],\n [11, 12, 337, 5, 7, 2, 76, 0, 0, 0],\n [4, 13, 3, 2, 2900, 0, 0, 0, 0, 0],\n [3, 6, 977, 2901, 0, 0, 0, 0, 0, 0],\n [3, 6, 904, 24, 1766, 0, 0, 0, 0, 0],\n [3, 2, 415, 416, 24, 399, 0, 0, 0, 0],\n [3, 2, 35, 7, 684, 1046, 350, 0, 0, 0],\n [4, 116, 3, 7, 2, 200, 0, 0, 0, 0],\n [87, 162, 1109, 94, 6, 223, 656, 0, 0, 0],\n [4, 26, 9, 71, 3, 6, 0, 0, 0, 0],\n [22, 3, 2, 71, 86, 0, 0, 0, 0, 0],\n [11, 12, 101, 314, 29, 30, 55, 7, 326, 224],\n [3, 14, 2, 2902, 50, 10, 2903, 0, 0, 0],\n [5, 23, 17, 10, 555, 0, 0, 0, 0, 0],\n [4, 3, 2, 57, 269, 8, 0, 0, 0, 0],\n [4, 39, 5, 69, 0, 0, 0, 0, 0, 0],\n [4, 13, 52, 15, 2, 16, 25, 8, 0, 0],\n [4, 13, 134, 3, 2, 28, 8, 2, 74, 20],\n [3, 2, 175, 159, 0, 0, 0, 0, 0, 0],\n [22, 3, 2, 417, 0, 0, 0, 0, 0, 0],\n [3, 6, 16, 905, 0, 0, 0, 0, 0, 0],\n [4, 26, 9, 465, 5, 123, 2, 301, 0, 0],\n [5, 2, 17, 7, 2, 66, 2904, 0, 0, 0],\n [11, 12, 39, 5, 7, 6, 27, 0, 0, 0],\n [4, 37, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 347, 0, 0, 0, 0, 0],\n [5, 2, 113, 2905, 19, 2, 822, 0, 0, 0],\n [4, 13, 3, 2, 600, 9, 2, 229, 0, 0],\n [5, 14, 289, 7, 6, 18, 0, 0, 0, 0],\n [4, 70, 5, 34, 65, 0, 0, 0, 0, 0],\n [11, 149, 3, 2, 35, 0, 0, 0, 0, 0],\n [3, 6, 10, 864, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 657, 16, 8, 10, 224, 0, 0],\n [4, 15, 6, 58, 688, 0, 0, 0, 0, 0],\n [4, 3, 823, 8, 2, 181, 0, 0, 0, 0],\n [11, 12, 174, 5, 100, 7, 6, 93, 0, 0],\n [15, 2, 16, 25, 8, 10, 134, 0, 0, 0],\n [3, 14, 10, 210, 185, 7, 6, 27, 0, 0],\n [11, 12, 629, 5, 452, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 362, 0, 0, 0, 0, 0],\n [5, 23, 906, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 214, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 77, 0, 0, 0, 0, 0],\n [4, 202, 3, 8, 2, 580, 0, 0, 0, 0],\n [11, 12, 17, 5, 53, 140, 0, 0, 0, 0],\n [3, 14, 10, 115, 8, 2, 437, 0, 0, 0],\n [4, 13, 3, 2, 184, 310, 0, 0, 0, 0],\n [4, 13, 5, 2, 515, 0, 0, 0, 0, 0],\n [4, 336, 9, 42, 3, 6, 0, 0, 0, 0],\n [11, 12, 39, 5, 269, 140, 0, 0, 0, 0],\n [29, 30, 55, 43, 112, 7, 2, 27, 0, 0],\n [3, 2, 16, 7, 2, 56, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 66, 0, 0, 0],\n [4, 26, 9, 153, 3, 51, 0, 0, 0, 0],\n [11, 12, 2906, 5, 7, 2, 191, 0, 0, 0],\n [22, 5, 34, 0, 0, 0, 0, 0, 0, 0],\n [3, 21, 190, 19, 783, 0, 0, 0, 0, 0],\n [3, 2, 16, 907, 24, 743, 2, 274, 0, 0],\n [22, 3, 2, 115, 0, 0, 0, 0, 0, 0],\n [2907, 1223, 24, 2908, 2909, 0, 0, 0, 0, 0],\n [3, 6, 16, 905, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 16, 94, 0, 0, 0, 0, 0],\n [3, 14, 272, 375, 389, 2, 175, 0, 0, 0],\n [3, 6, 128, 75, 251, 40, 318, 24, 784, 2],\n [3, 2, 18, 7, 13, 0, 0, 0, 0, 0],\n [4, 26, 9, 818, 3, 6, 0, 0, 0, 0],\n [4, 5, 2, 161, 96, 230, 398, 8, 2, 785],\n [3, 2, 178, 502, 0, 0, 0, 0, 0, 0],\n [3, 51, 10, 655, 51, 2, 16, 3, 38, 0],\n [4, 418, 3, 10, 1217, 9, 6, 385, 0, 0],\n [3, 6, 10, 194, 1505, 7, 2910, 0, 0, 0],\n [3, 2, 908, 2911, 0, 0, 0, 0, 0, 0],\n [11, 12, 39, 7, 6, 27, 0, 0, 0, 0],\n [3, 2, 272, 649, 8, 2, 2912, 0, 0, 0],\n [4, 1224, 5, 8, 2, 77, 0, 0, 0, 0],\n [11, 350, 3, 2, 210, 185, 0, 0, 0, 0],\n [4, 13, 3, 2, 210, 185, 0, 0, 0, 0],\n [3, 14, 10, 223, 8, 10, 223, 0, 0, 0],\n [4, 26, 9, 47, 3, 69, 0, 0, 0, 0],\n [68, 3, 2, 44, 285, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 305, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 58, 180, 0, 0, 0, 0],\n [3, 6, 16, 20, 205, 0, 0, 0, 0, 0],\n [3, 6, 16, 20, 151, 0, 0, 0, 0, 0],\n [3, 6, 10, 2913, 50, 2914, 0, 0, 0, 0],\n [3, 2, 127, 269, 8, 748, 0, 0, 0, 0],\n [4, 13, 3, 2, 114, 0, 0, 0, 0, 0],\n [11, 12, 822, 5, 69, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 77, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 38, 7, 46, 64, 88, 0],\n [3, 6, 2, 2158, 9, 10, 630, 0, 0, 0],\n [4, 13, 3, 2, 77, 0, 0, 0, 0, 0],\n [5, 2, 262, 94, 188, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 209, 1047, 0, 0, 0, 0],\n [4, 5, 2, 503, 900, 7, 2, 363, 50, 0],\n [3, 2, 61, 7, 1048, 0, 0, 0, 0, 0],\n [3, 2, 67, 9, 81, 2159, 117, 1506, 9, 1225],\n [3, 2, 42, 203, 0, 0, 0, 0, 0, 0],\n [4, 3, 1507, 8, 2, 77, 0, 0, 0, 0],\n [4, 3, 211, 8, 84, 9, 2, 319, 48, 0],\n [3, 21, 355, 1508, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 42, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 2915, 115, 0, 0, 0, 0, 0],\n [3, 14, 10, 175, 7, 6, 27, 0, 0, 0],\n [3, 14, 10, 282, 7, 2, 32, 0, 0, 0],\n [3, 226, 195, 2916, 239, 824, 0, 0, 0, 0],\n [3, 6, 28, 20, 1049, 0, 0, 0, 0, 0],\n [3, 2, 35, 20, 2160, 0, 0, 0, 0, 0],\n [4, 3, 19, 2, 64, 9, 2, 424, 0, 0],\n [3, 2, 126, 120, 24, 125, 0, 0, 0, 0],\n [4, 304, 3, 21, 0, 0, 0, 0, 0, 0],\n [3, 21, 137, 0, 0, 0, 0, 0, 0, 0],\n [3, 6, 187, 177, 24, 263, 1226, 371, 149, 0],\n [5, 17, 268, 217, 2, 45, 0, 0, 0, 0],\n [11, 12, 17, 5, 601, 7, 6, 1353, 0, 0],\n [4, 13, 3, 2, 143, 0, 0, 0, 0, 0],\n [54, 222, 3, 534, 19, 2, 64, 0, 0, 0],\n [4, 129, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 129, 290, 19, 55, 46, 450, 0],\n [4, 537, 9, 308, 5, 69, 0, 0, 0, 0],\n [3, 6, 59, 2161, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 8, 2, 157, 0, 0, 0],\n [4, 202, 3, 2, 2917, 204, 0, 0, 0, 0],\n [3, 117, 28, 782, 239, 2, 254, 0, 0, 0],\n [4, 67, 9, 81, 3, 6, 169, 717, 474, 0],\n [11, 12, 280, 5, 14, 0, 0, 0, 0, 0],\n [4, 26, 9, 202, 3, 167, 8, 2, 275, 0],\n [22, 5, 34, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 65, 0, 0, 0, 0, 0],\n [11, 12, 2918, 5, 8, 2, 330, 0, 0, 0],\n [4, 26, 9, 178, 3, 95, 357, 50, 0, 0],\n [3, 6, 10, 2162, 0, 0, 0, 0, 0, 0],\n [3, 2, 16, 269, 182, 19, 46, 347, 0, 0],\n [4, 3, 2, 136, 718, 0, 0, 0, 0, 0],\n [4, 786, 104, 281, 30, 2919, 6, 3, 10, 114],\n [15, 21, 79, 102, 10, 364, 2163, 8, 2, 517],\n [3, 2, 306, 130, 1509, 24, 749, 0, 0, 0],\n [3, 14, 10, 165, 153, 7, 6, 18, 0, 0],\n [15, 6, 28, 2921, 2922, 0, 0, 0, 0, 0],\n [4, 15, 2, 270, 98, 343, 3, 978, 190, 2],\n [4, 3, 2, 98, 94, 0, 0, 0, 0, 0],\n [5, 23, 39, 316, 0, 0, 0, 0, 0, 0],\n [4, 33, 9, 327, 3, 6, 0, 0, 0, 0],\n [4, 70, 29, 34, 378, 0, 0, 0, 0, 0],\n [22, 5, 2, 17, 53, 0, 0, 0, 0, 0],\n [15, 6, 28, 25, 10, 909, 0, 0, 0, 0],\n [4, 3, 95, 31, 0, 0, 0, 0, 0, 0],\n [3, 49, 107, 556, 24, 1768, 0, 0, 0, 0],\n [11, 12, 5, 416, 289, 0, 0, 0, 0, 0],\n [3, 2, 188, 125, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 1510, 138, 7, 2, 530, 9, 6],\n [4, 3, 2, 118, 9, 2, 2923, 979, 0, 0],\n [22, 3, 6, 0, 0, 0, 0, 0, 0, 0],\n [15, 2, 488, 475, 79, 225, 0, 0, 0, 0],\n [2924, 2, 178, 0, 0, 0, 0, 0, 0, 0],\n [4, 414, 5, 8, 2, 221, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 425, 9, 2, 222],\n [11, 12, 750, 5, 8, 2, 66, 9, 2, 152],\n [4, 3, 7, 2, 221, 0, 0, 0, 0, 0],\n [11, 1354, 5, 2, 438, 0, 0, 0, 0, 0],\n [3, 6, 10, 1769, 1355, 0, 0, 0, 0, 0],\n [5, 14, 10, 475, 9, 438, 0, 0, 0, 0],\n [4, 489, 3, 8, 2, 284, 0, 0, 0, 0],\n [3, 2, 16, 107, 19, 516, 0, 0, 0, 0],\n [22, 5, 34, 80, 0, 0, 0, 0, 0, 0],\n [4, 15, 2, 980, 64, 48, 109, 0, 0, 0],\n [3, 21, 161, 96, 85, 0, 0, 0, 0, 0],\n [5, 23, 121, 206, 1227, 1511, 0, 0, 0, 0],\n [41, 30, 55, 2, 272, 0, 0, 0, 0, 0],\n [4, 2925, 3, 2, 35, 2164, 0, 0, 0, 0],\n [4, 3, 7, 2, 18, 0, 0, 0, 0, 0],\n [4, 1770, 5, 2, 825, 86, 0, 0, 0, 0],\n [4, 29, 2, 206, 500, 19, 1356, 158, 650, 0],\n [22, 3, 2, 28, 7, 120, 358, 24, 101, 0],\n [3, 6, 28, 80, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 2926, 0, 0, 0, 0, 0, 0],\n [4, 33, 9, 114, 3, 7, 2, 66, 0, 0],\n [22, 3, 6, 0, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 92, 126, 0, 0, 0, 0, 0],\n [498, 15, 2927, 9, 2, 2928, 159, 0, 0, 0],\n [4, 13, 3, 2, 143, 0, 0, 0, 0, 0],\n [4, 3, 213, 2, 262, 351, 0, 0, 0, 0],\n [11, 12, 174, 29, 30, 55, 0, 0, 0, 0],\n [104, 6, 181, 1050, 1512, 0, 0, 0, 0, 0],\n [4, 111, 3, 83, 300, 0, 0, 0, 0, 0],\n [11, 12, 17, 87, 865, 144, 464, 108, 9, 2],\n [4, 13, 5, 158, 490, 0, 0, 0, 0, 0],\n [5, 23, 17, 107, 105, 24, 140, 0, 0, 0],\n [11, 12, 504, 5, 159, 0, 0, 0, 0, 0],\n [11, 12, 2929, 5, 14, 7, 2, 32, 0, 0],\n [11, 12, 39, 5, 14, 0, 0, 0, 0, 0],\n [29, 30, 55, 43, 215, 0, 0, 0, 0, 0],\n [11, 148, 23, 354, 1771, 1772, 689, 0, 0, 0],\n [3, 6, 37, 10, 2165, 354, 0, 0, 0, 0],\n [4, 32, 9, 2, 229, 3, 6, 0, 0, 0],\n [4, 3, 8, 2, 152, 0, 0, 0, 0, 0],\n [4, 3, 2, 271, 581, 260, 0, 0, 0, 0],\n [3, 6, 10, 301, 0, 0, 0, 0, 0, 0],\n [25, 30, 1120, 131, 2166, 10, 319, 48, 0, 0],\n [4, 3, 2, 491, 53, 8, 0, 0, 0, 0],\n [11, 2930, 15, 2, 903, 79, 0, 0, 0, 0],\n [89, 6, 27, 75, 40, 318, 0, 0, 0, 0],\n [11, 12, 910, 5, 8, 6, 505, 0, 0, 0],\n [4, 26, 9, 139, 3, 51, 0, 0, 0, 0],\n [3, 6, 10, 307, 324, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 322, 0, 0, 0, 0, 0],\n [4, 26, 9, 135, 5, 23, 0, 0, 0, 0],\n [15, 2, 1513, 25, 981, 0, 0, 0, 0, 0],\n [189, 2, 98, 281, 10, 1773, 0, 0, 0, 0],\n [4, 13, 3, 82, 130, 0, 0, 0, 0, 0],\n [4, 3, 7, 101, 9, 2, 1345, 0, 0, 0],\n [11, 359, 29, 2, 2931, 2932, 506, 911, 0, 0],\n [11, 359, 5, 2, 247, 19, 2933, 0, 0, 0],\n [3, 6, 27, 161, 96, 85, 0, 0, 0, 0],\n [3, 14, 447, 110, 0, 0, 0, 0, 0, 0],\n [22, 3, 6, 16, 107, 19, 826, 0, 0, 0],\n [148, 95, 323, 335, 10, 194, 153, 0, 0, 0],\n [4, 5, 34, 31, 0, 0, 0, 0, 0, 0],\n [22, 3, 2, 44, 107, 0, 0, 0, 0, 0],\n [4, 3, 2, 138, 73, 314, 260, 0, 0, 0],\n [4, 3, 95, 38, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 824, 251, 19, 159, 59, 218, 0],\n [11, 12, 1228, 5, 7, 2, 18, 0, 0, 0],\n [4, 13, 3, 2, 1121, 518, 0, 0, 0, 0],\n [4, 406, 3, 8, 2, 58, 134, 0, 0, 0],\n [866, 181, 3, 21, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 58, 310, 0, 0, 0, 0],\n [4, 3, 2, 16, 685, 19, 0, 0, 0, 0],\n [5, 14, 1357, 0, 0, 0, 0, 0, 0, 0],\n [3, 21, 10, 328, 81, 0, 0, 0, 0, 0],\n [5, 14, 297, 7, 2, 66, 0, 0, 0, 0],\n [22, 3, 2, 129, 0, 0, 0, 0, 0, 0],\n [11, 12, 187, 243, 29, 30, 55, 0, 0, 0],\n [4, 147, 9, 867, 3, 7, 2, 18, 0, 0],\n [4, 3, 211, 8, 2, 246, 0, 0, 0, 0],\n [11, 12, 17, 5, 53, 8, 2, 78, 0, 0],\n [4, 3, 6, 28, 20, 8, 158, 170, 0, 0],\n [4, 13, 5, 2, 262, 0, 0, 0, 0, 0],\n [4, 3, 123, 2, 262, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 58, 234, 0, 0, 0, 0],\n [4, 13, 3, 2, 44, 0, 0, 0, 0, 0],\n [3, 6, 146, 225, 0, 0, 0, 0, 0, 0],\n [3, 6, 98, 20, 10, 827, 0, 0, 0, 0],\n [3, 10, 519, 9, 56, 237, 2, 17, 0, 0],\n [4, 104, 36, 236, 0, 0, 0, 0, 0, 0],\n [68, 3, 2, 483, 138, 2, 235, 1514, 0, 0],\n [3, 2, 129, 416, 24, 399, 551, 0, 0, 0],\n [54, 292, 3, 7, 2, 358, 0, 0, 0, 0],\n [3, 520, 1122, 211, 24, 2934, 8, 2, 242, 0],\n [4, 39, 5, 7, 2, 128, 0, 0, 0, 0],\n [5, 23, 39, 2935, 0, 0, 0, 0, 0, 0],\n [2, 28, 3, 658, 0, 0, 0, 0, 0, 0],\n [11, 12, 629, 5, 142, 0, 0, 0, 0, 0],\n [5, 23, 787, 0, 0, 0, 0, 0, 0, 0],\n [4, 90, 3, 8, 2, 184, 861, 0, 0, 0],\n [3, 6, 10, 136, 630, 653, 0, 0, 0, 0],\n [54, 88, 1051, 2, 396, 0, 0, 0, 0, 0],\n [4, 13, 5, 46, 205, 0, 0, 0, 0, 0],\n [4, 88, 3, 2, 16, 38, 2, 194, 1229, 7],\n [3, 14, 10, 2167, 182, 19, 2, 186, 0, 0],\n [11, 12, 276, 2168, 828, 5, 100, 0, 0, 0],\n [4, 33, 9, 70, 3, 6, 0, 0, 0, 0],\n [4, 2169, 6, 16, 55, 751, 1515, 0, 0, 0],\n [4, 5, 2, 17, 94, 0, 0, 0, 0, 0],\n [68, 3, 2, 1230, 2936, 2170, 105, 40, 2, 325],\n [3, 2, 2171, 9, 602, 1516, 0, 0, 0, 0],\n [5, 235, 17, 53, 24, 80, 0, 0, 0, 0],\n [3, 6, 10, 659, 18, 0, 0, 0, 0, 0],\n [68, 5, 2, 2937, 145, 8, 0, 0, 0, 0],\n [11, 12, 1774, 15, 2, 226, 376, 0, 0, 0],\n [3, 6, 40, 2, 201, 0, 0, 0, 0, 0],\n [4, 32, 7, 2, 229, 5, 2, 113, 53, 7],\n [3, 10, 16, 38, 10, 344, 221, 0, 0, 0],\n [5, 14, 17, 110, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 20, 151, 0, 0, 0, 0],\n [22, 3, 2, 2938, 0, 0, 0, 0, 0, 0],\n [3, 2, 35, 20, 10, 118, 147, 912, 52, 0],\n [4, 26, 9, 47, 3, 8, 2, 63, 0, 0],\n [3, 14, 338, 660, 8, 60, 0, 0, 0, 0],\n [4, 219, 3, 2, 166, 268, 0, 0, 0, 0],\n [4, 3, 2, 655, 0, 0, 0, 0, 0, 0],\n [11, 12, 39, 5, 69, 0, 0, 0, 0, 0],\n [4, 29, 2, 538, 595, 0, 0, 0, 0, 0],\n [3, 2, 157, 1517, 0, 0, 0, 0, 0, 0],\n [29, 97, 2, 297, 25, 1518, 7, 158, 257, 0],\n [11, 12, 408, 365, 9, 139, 5, 69, 0, 0],\n [3, 14, 661, 139, 8, 2, 60, 0, 0, 0],\n [3, 6, 139, 50, 59, 492, 0, 0, 0, 0],\n [4, 3, 2, 2172, 138, 7, 2, 132, 0, 0],\n [4, 3, 2, 132, 62, 86, 0, 0, 0, 0],\n [4, 3, 123, 2, 57, 0, 0, 0, 0, 0],\n [4, 3, 2, 103, 2939, 0, 0, 0, 0, 0],\n [3, 6, 10, 1761, 71, 0, 0, 0, 0, 0],\n [3, 2, 28, 94, 73, 1519, 0, 0, 0, 0],\n [4, 26, 9, 47, 3, 2, 557, 507, 0, 0],\n [5, 14, 360, 8, 2, 63, 0, 0, 0, 0],\n [4, 5, 2, 1231, 8, 6, 71, 0, 0, 0],\n [15, 6, 79, 102, 59, 2940, 0, 0, 0, 0],\n [3, 117, 9, 2, 2173, 69, 110, 355, 7, 2174],\n [4, 26, 9, 71, 3, 6, 0, 0, 0, 0],\n [29, 30, 55, 1123, 0, 0, 0, 0, 0, 0],\n [11, 12, 349, 5, 982, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 453, 0, 0, 0, 0, 0],\n [11, 12, 2175, 5, 8, 46, 246, 0, 0, 0],\n [41, 30, 55, 2, 321, 244, 0, 0, 0, 0],\n [5, 2, 1124, 80, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 0, 0, 0, 0, 0, 0, 0],\n [87, 2, 354, 144, 94, 188, 0, 0, 0, 0],\n [15, 6, 28, 25, 183, 253, 8, 2, 78, 0],\n [4, 147, 197, 3, 49, 38, 0, 0, 0, 0],\n [11, 12, 2176, 2177, 29, 30, 55, 0, 0, 0],\n [4, 32, 3, 2, 16, 7, 0, 0, 0, 0],\n [3, 14, 10, 406, 2941, 8, 2, 58, 52, 0],\n [5, 34, 38, 2942, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 103, 31, 0, 0, 0, 0, 0],\n [3, 2, 122, 51, 3, 352, 123, 2, 439, 103],\n [4, 33, 9, 129, 3, 6, 0, 0, 0, 0],\n [99, 5, 119, 2, 386, 0, 0, 0, 0, 0],\n [5, 97, 2, 629, 8, 0, 0, 0, 0, 0],\n [4, 37, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 72, 80, 8, 0, 0, 0, 0],\n [4, 5, 97, 9, 2, 690, 53, 8, 0, 0],\n [4, 3, 984, 379, 2, 2943, 0, 0, 0, 0],\n [4, 13, 3, 2, 186, 8, 2, 64, 0, 0],\n [4, 293, 3, 2, 320, 62, 9, 0, 0, 0],\n [4, 3, 177, 2, 2179, 351, 0, 0, 0, 0],\n [11, 12, 829, 145, 5, 14, 0, 0, 0, 0],\n [11, 12, 168, 788, 25, 117, 1743, 0, 0, 0],\n [4, 13, 603, 3, 49, 20, 0, 0, 0, 0],\n [4, 293, 3, 2, 132, 62, 86, 0, 0, 0],\n [3, 6, 59, 440, 24, 222, 0, 0, 0, 0],\n [3, 2, 76, 434, 0, 0, 0, 0, 0, 0],\n [11, 12, 662, 0, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 13, 9, 2, 229, 0, 0, 0],\n [4, 3, 2, 913, 31, 0, 0, 0, 0, 0],\n [5, 34, 40, 10, 339, 0, 0, 0, 0, 0],\n [4, 401, 3, 2, 16, 31, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 56, 0, 0, 0],\n [5, 14, 112, 753, 0, 0, 0, 0, 0, 0],\n [5, 2, 286, 20, 1125, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 286, 0, 0, 0, 0, 0],\n [4, 70, 3, 6, 0, 0, 0, 0, 0, 0],\n [4, 70, 3, 6, 28, 31, 0, 0, 0, 0],\n [5, 2, 121, 17, 7, 6, 93, 9, 2, 133],\n [68, 104, 49, 36, 357, 102, 6, 0, 0, 0],\n [4, 13, 3, 2, 77, 0, 0, 0, 0, 0],\n [3, 6, 18, 75, 199, 0, 0, 0, 0, 0],\n [15, 6, 663, 102, 10, 251, 51, 179, 2944, 0],\n [25, 23, 17, 144, 464, 47, 656, 0, 0, 0],\n [11, 12, 206, 5, 20, 85, 1775, 0, 0, 0],\n [99, 3, 80, 0, 0, 0, 0, 0, 0, 0],\n [4, 37, 3, 456, 8, 2, 45, 0, 0, 0],\n [4, 13, 3, 2, 129, 0, 0, 0, 0, 0],\n [3, 6, 71, 2945, 0, 0, 0, 0, 0, 0],\n [4, 5, 23, 39, 0, 0, 0, 0, 0, 0],\n [11, 12, 160, 5, 14, 0, 0, 0, 0, 0],\n [5, 183, 160, 80, 0, 0, 0, 0, 0, 0],\n [5, 2, 160, 332, 2, 133, 219, 0, 0, 0],\n [22, 3, 2, 122, 285, 0, 0, 0, 0, 0],\n [22, 5, 2, 286, 53, 0, 0, 0, 0, 0],\n [29, 23, 39, 25, 1126, 0, 0, 0, 0, 0],\n [4, 13, 5, 23, 39, 0, 0, 0, 0, 0],\n [4, 3, 2, 232, 80, 123, 0, 0, 0, 0],\n [3, 2, 175, 159, 24, 345, 0, 0, 0, 0],\n [3, 2, 16, 20, 10, 204, 0, 0, 0, 0],\n [3, 2, 18, 7, 914, 0, 0, 0, 0, 0],\n [4, 3, 2, 57, 31, 0, 0, 0, 0, 0],\n [3, 6, 57, 1358, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 57, 316, 108, 9, 0, 0, 0],\n [3, 6, 57, 356, 0, 0, 0, 0, 0, 0],\n [4, 138, 3, 2, 42, 830, 73, 0, 0, 0],\n [4, 266, 3, 2, 321, 664, 0, 0, 0, 0],\n [3, 2, 57, 789, 0, 0, 0, 0, 0, 0],\n [3, 51, 10, 44, 0, 0, 0, 0, 0, 0],\n [68, 3, 2, 35, 268, 7, 101, 9, 2, 308],\n [4, 33, 9, 37, 457, 30, 55, 7, 2, 521],\n [3, 14, 10, 165, 576, 41, 0, 0, 0, 0],\n [11, 12, 553, 2946, 5, 7, 2, 180, 0, 0],\n [4, 3, 2, 48, 1037, 0, 0, 0, 0, 0],\n [4, 1233, 123, 2, 476, 284, 0, 0, 0, 0],\n [4, 3, 8, 2, 16, 8, 2, 2180, 2947, 0],\n [3, 2, 157, 62, 0, 0, 0, 0, 0, 0],\n [4, 5, 2, 409, 19, 2, 64, 0, 0, 0],\n [4, 3, 2181, 259, 9, 2, 218, 0, 0, 0],\n [15, 6, 169, 172, 19, 36, 915, 0, 0, 0],\n [4, 3, 2, 235, 898, 138, 7, 2, 18, 0],\n [11, 12, 39, 5, 7, 2, 209, 0, 0, 0],\n [4, 3, 2, 35, 377, 0, 0, 0, 0, 0],\n [3, 14, 10, 156, 182, 19, 2, 45, 315, 0],\n [3, 2, 28, 620, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 31, 8, 2, 681, 0, 0],\n [4, 13, 3, 2, 2948, 258, 0, 0, 0, 0],\n [3, 6, 10, 339, 0, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 31, 0, 0, 0, 0, 0],\n [4, 5, 2, 178, 1234, 40, 2, 501, 0, 0],\n [4, 13, 3, 2, 310, 0, 0, 0, 0, 0],\n [11, 149, 3, 2, 28, 0, 0, 0, 0, 0],\n [4, 32, 7, 2, 229, 3, 6, 0, 0, 0],\n [3, 6, 10, 208, 24, 466, 1235, 0, 0, 0],\n [3, 49, 7, 10, 868, 1520, 0, 0, 0, 0],\n [3, 2, 57, 199, 2, 231, 0, 0, 0, 0],\n [99, 3, 123, 2, 231, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 136, 0, 0, 0, 0, 0],\n [22, 3, 2, 136, 80, 7, 2, 27, 0, 0],\n [4, 3, 2, 44, 573, 177, 0, 0, 0, 0],\n [5, 14, 43, 173, 7, 2, 76, 0, 0, 0],\n [3, 14, 1776, 1359, 7, 6, 27, 0, 0, 0],\n [11, 12, 198, 5, 14, 0, 0, 0, 0, 0],\n [3, 14, 196, 7, 6, 27, 0, 0, 0, 0],\n [3, 6, 2, 467, 0, 0, 0, 0, 0, 0],\n [4, 13, 539, 5, 8, 2, 347, 0, 0, 0],\n [4, 13, 3, 2, 916, 8, 2, 78, 0, 0],\n [4, 3, 49, 38, 0, 0, 0, 0, 0, 0],\n [3, 6, 16, 40, 402, 0, 0, 0, 0, 0],\n [3, 2, 42, 255, 0, 0, 0, 0, 0, 0],\n [4, 5, 2, 286, 268, 8, 0, 0, 0, 0],\n [4, 13, 3, 2, 56, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 2182, 9, 2, 135, 0, 0],\n [3, 2, 214, 8, 24, 259, 0, 0, 0, 0],\n [4, 3, 2, 35, 2183, 19, 29, 0, 0, 0],\n [4, 3, 2, 831, 245, 38, 7, 82, 257, 0],\n [4, 3, 2, 35, 31, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 38, 0, 0, 0, 0, 0],\n [5, 34, 7, 2184, 0, 0, 0, 0, 0, 0],\n [3, 2, 76, 328, 0, 0, 0, 0, 0, 0],\n [4, 401, 15, 95, 663, 19, 36, 7, 2, 358],\n [4, 1127, 3, 2, 338, 7, 6, 18, 0, 0],\n [4, 13, 3, 2, 654, 0, 0, 0, 0, 0],\n [3, 51, 10, 310, 0, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 314, 8, 2, 273, 0, 0],\n [5, 14, 12, 17, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 2949, 8, 2, 240, 0, 0, 0],\n [11, 12, 85, 135, 0, 0, 0, 0, 0, 0],\n [4, 26, 9, 122, 3, 6, 0, 0, 0, 0],\n [4, 3, 2, 165, 143, 0, 0, 0, 0, 0],\n [4, 691, 1128, 3, 7, 2, 66, 0, 0, 0],\n [3, 21, 137, 81, 0, 0, 0, 0, 0, 0],\n [4, 3, 8, 2, 58, 170, 0, 0, 0, 0],\n [5, 2, 112, 125, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 540, 156, 0, 0, 0, 0, 0],\n [179, 6, 36, 10, 2950, 1521, 0, 0, 0, 0],\n [11, 12, 17, 5, 20, 151, 0, 0, 0, 0],\n [4, 754, 5, 34, 1236, 0, 0, 0, 0, 0],\n [54, 3, 1360, 7, 2, 76, 2, 440, 24, 2],\n [22, 3, 6, 0, 0, 0, 0, 0, 0, 0],\n [3, 6, 1522, 1777, 0, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 20, 120, 0, 0, 0, 0],\n [3, 14, 196, 0, 0, 0, 0, 0, 0, 0],\n [4, 5, 2, 17, 31, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 27, 0, 0, 0, 0, 0],\n [4, 3, 2, 16, 80, 8, 0, 0, 0, 0],\n [3, 14, 235, 207, 10, 2185, 7, 6, 61, 0],\n [4, 70, 3, 49, 65, 0, 0, 0, 0, 0],\n [4, 493, 9, 1237, 3, 6, 0, 0, 0, 0],\n [4, 754, 3, 83, 2186, 0, 0, 0, 0, 0],\n [4, 67, 3, 8, 2, 115, 0, 0, 0, 0],\n [11, 12, 173, 5, 7, 2, 76, 0, 0, 0],\n [11, 12, 174, 0, 0, 0, 0, 0, 0, 0],\n [4, 32, 3, 6, 0, 0, 0, 0, 0, 0],\n [22, 3, 2, 424, 0, 0, 0, 0, 0, 0],\n [11, 12, 275, 5, 40, 2, 60, 0, 0, 0],\n [11, 12, 171, 5, 14, 0, 0, 0, 0, 0],\n [15, 6, 136, 25, 1361, 213, 403, 490, 0, 0],\n [11, 12, 256, 5, 14, 110, 0, 0, 0, 0],\n [4, 26, 9, 44, 3, 51, 0, 0, 0, 0],\n [4, 143, 3, 123, 2, 39, 0, 0, 0, 0],\n [3, 14, 10, 604, 37, 7, 2, 27, 0, 0],\n [68, 104, 162, 2187, 6, 1778, 8, 2, 441, 0],\n [3, 14, 10, 251, 8, 2, 522, 24, 1779, 2188],\n [15, 95, 79, 203, 19, 36, 7, 10, 27, 0],\n [11, 12, 523, 9, 344, 5, 8, 2, 366, 0],\n [4, 13, 3, 2, 1230, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 77, 0, 0, 0, 0, 0],\n [11, 12, 253, 5, 14, 0, 0, 0, 0, 0],\n [41, 2, 210, 1523, 390, 6, 0, 0, 0, 0],\n [22, 3, 2, 2189, 0, 0, 0, 0, 0, 0],\n [3, 14, 10, 1524, 387, 0, 0, 0, 0, 0],\n [22, 5, 2, 917, 0, 0, 0, 0, 0, 0],\n [3, 21, 2952, 50, 6, 37, 19, 36, 8, 2],\n [3, 14, 520, 8, 2, 366, 0, 0, 0, 0],\n [4, 13, 5, 2, 582, 0, 0, 0, 0, 0],\n [11, 12, 92, 145, 5, 69, 0, 0, 0, 0],\n [4, 3, 2, 974, 1362, 0, 0, 0, 0, 0],\n [11, 12, 112, 41, 30, 55, 0, 0, 0, 0],\n [11, 12, 120, 226, 145, 5, 100, 7, 6, 18],\n [3, 2, 32, 1517, 0, 0, 0, 0, 0, 0],\n [3, 6, 10, 72, 24, 10, 98, 0, 0, 0],\n [3, 14, 10, 790, 8, 2, 1129, 0, 0, 0],\n [15, 2, 692, 25, 314, 0, 0, 0, 0, 0],\n [11, 41, 30, 524, 2, 72, 87, 144, 53, 140],\n [3, 21, 829, 7, 14, 0, 0, 0, 0, 0],\n [22, 5, 2, 17, 91, 40, 0, 0, 0, 0],\n [3, 192, 7, 2, 157, 0, 0, 0, 0, 0],\n [3, 2, 35, 20, 151, 0, 0, 0, 0, 0],\n [11, 12, 17, 25, 750, 0, 0, 0, 0, 0],\n [3, 2, 16, 119, 10, 224, 0, 0, 0, 0],\n [4, 13, 3, 2, 58, 56, 341, 0, 0, 0],\n [4, 3, 2, 33, 9, 293, 167, 50, 2, 2953],\n [3, 14, 10, 201, 7, 6, 128, 0, 0, 0],\n [4, 3, 182, 19, 2, 200, 0, 0, 0, 0],\n [4, 26, 9, 200, 5, 23, 0, 0, 0, 0],\n [3, 6, 10, 869, 1780, 169, 0, 0, 0, 0],\n [11, 12, 275, 5, 7, 6, 18, 0, 0, 0],\n [4, 13, 3, 2, 458, 7, 2, 27, 0, 0],\n [4, 13, 3, 2, 985, 0, 0, 0, 0, 0],\n [4, 489, 3, 8, 2, 27, 0, 0, 0, 0],\n [3, 2, 150, 268, 0, 0, 0, 0, 0, 0],\n [54, 37, 3, 356, 7, 2, 458, 0, 0, 0],\n [3, 2, 42, 356, 0, 0, 0, 0, 0, 0],\n [3, 2, 918, 898, 0, 0, 0, 0, 0, 0],\n [4, 3, 6, 494, 260, 0, 0, 0, 0, 0],\n [3, 2, 129, 2954, 0, 0, 0, 0, 0, 0],\n [3, 2, 103, 237, 112, 0, 0, 0, 0, 0],\n [5, 14, 572, 112, 8, 6, 1363, 0, 0, 0],\n [29, 30, 55, 10, 1781, 8, 2, 78, 0, 0],\n [4, 3, 216, 2, 282, 0, 0, 0, 0, 0],\n [3, 2, 106, 631, 105, 0, 0, 0, 0, 0],\n [3, 2, 576, 41, 434, 0, 0, 0, 0, 0],\n [3, 6, 146, 19, 384, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 919, 305, 0, 0, 0, 0],\n [5, 2, 17, 357, 50, 2, 178, 0, 0, 0],\n [4, 147, 3, 2, 2190, 0, 0, 0, 0, 0],\n [3, 14, 10, 1130, 8, 2, 476, 60, 0, 0],\n [4, 3, 2, 16, 80, 7, 0, 0, 0, 0],\n [3, 2, 57, 380, 0, 0, 0, 0, 0, 0],\n [87, 6, 71, 865, 144, 1131, 0, 0, 0, 0],\n [4, 3, 2, 118, 9, 2, 208, 423, 51, 3],\n [5, 2, 39, 7, 158, 419, 521, 0, 0, 0],\n [4, 90, 3, 6, 693, 252, 548, 0, 0, 0],\n [11, 12, 17, 73, 165, 558, 0, 0, 0, 0],\n [4, 13, 3, 2, 48, 0, 0, 0, 0, 0],\n [4, 13, 3, 82, 52, 0, 0, 0, 0, 0],\n [11, 12, 17, 5, 7, 2, 208, 0, 0, 0],\n [4, 33, 9, 37, 3, 6, 0, 0, 0, 0],\n [15, 2, 42, 25, 307, 583, 0, 0, 0, 0],\n [15, 2, 166, 25, 755, 0, 0, 0, 0, 0],\n [4, 694, 9, 427, 3, 8, 2, 254, 141, 9],\n [3, 6, 10, 442, 71, 0, 0, 0, 0, 0],\n [4, 3, 2, 28, 31, 0, 0, 0, 0, 0],\n [99, 3, 2, 28, 8, 2, 246, 0, 0, 0],\n [68, 3, 2, 16, 1782, 8, 10, 347, 0, 0],\n [3, 6, 59, 467, 24, 665, 0, 0, 0, 0],\n [5, 14, 43, 173, 7, 2, 76, 0, 0, 0],\n [4, 3, 95, 31, 0, 0, 0, 0, 0, 0],\n [22, 3, 2, 57, 0, 0, 0, 0, 0, 0],\n [3, 6, 18, 7, 13, 0, 0, 0, 0, 0],\n [4, 3, 2955, 108, 9, 2, 56, 7, 2, 363],\n [4, 3, 2, 230, 138, 7, 2, 56, 0, 0],\n [4, 15, 21, 172, 51, 2, 16, 8, 2, 417],\n [3, 6, 86, 2956, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 63, 0, 0, 0, 0, 0],\n [3, 6, 47, 340, 50, 30, 0, 0, 0, 0],\n [5, 311, 1525, 247, 50, 2957, 0, 0, 0, 0],\n [15, 6, 28, 25, 10, 2958, 1364, 2191, 2192, 0],\n [148, 2, 61, 513, 259, 0, 0, 0, 0, 0],\n [3, 49, 7, 10, 45, 0, 0, 0, 0, 0],\n [3, 6, 10, 463, 0, 0, 0, 0, 0, 0],\n [11, 12, 1365, 0, 0, 0, 0, 0, 0, 0],\n [11, 12, 2959, 9, 116, 5, 391, 278, 0, 0],\n [4, 3, 50, 921, 177, 6, 1345, 0, 0, 0],\n [4, 5, 2, 17, 53, 8, 0, 0, 0, 0],\n [15, 6, 2193, 25, 2194, 1783, 0, 0, 0, 0],\n [11, 12, 251, 1784, 5, 100, 0, 0, 0, 0],\n [11, 12, 248, 922, 5, 100, 0, 0, 0, 0],\n [3, 6, 10, 495, 48, 0, 0, 0, 0, 0],\n [3, 2, 71, 477, 0, 0, 0, 0, 0, 0],\n [29, 30, 343, 2, 525, 3, 1526, 1785, 19, 2],\n [4, 2, 16, 38, 0, 0, 0, 0, 0, 0],\n [15, 2, 42, 25, 10, 688, 0, 0, 0, 0],\n [41, 30, 55, 43, 168, 0, 0, 0, 0, 0],\n [4, 13, 5, 2, 1366, 101, 256, 0, 0, 0],\n [4, 37, 3, 14, 0, 0, 0, 0, 0, 0],\n [68, 457, 117, 2960, 2, 1786, 3, 2961, 110, 0],\n [22, 3, 2, 58, 909, 0, 0, 0, 0, 0],\n [87, 2, 16, 584, 1787, 10, 2195, 0, 0, 0],\n [4, 3, 986, 263, 2, 58, 170, 0, 0, 0],\n [4, 708, 247, 5, 8, 2, 60, 0, 0, 0],\n [4, 5, 2, 155, 8, 6, 354, 0, 0, 0],\n [4, 33, 9, 1132, 3, 7, 2, 221, 0, 0],\n [3, 6, 10, 137, 81, 0, 0, 0, 0, 0],\n [3, 14, 10, 666, 44, 8, 2, 156, 0, 0],\n [22, 15, 6, 478, 496, 0, 0, 0, 0, 0],\n [4, 141, 9, 2, 410, 5, 2, 231, 870, 8],\n [4, 13, 3, 2, 56, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 27, 0, 0, 0, 0, 0],\n [4, 13, 3, 2, 301, 0, 0, 0, 0, 0],\n [4, 5, 34, 107, 19, 281, 73, 23, 302, 0],\n [22, 3, 2, 57, 0, 0, 0, 0, 0, 0],\n [15, 2, 72, 361, 2962, 0, 0, 0, 0, 0],\n [11, 12, 2963, 5, 7, 2, 18, 0, 0, 0],\n [4, 911, 2964, 0, 0, 0, 0, 0, 0, 0],\n [4, 13, 3, 6, 127, 0, 0, 0, 0, 0],\n [4, 521, 3, 6, 127, 7, 0, 0, 0, 0],\n ...]"}, "metadata": {}}]}, {"cell_type": "code", "source": "#convert word to idx", "metadata": {}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": "x = [(idx,len(item)) for idx,item in enumerate(train_questions)]", "metadata": {}, "execution_count": 9, "outputs": []}, {"cell_type": "markdown", "source": "## 3.\u8bad\u7ec3\u8fc7\u7a0b", "metadata": {}}, {"cell_type": "markdown", "source": "### 3.1 \u8d85\u53c2\u6570\u8bbe\u7f6e", "metadata": {}}, {"cell_type": "code", "source": "#from easydict import EasyDict as edict\noptions = OrderedDict()\n# data related\noptions['data_path'] = './data/'\n#options['feature_file'] = 'trainval_feat.h5'\n#options['expt_folder'] = 'expt_1'\noptions['model_name'] = 'imageqa'\noptions['train_split'] = 'trainval1'\noptions['val_split'] = 'val2'\noptions['shuffle'] = True\noptions['reverse'] = True\noptions['sample_answer'] = True\n\noptions['num_region'] = 196\noptions['region_dim'] = 512\n\noptions['n_words'] = 13746\noptions['n_output'] = 1000\n\n# structure options\noptions['combined_num_mlp'] = 1\noptions['combined_mlp_drop_0'] = True\noptions['combined_mlp_act_0'] = 'linear'\noptions['sent_drop'] = False\noptions['use_tanh'] = False\n\noptions['use_attention_drop'] = False\n\n# dimensions\noptions['n_emb'] = 500\noptions['n_dim'] = 1024\noptions['n_image_feat'] = options['region_dim']\noptions['n_common_feat'] = 500\noptions['n_attention'] = 512\n\n# initialization\noptions['init_type'] = 'uniform'\noptions['range'] = 0.01\noptions['std'] = 0.01\noptions['init_lstm_svd'] = False\n\noptions['forget_bias'] = np.float32(1.0)\n\n# learning parameters\noptions['optimization'] = 'sgd' # choices\noptions['batch_size'] = 100\noptions['lr'] = np.float32(0.05)\noptions['w_emb_lr'] = np.float32(80)\noptions['momentum'] = np.float32(0.9)\noptions['gamma'] = 1\noptions['step'] = 10\noptions['step_start'] = 100\noptions['max_epochs'] = 50\noptions['weight_decay'] = 0.0005\noptions['decay_rate'] = np.float32(0.999)\noptions['drop_ratio'] = np.float32(0.5)\noptions['smooth'] = np.float32(1e-8)\noptions['grad_clip'] = np.float32(0.1)\n\n# log params\noptions['disp_interval'] = 10\noptions['eval_interval'] = 1000\noptions['save_interval'] = 500\n\n#new\noptions['dict_size'] = 6620", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "code", "source": "# vgg\u6a21\u578b\u5bfc\u5165\u76f8\u5173\u5305", "metadata": {}, "execution_count": 11, "outputs": []}, {"cell_type": "code", "source": "# vgg model\nmox.file.copy_parallel(src_url=\"s3://nlp.final/san/code/image_model/src\",dst_url=\"./src/\")\nfrom PIL import Image, ImageFile\nfrom src.utils.logging import get_logger\nfrom src.dataset import classification_dataset\nfrom easydict import EasyDict as edict\nfrom src.config import imagenet_cfg as vgg_cfg\nfrom src.vgg import vgg16\n\nimport time\nimport datetime\nimport mindspore.dataset.vision.py_transforms as vision", "metadata": {}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "image_path = {\n    \"train\":\"./data/images/train\",\n    \"test\":\"./data/images/test\",\n    \"val\":\"./data/images/val\"\n}\n\nannotation_path = {\n    \"train\":\"./data/annotation/train.json\",\n    \"test\":\"./data/annotation/test.json\",\n    \"val\":\"./data/annotation/val.json\"\n}\n\nmodel_cfg= edict({\n    \"annotation\":annotation_path,\n    \"image\":image_path,\n    \"cnn_ckpt_19\":\"./data/vgg19_ascend_v111_imagenet2012_research_cv_bs64_acc74.ckpt\",\n    \"cnn_ckpt_16\":\"./data/vgg16_ascend_v120_imagenet2012_official_cv_bs32_acc73.ckpt\",\n    \"log_path\":\"./outputs\",\n\n    \"device_target\": 'Ascend',\n    \"per_batch_size\": 32,\n    \"graph_ckpt\":1,\n    \"rank\": 0,\n    \"group_size\":1\n})", "metadata": {}, "execution_count": 13, "outputs": []}, {"cell_type": "code", "source": "context.set_context(mode=context.GRAPH_MODE, device_target=model_cfg.device_target, device_id=0,enable_auto_mixed_precision=True,save_graphs=False)", "metadata": {}, "execution_count": 14, "outputs": []}, {"cell_type": "code", "source": "model_cfg.outputs_dir = os.path.join(model_cfg.log_path,\n                                    datetime.datetime.now().strftime('%Y-%m-%d_time_%H_%M_%S'))\n\nmodel_cfg.logger = get_logger(model_cfg.outputs_dir, model_cfg.rank)", "metadata": {}, "execution_count": 16, "outputs": []}, {"cell_type": "markdown", "source": "### 3.2\u6a21\u578b\u642d\u5efa", "metadata": {}}, {"cell_type": "code", "source": "# \u5bfc\u5165vgg\u6a21\u578b\nmodel_cfg.logger.important_info('start create vgg')\nvgg = vgg16(vgg_cfg.num_classes, vgg_cfg, phase=\"test\",include_top = False)\nvgg.add_flags_recursive(fp16=False)\nvgg.set_train(False)\nmodel_cfg.logger.important_info('start load checkpoint')\nparam_dict = load_checkpoint(model_cfg.cnn_ckpt_16)\nload_param_into_net(vgg, param_dict)", "metadata": {}, "execution_count": 17, "outputs": [{"name": "stdout", "text": "2021-07-13 14:03:54,808:INFO:\n**********************************************************************\n**********************************************************************\n**\n**\n**        start create vgg\n**\n**\n**********************************************************************\n**********************************************************************\n\ninit network\n2021-07-13 14:04:04,804:INFO:\n**********************************************************************\n**********************************************************************\n**\n**\n**        start load checkpoint\n**\n**\n**********************************************************************\n**********************************************************************\n\n", "output_type": "stream"}, {"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "[]"}, "metadata": {}}]}, {"cell_type": "code", "source": "def img2tensor(img_root,img_prefix,img_id):\n        img_path = os.path.join(img_root,img_prefix+\"_\"+str(img_id).zfill(12)+\".jpg\")\n        print(\"img_path:\",img_path)\n        img = Image.open(img_path).convert('RGB')\n        transform = edict({\n#             \"Decode\": vision.Decode(),\n            \"Resize\": vision.Resize((512, 512)),\n            \"CenterCrop\": vision.CenterCrop(448),\n#             \"Normalize\": vision.Normalize(mean=mean, std=std),\n#             \"HWC2CHW\": vision.HWC2CHW(),\n            \"ToTensor\":vision.ToTensor()\n        })\n        # img = transform.Decode(img)\n        img = transform.Resize(img)\n        img = transform.CenterCrop(img)\n        img = transform.ToTensor(img)\n        print(\"totensor:\",img)\n    #     img = transform.Normalize(img) # CHW\n    #     print(\"normalized:\",img)\n    #     img = transform.HWC2CHW(img)\n        img = [img]\n        tensor = Tensor(img,mstype.float32)\n        return tensor", "metadata": {}, "execution_count": 18, "outputs": []}, {"cell_type": "code", "source": "def get_feature_map(tensor):\n    model_cfg.logger.important_info('start get feature map')\n    output = vgg(tensor)  \n    return output", "metadata": {}, "execution_count": 19, "outputs": []}, {"cell_type": "code", "source": "img = img2tensor(model_cfg.image.train,\"COCO_train2014\",9)\nimage_feat = get_feature_map(img)", "metadata": {}, "execution_count": 43, "outputs": [{"name": "stdout", "text": "img_path: ./data/images/train/COCO_train2014_000000000009.jpg\ntotensor: [[[0.         0.00392157 0.         ... 0.05098039 0.04705882 0.04705882]\n  [0.         0.00784314 0.00392157 ... 0.04313726 0.04705882 0.05490196]\n  [0.         0.         0.         ... 0.04313726 0.05490196 0.05490196]\n  ...\n  [0.827451   0.81960785 0.80784315 ... 0.03137255 0.01176471 0.04705882]\n  [0.81960785 0.8156863  0.8        ... 0.01568628 0.05098039 0.14901961]\n  [0.827451   0.80784315 0.79607844 ... 0.09411765 0.1764706  0.20392157]]\n\n [[0.13333334 0.13725491 0.13725491 ... 0.         0.         0.        ]\n  [0.12941177 0.13725491 0.13333334 ... 0.         0.         0.        ]\n  [0.12941177 0.14117648 0.14117648 ... 0.00392157 0.00392157 0.        ]\n  ...\n  [0.7058824  0.69411767 0.68235296 ... 0.20392157 0.19215687 0.20392157]\n  [0.69803923 0.6901961  0.6745098  ... 0.1764706  0.19215687 0.27058825]\n  [0.7058824  0.6862745  0.67058825 ... 0.22745098 0.29411766 0.30588236]]\n\n [[0.5176471  0.5137255  0.5058824  ... 0.17254902 0.16470589 0.16470589]\n  [0.5137255  0.5137255  0.5058824  ... 0.16862746 0.16470589 0.16862746]\n  [0.5176471  0.5176471  0.50980395 ... 0.16470589 0.1764706  0.18431373]\n  ...\n  [0.         0.00392157 0.00784314 ... 0.5019608  0.4627451  0.45490196]\n  [0.         0.00784314 0.         ... 0.43137255 0.44313726 0.50980395]\n  [0.00784314 0.00392157 0.00392157 ... 0.46666667 0.5372549  0.56078434]]]\n2021-07-13 14:36:16,486:INFO:\n**********************************************************************\n**********************************************************************\n**\n**\n**        start get feature map\n**\n**\n**********************************************************************\n**********************************************************************\n\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "image_feat = image_feat.transpose(0,2,3,1)", "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "execution_count": 44, "outputs": [{"execution_count": 44, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[512], dtype=Float32, value= [ 0.00000000e+00,  7.96386719e-01,  0.00000000e+00,  4.60693359e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.41162109e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.32519531e-01,  0.00000000e+00,  0.00000000e+00,  1.84765625e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.68542480e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  1.05529785e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.71966553e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  3.96679688e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.44824219e-01,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.44409180e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.37670898e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.19335938e+00,  0.00000000e+00,  1.40380859e-01,  6.02539062e-01,  0.00000000e+00,  3.67919922e-01, \n  0.00000000e+00,  6.22253418e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  1.98974609e-01,  0.00000000e+00,  0.00000000e+00,  5.78125000e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.33203125e-01,  7.23632812e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  1.88378906e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.46972656e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.80078125e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.73339844e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  8.57910156e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.19531250e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.08032227e-01,  1.82128906e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.29199219e-01,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.02246094e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.78002930e-02,  9.63867188e-01,  0.00000000e+00,  4.39208984e-01, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.13281250e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.43945312e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.06262207e-01,  2.32031250e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  1.25781250e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.90039062e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  7.69042969e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.33496094e-01, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.05273438e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.52148438e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  1.04980469e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.20312500e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  9.31640625e-01,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.03369141e-01,  0.00000000e+00,  4.14550781e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.94335938e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.09863281e-01,  0.00000000e+00,  1.13769531e-01, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.58593750e+00,  1.34570312e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.74511719e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  1.29776001e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.98046875e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00, \n  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"}, "metadata": {}}]}, {"cell_type": "code", "source": "image_feat = image_feat.reshape(1, 196, 512)\ntype(image_feat)", "metadata": {}, "execution_count": 52, "outputs": [{"execution_count": 52, "output_type": "execute_result", "data": {"text/plain": "mindspore.common.tensor.Tensor"}, "metadata": {}}]}, {"cell_type": "code", "source": "def init_weight(n, d, options):\n    ''' initialize weight matrix\n    options['init_type'] determines\n    gaussian or uniform initlizaiton\n    '''\n    if options['init_type'] == 'gaussian':\n        return (np.random.randn(n, d).astype(floatX)) * options['std']\n    elif options['init_type'] == 'uniform':\n        # [-range, range]\n        return ((np.random.rand(n, d) * 2 - 1) * \\\n                options['range']).astype(floatX)\ndef ortho_weight(ndim):\n    \"\"\"\n    Random orthogonal weights, we take\n    the right matrix in the SVD.\n\n    Remember in SVD, u has the same # rows as W\n    and v has the same # of cols as W. So we\n    are ensuring that the rows are\n    orthogonal.\n    \"\"\"\n    W = np.random.randn(ndim, ndim)\n    u, _, _ = np.linalg.svd(W)\n    return u.astype('float32')\n\ndef init_fflayer(params, nin, nout, options, prefix='ff'):\n    ''' initialize ff layer\n    '''\n    params[prefix + '_w'] = init_weight(nin, nout, options)\n    params[prefix + '_b'] = np.zeros(nout, dtype='float32')\n    return params\n\ndef init_lstm_layer(params, nin, ndim, options, prefix='lstm'):\n    ''' initializt lstm layer\n    '''\n    params[prefix + '_w_x'] = init_weight(nin, 4 * ndim, options)\n    # use svd trick to initializ\n    if options['init_lstm_svd']:\n        params[prefix + '_w_h'] = np.concatenate([ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim),\n                                                  ortho_weight(ndim)],\n                                                 axis=1)\n    else:\n        params[prefix + '_w_h'] = init_weight(ndim, 4 * ndim, options)\n    params[prefix + '_b_h'] = np.zeros(4 * ndim, dtype='float32')\n    # set forget bias to be positive\n    params[prefix + '_b_h'][ndim : 2*ndim] = np.float32(options.get('forget_bias', 0))\n    return params\n\n# initialize the parmaters\ndef init_params(options):\n    ''' Initialize all the parameters\n    '''\n    params = OrderedDict()\n    n_words = options['n_words']\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    n_image_feat = options['n_image_feat']\n    n_common_feat = options['n_common_feat']\n    n_output = options['n_output']\n    n_attention = options['n_attention']\n\n    params['w_emb'] = ((np.random.rand(n_words, n_emb) * 2 - 1) * 0.5).astype(floatX)\n\n    params = init_fflayer(params, n_image_feat, n_dim, options,\n                          prefix='image_mlp')\n\n    # attention model based parameters\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_1')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_1')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='image_att_mlp_2')\n    params = init_fflayer(params, n_dim, n_attention, options,\n                          prefix='sent_att_mlp_2')\n    params = init_fflayer(params, n_attention, 1, options,\n                          prefix='combined_att_mlp_2')\n\n\n    # params for sentence image mlp\n    for i in range(options['combined_num_mlp']):\n        if i == 0 and options['combined_num_mlp'] == 1:\n            params = init_fflayer(params, n_dim, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == 0 and options['combined_num_mlp'] != 1:\n            params = init_fflayer(params, n_dim, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n        elif i == options['combined_num_mlp'] - 1 :\n            params = init_fflayer(params, n_common_feat, n_output,\n                                  options, prefix='combined_mlp_%d'%(i))\n        else:\n            params = init_fflayer(params, n_common_feat, n_common_feat,\n                                  options, prefix='combined_mlp_%d'%(i))\n\n    # lstm layer\n    params = init_lstm_layer(params, n_emb, n_dim, options, prefix='sent_lstm')\n\n    return params\n\ndef init_shared_params(params):\n    ''' return a shared version of all parameters\n    '''\n    global shared_params\n    shared_params = OrderedDict()\n    for k, p in params.items():\n        shared_params[k] = params[k]\n\n    return shared_params\n\ndef fflayer(shared_params, x, options, prefix='ff', act_func='tanh'):\n    ''' fflayer: multiply weight then add bias\n    '''\n    tanh = nn.Tanh()\n    input = mindspore.ops.dot(x, Tensor(shared_params[prefix + '_w'])) + \\\n                          Tensor(shared_params[prefix + '_b'])\n    return tanh(input)\n\ndef get_lr(options, curr_epoch):\n    if options['optimization'] == 'sgd':\n        power = max((curr_epoch - options['step_start']) / options['step'], 0)\n        power = math.ceil(power)\n        return options['lr'] * (options['gamma'] ** power)  #\n    else:\n        return options['lr']", "metadata": {}, "execution_count": 60, "outputs": []}, {"cell_type": "code", "source": "floatX = np.float32\nbatch_size = options['batch_size']\nmax_epochs = options['max_epochs']\n\n###############\n# build model #\n###############\nparams = init_params(options)\nshared_params = init_shared_params(params)", "metadata": {}, "execution_count": 61, "outputs": []}, {"cell_type": "code", "source": "image_feat = fflayer(shared_params, image_feat, options,\n                             prefix='image_mlp',\n                              act_func='tanh')", "metadata": {}, "execution_count": 62, "outputs": []}, {"cell_type": "code", "source": "image_feat = image_feat[0]", "metadata": {}, "execution_count": 64, "outputs": []}, {"cell_type": "code", "source": "def lstm_layer(shared_params, x, mask, h_0, c_0, options, prefix='lstm'):\n    ''' lstm layer:\n    :param shared_params: shared parameters\n    :param x: input, T x batch_size x n_emb\n    :param mask: mask for x, T x batch_size\n    '''\n    n_emb = options['n_emb']\n    n_dim = options['n_dim']\n    # weight matrix for x, n_emb x 4*n_dim (ifoc)\n    lstm_w_x = shared_params[prefix + '_w_x']\n    # weight matrix for h, n_dim x 4*n_dim\n    lstm_w_h = shared_params[prefix + '_w_h']\n    lstm_b_h = shared_params[prefix + '_b_h']\n    h_0 = h_0[:x.shape[1]]\n    c_0 = c_0[:x.shape[1]]\n    question_net = LSTM(n_emb, n_dim)\n    output, (h, c) = question_net(x, (h_0, c_0))\n    return h, c", "metadata": {}, "execution_count": 37, "outputs": []}, {"cell_type": "code", "source": "def build_model(shared_params, options):\n    #input_idx = Tensor.imatrix('input_idx')\n    input_idx = Tensor()\n    global empty_word\n    empty_word = np.zeros((1, options['n_emb']), dtype='float32')\n    w_emb_extend = Tensor.concatenate([empty_word, shared_params['w_emb']],\n                                 axis=0)\n    input_emb = w_emb_extend[input_idx]\n    \n    # get the transformed image feature\n    global h_0, c_0\n    h_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    c_0 = np.zeros((batch_size, n_dim), dtype='float32')\n    h_encode, c_encode = lstm_layer(shared_params, input_emb, input_mask,\n                                    h_0, c_0, options, prefix='sent_lstm')\n    return h_encodem, c_encode", "metadata": {}, "execution_count": 38, "outputs": []}, {"cell_type": "code", "source": "input_idx = np.ones((6618,100),dtype = 'int32')\nshared_params['w_emb'] = ((np.random.rand(13746, 500) * 2 - 1) * 0.5).astype(floatX)\nempty_word = np.zeros((1, 500), dtype='float32')\nw_emb_extend = shared_params['w_emb']\ninput_emb = w_emb_extend[input_idx]", "metadata": {}, "execution_count": 40, "outputs": []}, {"cell_type": "code", "source": "class LSTM(nn.Cell):\n    def __init__(self, options, is_training=True):\n        super(LSTM, self).__init__()\n        if is_training:\n            self.batch_size = options['batch_size']\n        else:\n            self.batch_size = 1\n            \n        self.n_dim = options['n_dim']\n        self.n_emb = options['n_emb']\n        self.dropout = options['drop_ratio']\n        \n        # TODO\n        self.h = Tensor(np.zeros((1,self.batch_size, self.n_dim), dtype='float32'))\n        self.c = Tensor(np.zeros((1,self.batch_size, self.n_dim), dtype='float32'))\n        \n        self.rnn = nn.LSTM(self.n_emb,self.n_dim,1,True,True,self.dropout)\n        #self.cast = P.Cast()\n\n    def construct(self, x):\n        #x = self.cast(x, mstype.float16)\n        output,(h1,c1) = self.rnn(x, (self.h,self.c))\n        return output,(h1,c1)\n\nclass Question(nn.Cell):\n    def __init__(self, options, is_training=True):\n        super(Question, self).__init__()\n        #dict_size(vocab_size)\n        self.dict_size = options['dict_size']\n        #n_dim (hidden_size)\n        self.n_dim = options['n_dim']\n        self.n_emb = options['n_emb']\n        \n        if is_training:\n            self.batch_size = options['batch_size']\n        else:\n            self.batch_size = 1\n\n        #self.trans = P.Transpose()\n        #self.perm = (1, 0, 2)\n        \n        #HIGHLIGHT \u7b2c\u4e8c\u4e2a\u53c2\u6570n_dim -> n_emb\n        self.embedding = nn.Embedding(self.dict_size, self.n_emb)\n        #?\n        self.lstm = LSTM(options, is_training=is_training).to_float(mstype.float16)\n        #self.h = Tensor(np.zeros((self.batch_size, self.n_dim)).astype(np.float16))\n        #self.c = Tensor(np.zeros((self.batch_size, self.n_dim)).astype(np.float16))\n\n    def construct(self, question_input):\n        embeddings = self.embedding(question_input)\n        #embeddings = self.trans(embeddings, self.perm)\n        output, (hn,cn) = self.lstm(embeddings)\n        return output, hn, cn\n", "metadata": {}, "execution_count": 41, "outputs": []}, {"cell_type": "code", "source": "class VQA(nn.Cell):\n    def __init__(self, config, is_train=True):\n        super(Seq2Seq, self).__init__()\n        self.max_len = config.max_seq_length\n        self.is_train = is_train\n\n        #self.encoder = Encoder(config, is_train)\n        #self.decoder = Decoder(config, is_train)\n        self.expanddims = P.ExpandDims()\n        self.squeeze = P.Squeeze(axis=0)\n        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n        self.concat = P.Concat(axis=1)\n        self.concat2 = P.Concat(axis=0)\n        self.select = P.Select()\n        self.softmax = nn.Softmax()\n        \n        \n        ##### my #####\n        self.question = Question(options, is_train)\n        self.vgg = vgg\n    \n    def construct(self, src, dst):\n        ### TODO:get image_feat\n        output,h_encode,c_encode = self.question(src)\n        h_encode = h_encode[0][-1]\n        \n        image_feat_down = fflayer(shared_params, image_feat, options,\n                             prefix='image_mlp',\n                              act_func=options.get('image_mlp_act',\n                                                   'tanh'))\n\n        image_feat_attention_1 = fflayer(shared_params, image_feat_down, options,\n                                         prefix='image_att_mlp_1',\n                                         act_func=options.get('image_att_mlp_act',\n                                                              'tanh'))\n        \n        h_encode_attention_1 = fflayer(shared_params, h_encode, options,\n                                       prefix='sent_att_mlp_1',\n                                       act_func=options.get('sent_att_mlp_act',\n                                                            'tanh'))  #\n        combined_feat_attention_1 = image_feat_attention_1 + \\\n                                    h_encode_attention_1[:, None, :]\n        \n        ###\u6682\u65f6\u4e0d\u7ba1\n        #if options['use_attention_drop']:\n            #combined_feat_attention_1 = dropout_layer(combined_feat_attention_1,\n                                                      #dropout, trng, drop_ratio)\n            \n        combined_feat_attention_1 = fflayer(shared_params,\n                                            combined_feat_attention_1, options,\n                                            prefix='combined_att_mlp_1',\n                                            act_func=options.get(\n                                                'combined_att_mlp_act',\n                                                'tanh'))\n        \n        prob_attention_1 = self.softmax(combined_feat_attention_1[:, :, 0])\n        image_feat_ave_1 = (prob_attention_1[:, :, None] * image_feat_down).sum(axis=1)\n\n        combined_hidden_1 = image_feat_ave_1 + h_encode\n        \n        # second layer attention model\n        image_feat_attention_2 = fflayer(shared_params, image_feat_down, options,\n                                         prefix='image_att_mlp_2',\n                                         act_func=options.get('image_att_mlp_act',\n                                                              'tanh'))\n        h_encode_attention_2 = fflayer(shared_params, combined_hidden_1, options,\n                                       prefix='sent_att_mlp_2',\n                                       act_func=options.get('sent_att_mlp_act',\n                                                            'tanh'))\n        combined_feat_attention_2 = image_feat_attention_2 + \\\n                                    h_encode_attention_2[:, None, :]\n        \n        ### \u6682\u65f6\u4e0d\u505a\n        #if options['use_attention_drop']:\n            #combined_feat_attention_2 = dropout_layer(combined_feat_attention_2,\n                                                      #dropout, trng, drop_ratio)\n\n        combined_feat_attention_2 = fflayer(shared_params,\n                                            combined_feat_attention_2, options,\n                                            prefix='combined_att_mlp_2',\n                                            act_func=options.get(\n                                                'combined_att_mlp_act', 'tanh'))\n        \n        prob_attention_2 = self.softmax(combined_feat_attention_2[:, :, 0])\n\n        image_feat_ave_2 = (prob_attention_2[:, :, None] * image_feat_down).sum(axis=1)\n\n        return outputs", "metadata": {}, "execution_count": 42, "outputs": []}, {"cell_type": "code", "source": "options.get('image_mlp_act',\n                                                   'tanh')", "metadata": {}, "execution_count": 43, "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {"text/plain": "'tanh'"}, "metadata": {}}]}, {"cell_type": "code", "source": "from mindspore import dtype as mstype\ntest_batch = 8\nall_question_idx = Tensor(np.array(all_question_idx),mstype.int32)", "metadata": {}, "execution_count": 44, "outputs": []}, {"cell_type": "code", "source": "x = all_question_idx[0:8,]", "metadata": {}, "execution_count": 45, "outputs": []}, {"cell_type": "code", "source": "#modify\nembedding = nn.Embedding(options['dict_size'], options['n_emb'],True)\nembeddings = embedding(x)", "metadata": {}, "execution_count": 21, "outputs": []}, {"cell_type": "code", "source": "print(embeddings.shape)", "metadata": {}, "execution_count": 22, "outputs": [{"name": "stdout", "text": "(8, 10, 500)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "#\u8fd9\u91cc\u76848\u662fbatch_size\nh = Tensor(np.zeros((1,8, options['n_dim']), dtype='float32'))\nc = Tensor(np.zeros((1,8, options['n_dim']), dtype='float32'))\n        \nnet = nn.LSTM(options['n_emb'],options['n_dim'],1,True,True)\noutput,(h1,c1) = net(embeddings, (h,c))", "metadata": {}, "execution_count": 24, "outputs": []}, {"cell_type": "code", "source": "h1[0][-1]", "metadata": {"scrolled": true}, "execution_count": 31, "outputs": [{"execution_count": 31, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[1024], dtype=Float32, value= [ 8.98742676e-03,  9.22393799e-03,  1.33590698e-02,  2.92968750e-03,  7.48443604e-03,  1.69849396e-03,  5.61904907e-03, -9.92584229e-03,  3.23486328e-03, -2.92205811e-03, -1.00402832e-02,  2.30216980e-03, \n -1.04522705e-02, -4.16183472e-03, -1.07803345e-02, -2.21633911e-03,  2.01225281e-03, -8.78906250e-03, -1.07002258e-03,  4.51087952e-04,  9.27734375e-03, -9.95635986e-03, -1.16539001e-03,  4.73022461e-03, \n -1.31225586e-02, -1.08795166e-02,  7.14874268e-03,  1.12991333e-02, -2.97164917e-03, -2.96401978e-03, -1.66320801e-02, -9.59014893e-03,  1.43966675e-02, -3.15475464e-03,  1.08413696e-02, -1.70898438e-02, \n -1.67083740e-02,  3.79753113e-03,  7.95745850e-03,  1.24130249e-02, -1.20925903e-02,  1.27792358e-02, -1.37786865e-02, -2.83050537e-03,  9.22203064e-04, -1.03759766e-02, -1.28784180e-02,  6.46972656e-03, \n  1.08184814e-02, -1.36256218e-04,  4.58145142e-03, -1.33285522e-02, -8.91113281e-03,  7.35473633e-03,  1.01776123e-02,  1.12533569e-02, -4.10461426e-03,  3.15093994e-03, -1.57318115e-02,  1.00860596e-02, \n -6.93130493e-03, -4.21905518e-03,  3.13949585e-03, -5.11932373e-03, -6.04629517e-03,  3.43132019e-03,  4.70352173e-03, -4.09698486e-03,  4.03594971e-03, -5.50842285e-03, -8.59069824e-03,  9.38415527e-03, \n  1.00173950e-02, -1.62658691e-02, -1.04370117e-02, -9.85717773e-03, -2.13241577e-03,  4.57000732e-03,  1.72119141e-02,  7.09533691e-03, -1.58843994e-02,  3.56292725e-03,  7.24792480e-03,  7.60269165e-03, \n  1.28479004e-02,  2.65312195e-03,  8.56018066e-03, -1.23262405e-04, -4.90188599e-03,  7.20977783e-04, -1.57012939e-02,  1.75595284e-04,  8.59737396e-04, -3.61824036e-03,  5.67436218e-04, -1.42288208e-02, \n  2.87055969e-04, -5.94615936e-04,  1.71661377e-02,  7.07244873e-03, -6.29043579e-03, -1.22909546e-02, -1.16577148e-02, -3.00025940e-03, -1.15356445e-02, -6.33621216e-03,  1.19094849e-02,  2.01988220e-03, \n  7.57694244e-04, -1.90429688e-02, -7.47680664e-03, -5.86318970e-03,  3.79943848e-03, -1.97296143e-02,  7.57598877e-03, -1.22528076e-02,  4.29534912e-03, -2.84194946e-03, -8.86917114e-04, -8.39233398e-03, \n  1.04293823e-02, -1.21917725e-02,  1.33132935e-02, -1.20162964e-02,  1.21612549e-02,  1.63726807e-02,  1.41220093e-02,  4.48226929e-03, -1.67846680e-02,  3.55720520e-03,  8.87298584e-03, -1.43508911e-02, \n -5.58471680e-03,  1.88636780e-03, -1.77459717e-02, -1.43203735e-02, -4.78363037e-03,  1.17492676e-02,  1.31454468e-02, -1.21765137e-02,  1.08566284e-02, -3.64303589e-03, -5.47790527e-03,  8.40759277e-03, \n -6.50405884e-03,  8.32366943e-03, -5.01632690e-03,  4.84466553e-03, -1.06887817e-02, -1.57470703e-02, -3.99017334e-03, -8.29315186e-03,  1.17778778e-03,  1.25503540e-02,  1.28784180e-02,  1.39007568e-02, \n  1.04522705e-02, -6.48880005e-03,  5.13458252e-03, -1.48620605e-02, -1.35498047e-02,  6.65283203e-03, -1.35345459e-02, -8.76617432e-03,  7.49206543e-03,  1.52282715e-02, -5.00488281e-03,  3.16047668e-03, \n  2.84957886e-03, -1.23443604e-02, -9.96589661e-04,  9.26208496e-03, -8.70513916e-03, -3.41606140e-03, -3.75747681e-03, -1.42288208e-03,  1.00173950e-02, -1.66168213e-02,  6.40869141e-03, -1.42669678e-02, \n  7.07244873e-03, -8.08715820e-03, -8.24737549e-03,  1.48391724e-02, -1.39083862e-02,  7.09533691e-03, -5.94139099e-04,  1.97601318e-02, -6.77871704e-03, -1.04598999e-02,  3.74794006e-03, -1.64947510e-02, \n -1.48239136e-02,  1.02844238e-02,  9.49096680e-03,  5.77926636e-03,  1.31835938e-02,  1.41296387e-02,  1.02615356e-02,  2.91442871e-03,  1.00708008e-02, -5.70678711e-03, -7.06863403e-03, -9.84191895e-03, \n -9.68170166e-03, -1.64642334e-02,  2.31552124e-03,  1.10092163e-02,  1.35421753e-02,  3.26728821e-03, -2.01225281e-03,  7.53402710e-04, -2.97737122e-03, -5.36727905e-03,  1.51138306e-02,  5.36346436e-03, \n  1.13830566e-02,  1.87835693e-02,  9.02557373e-03, -9.89532471e-03, -6.14929199e-03,  7.36236572e-03,  7.30514526e-03, -4.31442261e-03, -1.44805908e-02, -1.15814209e-02,  8.02612305e-03,  6.48498535e-03, \n -6.11305237e-04, -1.15814209e-02, -1.36566162e-02, -1.04751587e-02, -9.53674316e-03, -3.77082825e-03, -1.14345551e-03,  1.42765045e-03, -2.67982483e-03,  2.40135193e-03,  3.97872925e-03,  1.02233887e-03, \n -2.21633911e-03, -1.11236572e-02, -1.79195404e-03,  1.43966675e-02,  5.99861145e-04,  4.52423096e-03,  1.00784302e-02, -9.30023193e-03,  1.11618042e-02, -1.13906860e-02, -4.18853760e-03, -1.80358887e-02, \n -3.52478027e-03, -1.48239136e-02,  5.70678711e-03, -1.24359131e-02, -1.59149170e-02,  1.43508911e-02,  1.54876709e-02,  4.94384766e-03,  1.16577148e-02,  8.40759277e-03,  8.66699219e-03, -8.02040100e-04, \n -8.43811035e-03, -1.64489746e-02,  1.22299194e-02, -6.26754761e-03,  1.13449097e-02,  4.90570068e-03, -8.53729248e-03,  7.14111328e-03, -1.73645020e-02, -5.26428223e-03,  5.03063202e-04,  1.43051147e-02, \n -1.82037354e-02, -1.21536255e-02,  1.08032227e-02,  8.16345215e-03,  1.60064697e-02, -7.04193115e-03,  1.70745850e-02, -1.42211914e-02,  6.93511963e-03,  3.82041931e-03, -1.10321045e-02,  1.36795044e-02, \n -2.61068344e-04,  3.97109985e-03,  2.99453735e-03, -1.01699829e-02,  4.04739380e-03, -6.91604614e-03, -1.09863281e-02, -8.69750977e-03,  8.68225098e-03,  1.59301758e-02,  2.86674500e-03,  3.65257263e-04, \n  2.74181366e-04, -6.92367554e-04,  1.47247314e-03,  1.58691406e-02, -9.94873047e-03,  6.56509399e-03,  3.47137451e-03, -9.11712646e-03,  9.86480713e-03,  1.29604340e-03,  2.81095505e-04, -1.94244385e-02, \n -8.89587402e-03,  6.75582886e-03, -7.68661499e-03, -5.77926636e-03, -1.39083862e-02, -5.34057617e-03,  1.17874146e-02,  3.79180908e-03, -1.08623505e-03,  1.85394287e-02, -1.27944946e-02, -8.22448730e-03, \n -6.55746460e-03, -4.76074219e-03, -1.17797852e-02,  1.61132812e-02, -1.45568848e-02, -4.47845459e-03, -1.23519897e-02, -8.07952881e-03, -1.55925751e-03, -1.00479126e-02, -3.00598145e-03, -1.53884888e-02, \n  1.35421753e-02, -1.11618042e-02, -2.43186951e-03, -1.25579834e-02, -5.56945801e-03, -1.54800415e-02,  8.67247581e-05, -5.98907471e-03, -1.51138306e-02, -2.01797485e-03,  7.38501549e-05,  1.41143799e-02, \n  5.57422638e-04,  9.97161865e-03,  1.32322311e-04,  1.03073120e-02, -1.32980347e-02, -9.57489014e-03, -7.10296631e-03,  3.08036804e-03, -1.20162964e-02,  6.32095337e-03, -1.44348145e-02, -4.79888916e-03, \n -1.05133057e-02,  8.84246826e-03, -1.86614990e-02, -5.30624390e-03, -9.33074951e-03,  6.77108765e-03,  6.94274902e-03, -1.37557983e-02,  2.73466110e-04,  3.88717651e-03,  7.03811646e-03, -1.28402710e-02, \n  5.23757935e-03, -4.39071655e-03,  1.09710693e-02, -1.04370117e-02, -8.78143311e-03,  1.11770630e-02,  1.58081055e-02,  1.39999390e-02,  1.83868408e-03,  7.36999512e-03,  1.95770264e-02,  1.54876709e-03, \n  9.33074951e-03, -2.27737427e-03, -1.15585327e-03,  9.47570801e-03, -2.15454102e-02,  3.42941284e-03,  1.33285522e-02, -1.22146606e-02, -5.59234619e-03, -1.26571655e-02,  2.09999084e-03,  1.22985840e-02, \n -6.07299805e-03, -1.57775879e-02,  8.11767578e-03, -3.12423706e-03, -1.38549805e-02,  4.61196899e-03,  5.32913208e-03, -1.83410645e-02,  8.87298584e-03,  7.68280029e-03,  6.54220581e-04,  1.06430054e-02, \n  1.61743164e-03, -1.20391846e-02,  6.94656372e-03, -1.77154541e-02,  1.15890503e-02, -6.77108765e-04, -7.94219971e-03, -8.56781006e-03,  6.01577759e-03,  4.10079956e-03,  7.30037689e-04, -1.10931396e-02, \n -7.94982910e-03,  7.62176514e-03, -9.46044922e-03,  7.45391846e-03,  7.11822510e-03, -8.33892822e-03, -6.01196289e-03, -1.41220093e-02,  4.32205200e-03, -7.86590576e-03,  1.89876556e-03,  1.17950439e-02, \n -8.77380371e-03, -1.34944916e-03,  1.47018433e-02,  1.29318237e-02, -5.09643555e-03,  1.41143799e-03,  7.90405273e-03,  1.80206299e-02,  7.15255737e-03, -1.81732178e-02,  1.79443359e-02,  4.86755371e-03, \n -2.39181519e-03,  8.84246826e-03, -1.54876709e-02, -4.74929810e-03, -1.52587891e-02,  1.05590820e-02,  1.29623413e-02,  8.78906250e-03,  4.95529175e-03, -7.58361816e-03, -1.25732422e-02, -8.05664062e-03, \n -8.00323486e-03,  1.96266174e-03, -4.60052490e-03, -1.31454468e-02, -1.04370117e-02,  3.55720520e-03,  8.95690918e-03,  2.87055969e-03, -1.97982788e-03,  2.85339355e-03,  1.26495361e-02, -9.38415527e-03, \n -1.13143921e-02, -5.57708740e-03, -6.66427612e-03, -5.27954102e-03,  1.58538818e-02,  1.17397308e-03, -1.32904053e-02,  3.02124023e-03,  4.82559204e-03,  3.51667404e-04, -1.97906494e-02, -1.59606934e-02, \n -1.28936768e-03, -5.41687012e-03, -1.53961182e-02,  5.00488281e-03, -1.75189972e-03,  6.44302368e-03,  8.41522217e-03,  8.03375244e-03, -5.82504272e-03,  2.07824707e-02, -2.60543823e-03, -8.23497772e-04, \n -7.49588013e-03,  5.51605225e-03, -8.24737549e-03, -5.50460815e-03, -5.56564331e-03,  1.11083984e-02, -6.16073608e-03, -6.03485107e-03,  7.43103027e-03,  1.90639496e-03,  5.90133667e-03, -1.82952881e-02, \n  1.94835663e-03, -1.48487091e-03,  8.11004639e-03,  4.42123413e-03, -9.84191895e-04, -2.78091431e-03, -9.68933105e-03, -1.10702515e-02,  9.99450684e-03,  8.43811035e-03, -1.07421875e-02, -1.57070160e-03, \n -4.22286987e-03,  7.43865967e-03, -1.72882080e-02, -1.47399902e-02, -1.39312744e-02, -1.33972168e-02, -1.66034698e-03, -1.06887817e-02, -7.27176666e-04,  1.75170898e-02, -1.19476318e-02,  1.05209351e-02, \n  6.81304932e-03,  7.97271729e-03, -6.38961792e-03, -5.22613525e-03,  1.09100342e-02,  1.40285492e-03, -3.27873230e-03, -3.62586975e-03, -6.19888306e-03,  1.23977661e-02,  1.74713135e-02,  1.11579895e-03, \n  1.69677734e-02, -6.05392456e-03, -1.02996826e-03,  1.56402588e-02,  1.88140869e-02, -2.75039673e-03, -1.94702148e-02,  1.18255615e-02,  4.19235229e-03,  7.25555420e-03, -9.07135010e-03, -4.89807129e-03, \n -1.29852295e-02,  1.25350952e-02,  2.09236145e-03,  1.29699707e-02, -9.14764404e-03, -1.12819672e-03, -1.65100098e-02, -1.40762329e-02, -1.75628662e-02,  1.51824951e-02,  7.61508942e-04,  1.02844238e-02, \n  6.49642944e-03, -3.31497192e-03,  5.87463379e-03, -6.30569458e-03,  2.06451416e-02, -7.34329224e-03,  1.09634399e-02,  1.84020996e-02,  9.95635986e-03,  1.53884888e-02,  1.05819702e-02,  1.11312866e-02, \n  1.86157227e-02, -9.58251953e-03, -2.48336792e-03, -7.60650635e-03, -3.19957733e-04,  4.54711914e-03, -1.34963989e-02, -3.39508057e-03, -1.02844238e-02,  1.07955933e-02, -6.37769699e-06, -1.47323608e-02, \n  1.40762329e-03,  6.17599487e-03,  4.84466553e-03, -5.68008423e-03, -1.16195679e-02,  1.35574341e-02,  4.17327881e-03, -1.57775879e-02,  3.98635864e-03,  1.81484222e-03,  9.33074951e-03,  6.43539429e-03, \n -4.45556641e-03, -4.18853760e-03,  1.03225708e-02, -1.94244385e-02, -4.42504883e-03,  1.02005005e-02, -5.42831421e-03, -8.01849365e-03,  3.01551819e-03, -7.91168213e-03, -5.82885742e-03,  5.23757935e-03, \n -1.31530762e-02, -8.42285156e-03,  4.28771973e-03,  5.80596924e-03, -7.51495361e-03,  8.12530518e-03, -6.91604614e-03,  6.61468506e-03, -1.59549713e-03,  4.12750244e-03,  1.32446289e-02,  1.57623291e-02, \n  1.48296356e-04, -1.35040283e-02,  2.01721191e-02, -9.80377197e-03,  4.50611115e-04, -2.12669373e-03, -6.19506836e-03,  1.63879395e-02,  1.90277100e-02, -1.35116577e-02, -1.16729736e-02, -3.60870361e-03, \n  5.13458252e-03,  1.64031982e-02,  6.01577759e-03,  1.09786987e-02,  1.65820122e-04,  2.64549255e-03,  1.02996826e-02,  9.60540771e-03,  2.82096863e-03,  2.25639343e-03, -7.40051270e-03,  4.70733643e-03, \n  1.58843994e-02, -1.56402588e-02,  1.36032104e-02, -1.50299072e-02,  6.76155090e-04,  1.21154785e-02,  1.89208984e-03, -1.04522705e-02,  5.52368164e-03, -1.23596191e-02,  8.26716423e-05,  3.32260132e-03, \n  1.09329224e-02, -7.82012939e-03, -5.23757935e-03,  8.68225098e-03,  4.73022461e-03,  1.13906860e-02,  4.63104248e-03, -1.67083740e-02,  1.03378296e-02,  1.39312744e-02, -1.67541504e-02, -1.09481812e-02, \n -4.91142273e-04, -1.04751587e-02,  5.07354736e-03, -1.77001953e-03, -2.44903564e-03, -1.60980225e-02, -1.03836060e-02,  1.91955566e-02, -1.46865845e-02,  1.47857666e-02,  1.35879517e-02,  4.16946411e-03, \n -2.42424011e-03,  5.54275513e-03,  2.75230408e-03, -5.75637817e-03,  5.87463379e-03, -6.56890869e-03,  2.45475769e-03, -1.47857666e-02,  1.28479004e-02, -7.22885132e-04,  5.55801392e-03, -1.55715942e-02, \n -2.60353088e-03,  7.30514526e-03, -1.20697021e-02, -2.82287598e-03, -1.82189941e-02,  5.10787964e-03, -1.59454346e-02, -4.68826294e-03,  1.36089325e-03, -7.77435303e-03, -1.03988647e-02,  5.82504272e-03, \n -7.57980347e-03, -3.48472595e-03,  5.65719604e-03,  1.07879639e-02,  1.66473389e-02,  1.41372681e-02, -1.68914795e-02, -7.32803345e-03, -1.99432373e-02, -1.08184814e-02,  3.88526917e-03, -4.95910645e-03, \n -9.34600830e-03,  1.55868530e-02,  3.39889526e-03, -1.31988525e-02, -2.84957886e-03, -5.60760498e-03, -9.98687744e-03,  7.85350800e-04,  1.16119385e-02,  1.17301941e-04, -1.08566284e-02,  1.46389008e-03, \n -5.96237183e-03, -9.29260254e-03,  1.46179199e-02,  4.50897217e-03, -1.78833008e-02, -8.97216797e-03,  2.39753723e-03,  5.60379028e-03, -1.59606934e-02,  2.33054161e-04,  5.61904907e-03, -7.83538818e-03, \n  8.85772705e-03, -5.35202026e-03, -1.20925903e-02, -8.85772705e-03,  5.85174561e-03, -8.14056396e-03,  4.47463989e-03,  7.03048706e-03, -1.42097473e-03,  9.39178467e-03, -6.87026978e-03,  1.09786987e-02, \n -1.57165527e-02,  1.36089325e-03,  9.68170166e-03, -1.47628784e-02,  2.14934349e-04, -3.66973877e-03,  4.91333008e-03,  6.40106201e-03,  2.23517418e-04, -1.05209351e-02, -1.03302002e-02,  1.20391846e-02, \n -1.79290771e-03, -6.85119629e-03,  1.44767761e-03,  1.15356445e-02,  3.14712524e-03,  1.84173584e-02, -6.81686401e-03,  1.44424438e-02, -8.91113281e-03,  1.37939453e-02,  2.83050537e-03,  1.26457214e-03, \n -4.55856323e-04,  5.15365601e-03, -5.95855713e-03, -1.78623199e-03, -4.16183472e-03,  7.77053833e-03,  5.65719604e-03,  7.55310059e-03, -1.14135742e-02, -9.44519043e-03,  8.75091553e-03, -1.25579834e-02, \n -1.03530884e-02,  1.99127197e-02,  4.76074219e-03, -1.98364258e-03,  3.24440002e-03,  1.42974854e-02, -1.36718750e-02,  2.27165222e-03, -1.90429688e-02, -1.37405396e-02,  1.00250244e-02,  5.04302979e-03, \n  1.79100037e-03, -1.25408173e-03,  8.76617432e-03,  1.29241943e-02, -1.40228271e-02, -6.07967377e-04, -4.81009483e-05, -1.03759766e-03,  7.98034668e-03, -5.85174561e-03,  1.16729736e-02, -2.73513794e-03, \n -1.31454468e-02,  5.53894043e-03,  5.89847565e-04,  1.93595886e-03,  9.33837891e-03,  7.19833374e-03, -9.07135010e-03, -1.58691406e-03,  3.42369080e-03, -2.09999084e-03, -1.01394653e-02,  2.94303894e-03, \n -3.44467163e-03,  1.71279907e-03, -1.98974609e-02,  1.55029297e-02, -8.74328613e-03, -8.13603401e-05,  6.33239746e-03,  2.34069824e-02, -1.80969238e-02, -1.39999390e-02,  1.62696838e-03, -7.22503662e-03, \n  2.21061707e-03,  1.10931396e-02, -5.19752502e-04, -1.93939209e-02, -1.39312744e-02, -1.44805908e-02,  5.92803955e-03, -1.01318359e-02,  1.30538940e-02, -1.26953125e-02,  1.14517212e-02,  4.77218628e-03, \n  9.68170166e-03, -1.36871338e-02,  5.05065918e-03, -1.55029297e-02,  1.47933960e-02,  1.12686157e-02,  7.52258301e-03,  1.05972290e-02, -3.77082825e-03, -2.93493271e-04, -4.71496582e-03, -9.81140137e-03, \n  6.42013550e-03,  9.78851318e-03, -1.43508911e-02, -3.41415405e-03,  1.78070068e-02, -1.07421875e-02,  1.19018555e-02, -2.76565552e-05,  9.24682617e-03, -5.69152832e-03,  1.41620636e-03, -8.95690918e-03, \n  4.01687622e-03,  1.02691650e-02,  7.96508789e-03, -9.88769531e-03, -1.35192871e-02,  2.73513794e-03, -1.10244751e-02, -1.35650635e-02, -1.54876709e-02,  5.65338135e-03,  1.42974854e-02,  1.04751587e-02, \n  6.08825684e-03,  1.39312744e-02, -1.34124756e-02,  6.32858276e-03, -8.77380371e-03,  5.79833984e-03,  5.65719604e-03,  7.59124756e-03, -1.14669800e-02, -7.08389282e-03, -1.26342773e-02, -5.79071045e-03, \n -5.93948364e-03, -1.44195557e-02,  3.79753113e-03, -3.74221802e-03, -6.19125366e-03, -3.92913818e-03, -6.40487671e-03, -7.65228271e-03, -1.41067505e-02,  6.98852539e-03, -5.88607788e-03,  1.23748779e-02, \n -1.60369873e-02,  1.11007690e-02,  1.20391846e-02, -9.26208496e-03, -2.10380554e-03, -6.99234009e-03, -4.35638428e-03, -8.63647461e-03, -1.36032104e-02, -3.22914124e-03,  1.67999268e-02,  7.91931152e-03, \n  4.52041626e-03, -9.72747803e-03,  3.58963013e-03, -4.74548340e-03, -4.98962402e-03, -9.33837891e-03, -8.10241699e-03, -6.29806519e-03, -2.02560425e-03, -7.00759888e-03,  1.55487061e-02, -7.84301758e-03, \n -4.08172607e-03,  2.88200378e-03,  1.20315552e-02,  1.65252686e-02, -6.52313232e-03, -1.25274658e-02,  1.40762329e-02, -1.58538818e-02, -7.13825226e-04, -1.26419067e-02,  3.26919556e-03,  1.27487183e-02, \n -3.30734253e-03, -8.68988037e-03, -2.96592712e-04,  8.25500488e-03,  1.53045654e-02, -1.69372559e-02, -5.49697876e-03,  1.54876709e-02,  2.10762024e-03, -4.90951538e-03,  1.33705139e-03, -1.23062134e-02, \n -1.54724121e-02,  5.16128540e-03, -6.60324097e-03,  9.20867920e-03, -4.90951538e-03,  1.46408081e-02,  1.46560669e-02, -1.03225708e-02,  1.02310181e-02,  6.38198853e-03, -9.48333740e-03, -1.86462402e-02, \n -3.09944153e-05,  6.99234009e-03, -9.84191895e-04,  1.37710571e-02,  3.42559814e-03,  1.97029114e-03, -1.01318359e-02,  1.03988647e-02, -3.09944153e-03,  3.35884094e-03,  5.80596924e-03, -1.36337280e-02, \n -2.45475769e-03, -3.37600708e-03,  1.94091797e-02, -1.06353760e-02, -6.12258911e-03,  5.37109375e-03, -9.52911377e-03, -2.07710266e-03,  4.53567505e-03,  5.82885742e-03,  1.13105774e-03,  3.89099121e-03, \n -9.40704346e-03, -5.06210327e-03, -3.93295288e-03, -9.10186768e-03, -1.05361938e-02, -1.54037476e-02,  7.47680664e-03, -1.87225342e-02, -1.26495361e-02, -1.17034912e-02, -1.01547241e-02,  1.32751465e-02, \n  3.98254395e-03,  1.28250122e-02, -1.61437988e-02, -6.37435913e-03, -1.16577148e-02, -1.11770630e-02, -3.33786011e-03, -2.75802612e-03,  1.53121948e-02,  9.20104980e-03, -6.19411469e-04,  4.77313995e-04, \n  5.24139404e-03, -1.21536255e-02,  7.75527954e-03,  6.51550293e-03])"}, "metadata": {}}]}, {"cell_type": "code", "source": "\nnet = nn.Embedding(20000, 768,  True)\ninput_data = Tensor(np.ones([8, 128]), mindspore.int32)\n\n# Maps the input word IDs to word embedding.\noutput = net(input_data)\nresult = output.shape\nprint(result)\n\n", "metadata": {}, "execution_count": 10, "outputs": [{"name": "stdout", "text": "(8, 128, 768)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "h = Tensor([[1,2,1],\n            [1,1,1]], mstype.float32)\nc = Tensor(np.ones((1,3,2)), mstype.float32)", "metadata": {}, "execution_count": 39, "outputs": []}, {"cell_type": "code", "source": "output = mindspore.ops.dot(h,c)", "metadata": {}, "execution_count": 40, "outputs": []}, {"cell_type": "code", "source": "output", "metadata": {}, "execution_count": 41, "outputs": [{"execution_count": 41, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[2, 1, 2], dtype=Float32, value=\n[[[ 4.00000000e+00,  4.00000000e+00]],\n [[ 3.00000000e+00,  3.00000000e+00]]])"}, "metadata": {}}]}, {"cell_type": "code", "source": "h.shape", "metadata": {}, "execution_count": 42, "outputs": [{"execution_count": 42, "output_type": "execute_result", "data": {"text/plain": "(2, 3)"}, "metadata": {}}]}, {"cell_type": "code", "source": "c.shape", "metadata": {}, "execution_count": 43, "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {"text/plain": "(1, 3, 2)"}, "metadata": {}}]}, {"cell_type": "code", "source": "input_x1 = Tensor(np.ones(shape=[2, 3]), mindspore.float32)\ninput_x2 = Tensor(np.ones(shape=[1, 3, 2]), mindspore.float32)\noutput = mindspore.ops.dot(input_x1, input_x2)\nprint(output)\n", "metadata": {}, "execution_count": 45, "outputs": [{"name": "stdout", "text": "[[[3. 3.]]\n\n [[3. 3.]]]\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "print(output.shape)\nprint(input_x1.shape)\nprint(input_x2.shape)", "metadata": {}, "execution_count": 49, "outputs": [{"name": "stdout", "text": "(2, 1, 2)\n(2, 3)\n(1, 3, 2)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "x1 = np.array([[[1,0,1],[0,0,1]],[[1,1,0],[1,1,1]]])\nprint(x1.shape)\ninput_x1 = Tensor(x1, mindspore.float32)", "metadata": {}, "execution_count": 37, "outputs": [{"name": "stdout", "text": "(2, 2, 3)\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "input_x1.reshape(1,4,3)", "metadata": {}, "execution_count": 38, "outputs": [{"execution_count": 38, "output_type": "execute_result", "data": {"text/plain": "Tensor(shape=[1, 4, 3], dtype=Float32, value=\n[[[ 1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n  [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n  [ 1.00000000e+00,  1.00000000e+00,  0.00000000e+00]\n  [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00]]])"}, "metadata": {}}]}, {"cell_type": "code", "source": "[[[0,1,2]] , [[0,1,2]] ]", "metadata": {}, "execution_count": null, "outputs": []}]}